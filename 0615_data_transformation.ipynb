{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda create -n tft_env python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime as dte\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v2' has no attribute '__internal__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16876\\2245522176.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \"\"\"\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\keras\\models\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFunctional\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtensor\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayout_map\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlayout_map_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\keras\\backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_coordinator_utils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\keras\\backend_config.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mkeras_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"keras.backend.epsilon\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_dispatch_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \"\"\"Returns the value of the fuzz factor used in numeric expressions.\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v2' has no attribute '__internal__'"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import data_formatters.base\n",
    "#import expt_settings.configs\n",
    "#import libs.hyperparam_opt\n",
    "import libs.tft_model\n",
    "import libs.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_formatter_class = {\n",
    "        'volatility': data_formatters.volatility.VolatilityFormatter,\n",
    "        'electricity': data_formatters.electricity.ElectricityFormatter,\n",
    "        'traffic': data_formatters.traffic.TrafficFormatter,\n",
    "        'favorita': data_formatters.favorita.FavoritaFormatter\n",
    "    }\n",
    "formatter = data_formatter_class('favorita')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'use_gpu' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21144\\4209327453.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         log_device_placement=False, device_count={'GPU': 0})\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtf_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_tensorflow_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'use_gpu' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.keras.backend as K\n",
    "default_keras_session = K.get_session()\n",
    "# cpu\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # for training on cpu\n",
    "tf_config = tf.ConfigProto(\n",
    "        log_device_placement=False, device_count={'GPU': 0})\n",
    "\n",
    "if use_gpu:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"gpu\", gpu_id=0)\n",
    "\n",
    "else:\n",
    "    tf_config = utils.get_default_tensorflow_config(tf_device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = 'favorita_out.csv'\n",
    "df = pd.read_csv(data_csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'dropout_rate': 0.1,\n",
    "        'hidden_layer_size': 240,\n",
    "        'learning_rate': 0.001,\n",
    "        'minibatch_size': 128,\n",
    "        'max_gradient_norm': 100.,\n",
    "        'num_heads': 4,\n",
    "        'stack_size': 1\n",
    "    }\n",
    "\n",
    "fixed_params = {\n",
    "        'total_time_steps': 120,\n",
    "        'num_encoder_steps': 90,\n",
    "        'num_epochs': 100,\n",
    "        'early_stopping_patience': 5,\n",
    "        'multiprocessing_workers': 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import enum\n",
    "class DataTypes(enum.IntEnum):\n",
    "  \"\"\"Defines numerical types of each column.\"\"\"\n",
    "  REAL_VALUED = 0\n",
    "  CATEGORICAL = 1\n",
    "  DATE = 2\n",
    "\n",
    "class InputTypes(enum.IntEnum):\n",
    "  \"\"\"Defines input types of each column.\"\"\"\n",
    "  TARGET = 0\n",
    "  OBSERVED_INPUT = 1\n",
    "  KNOWN_INPUT = 2\n",
    "  STATIC_INPUT = 3\n",
    "  ID = 4  # Single column used as an entity identifier\n",
    "  TIME = 5 \n",
    "\n",
    "column_definition = [\n",
    "      ('traj_id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "      ('date', DataTypes.DATE, InputTypes.TIME),\n",
    "\n",
    "      ('log_sales', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "\n",
    "      ('onpromotion', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n",
    "      ('day_of_week', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n",
    "      ('national_hol', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n",
    "      ('regional_hol', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n",
    "      ('local_hol', DataTypes.CATEGORICAL, InputTypes.KNOWN_INPUT),\n",
    "      \n",
    "      ('transactions', DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n",
    "      ('oil', DataTypes.REAL_VALUED, InputTypes.OBSERVED_INPUT),\n",
    "\n",
    "      ('day_of_month', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('month', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('open', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "\n",
    "      ('item_nbr', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('store_nbr', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('city', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('state', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('type', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('cluster', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('family', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('class', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "      ('perishable', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_single_column(input_type):\n",
    "      length = len([tup for tup in column_definition if tup[2] == input_type])\n",
    "\n",
    "      if length != 1:\n",
    "        raise ValueError('Illegal number of inputs ({}) of type {}'.format(\n",
    "            length, input_type))\n",
    "\n",
    "_check_single_column(InputTypes.ID)\n",
    "_check_single_column(InputTypes.TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = [tup for tup in column_definition if tup[2] == InputTypes.ID]\n",
    "time = [tup for tup in column_definition if tup[2] == InputTypes.TIME]\n",
    "real_inputs = [\n",
    "        tup for tup in column_definition if tup[1] == DataTypes.REAL_VALUED and\n",
    "        tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_definition_map = {tup[0]: tup for tup in column_definition}\n",
    "col_order = [\n",
    "        'item_nbr', 'store_nbr', 'city', 'state', 'type', 'cluster', 'family',\n",
    "        'class', 'perishable', 'onpromotion', 'day_of_week', 'national_hol',\n",
    "        'regional_hol', 'local_hol'\n",
    "    ]\n",
    "categorical_inputs = [\n",
    "        col_definition_map[k] for k in col_order if k in col_definition_map\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_definitions = identifier + time + real_inputs + categorical_inputs\n",
    "fixed_params['column_definition'] = identifier + time + real_inputs + categorical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2206    2013-01-01\n",
       "817     2013-01-02\n",
       "2207    2013-01-02\n",
       "818     2013-01-03\n",
       "2208    2013-01-03\n",
       "           ...    \n",
       "1634    2015-03-30\n",
       "815     2015-03-30\n",
       "3095    2015-03-31\n",
       "3025    2015-03-31\n",
       "816     2015-03-31\n",
       "Name: date, Length: 3096, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.date.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_boundary = dte.datetime(2013, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = fixed_params['total_time_steps'] # 120\n",
    "lookback = fixed_params['num_encoder_steps'] # 90\n",
    "forecast_horizon = time_steps - lookback # forecast_horizon=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>open</th>\n",
       "      <th>date</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>...</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>transactions</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>national_hol</th>\n",
       "      <th>regional_hol</th>\n",
       "      <th>local_hol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103520.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-01-04 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>93.12</td>\n",
       "      <td>...</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1863.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103520.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-01-05 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1509.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Recupero puente Navidad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103520.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-01-07 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>93.20</td>\n",
       "      <td>...</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1807.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103520.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-01-08 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>93.21</td>\n",
       "      <td>...</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1869.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>103520.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-01-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-01-09</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>93.08</td>\n",
       "      <td>...</td>\n",
       "      <td>GROCERY I</td>\n",
       "      <td>1028</td>\n",
       "      <td>0</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr  item_nbr  unit_sales onpromotion   traj_id  \\\n",
       "0        1.0  103520.0         2.0         NaN  1_103520   \n",
       "1        1.0  103520.0         3.0         NaN  1_103520   \n",
       "3        1.0  103520.0         2.0         NaN  1_103520   \n",
       "4        1.0  103520.0         6.0         NaN  1_103520   \n",
       "5        1.0  103520.0         3.0         NaN  1_103520   \n",
       "\n",
       "                      unique_id  open       date  log_sales    oil  ...  \\\n",
       "0  1_103520_2013-01-04 00:00:00   1.0 2013-01-04   0.693147  93.12  ...   \n",
       "1  1_103520_2013-01-05 00:00:00   1.0 2013-01-05   1.098612  -1.00  ...   \n",
       "3  1_103520_2013-01-07 00:00:00   1.0 2013-01-07   0.693147  93.20  ...   \n",
       "4  1_103520_2013-01-08 00:00:00   1.0 2013-01-08   1.791759  93.21  ...   \n",
       "5  1_103520_2013-01-09 00:00:00   1.0 2013-01-09   1.098612  93.08  ...   \n",
       "\n",
       "      family class perishable  transactions day_of_week  day_of_month  month  \\\n",
       "0  GROCERY I  1028          0        1863.0           4             4      1   \n",
       "1  GROCERY I  1028          0        1509.0           5             5      1   \n",
       "3  GROCERY I  1028          0        1807.0           0             7      1   \n",
       "4  GROCERY I  1028          0        1869.0           1             8      1   \n",
       "5  GROCERY I  1028          0        1910.0           2             9      1   \n",
       "\n",
       "              national_hol  regional_hol  local_hol  \n",
       "0                      NaN           NaN        NaN  \n",
       "1  Recupero puente Navidad           NaN        NaN  \n",
       "3                      NaN           NaN        NaN  \n",
       "4                      NaN           NaN        NaN  \n",
       "5                      NaN           NaN        NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df_lists = {'train': [], 'valid': [], 'test': []}\n",
    "for _, sliced in df.groupby('traj_id'):\n",
    "      index = sliced['date']\n",
    "      train = sliced.loc[index < valid_boundary]\n",
    "      train_len = len(train)\n",
    "      valid_len = train_len + forecast_horizon # valid_len = train_len + 30\n",
    "      valid = sliced.iloc[train_len - lookback:valid_len, :]\n",
    "      test = sliced.iloc[valid_len - lookback:valid_len + forecast_horizon, :]\n",
    "\n",
    "      sliced_map = {'train': train, 'valid': valid, 'test': test}\n",
    "\n",
    "      for k in sliced_map:\n",
    "        item = sliced_map[k]\n",
    "\n",
    "        if len(item) >= time_steps:\n",
    "          df_lists[k].append(item)\n",
    "\n",
    "# time_steps = fixed_params['total_time_steps'] # 120\n",
    "# lookback = fixed_params['num_encoder_steps'] # 90\n",
    "# forecast_horizon = time_steps - lookback # forecast_horizon=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {k: pd.concat(df_lists[k], axis=0) for k in df_lists} # k= train,valid,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dfs['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_103520', '1_103665', '1_96995', '25_103665']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [tup[0] for tup in column_definition if tup[2] == InputTypes.ID]\n",
    "id_column = l[0]  # l[0]=='traj_id'\n",
    "identifiers = list(train[id_column].unique())\n",
    "identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(frame): # must have same traj_id among train, valid and test dataset\n",
    "    index = frame['traj_id']\n",
    "    return frame.loc[index.apply(lambda x: x in set(identifiers))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 24)\n",
      "(360, 24)\n",
      "(360, 24)\n"
     ]
    }
   ],
   "source": [
    "valid = filter_ids(dfs['valid'])\n",
    "test = filter_ids(dfs['test'])\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_column_definition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21144\\2452909157.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcolumn_definitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_column_definition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'get_column_definition' is not defined"
     ]
    }
   ],
   "source": [
    "column_definitions = get_column_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions)\n",
    "target_column = utils.get_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Format real scalers\n",
    "_real_scalers = {}\n",
    "for col in ['oil', 'transactions', 'log_sales']:\n",
    "    _real_scalers[col] = (df[col].mean(), df[col].std())\n",
    "\n",
    "_target_scaler = (df[target_column].mean(), df[target_column].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Format categorical scalers\n",
    "categorical_inputs_name = utils.extract_cols_from_data_type(\n",
    "          DataTypes.CATEGORICAL, real_inputs + categorical_inputs,\n",
    "          {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "categorical_scalers = {}\n",
    "num_classes = []\n",
    "\n",
    "valid_idx = df['traj_id'].apply(lambda x: x in set(identifiers))\n",
    "for col in categorical_inputs_name:\n",
    "# Set all to str so that we don't have mixed integer/string columns\n",
    "    srs = df[col].apply(str).loc[valid_idx]\n",
    "    categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(\n",
    "            srs.values)\n",
    "    num_classes.append(srs.nunique())\n",
    "num_classes_per_cat_input = num_classes\n",
    "_cat_scalers = categorical_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'oil': (64.74225129198966, 44.23409343597884), 'transactions': (1400.2722868217054, 613.487068343699), 'log_sales': (0.9648701125069206, 0.7171221902090524)}\n",
      "(0.9648701125069206, 0.7171221902090524)\n",
      "{'item_nbr': LabelEncoder(), 'store_nbr': LabelEncoder(), 'city': LabelEncoder(), 'state': LabelEncoder(), 'type': LabelEncoder(), 'cluster': LabelEncoder(), 'family': LabelEncoder(), 'class': LabelEncoder(), 'perishable': LabelEncoder(), 'onpromotion': LabelEncoder(), 'day_of_week': LabelEncoder(), 'national_hol': LabelEncoder(), 'regional_hol': LabelEncoder(), 'local_hol': LabelEncoder()}\n"
     ]
    }
   ],
   "source": [
    "print(_real_scalers)\n",
    "print(_target_scaler)\n",
    "print(_cat_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(df):\n",
    "    \"\"\"Performs feature transformations.\n",
    "\n",
    "    This includes both feature engineering, preprocessing and normalisation.\n",
    "\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "\n",
    "    \"\"\"\n",
    "    output = df.copy()\n",
    "\n",
    "    if _real_scalers is None and _cat_scalers is None:\n",
    "      raise ValueError('Scalers have not been set!')\n",
    "\n",
    "    #column_definitions = get_column_definition()\n",
    "\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "    # (1) Format real inputs: standardization\n",
    "    for col in ['log_sales', 'oil', 'transactions']:\n",
    "      mean, std = _real_scalers[col]\n",
    "      output[col] = (df[col] - mean) / std  # standardization\n",
    "\n",
    "      if col == 'log_sales':\n",
    "        output[col] = output[col].fillna(0.)  # mean imputation\n",
    "\n",
    "    # (2) Format categorical inputs: LabelEncoder()\n",
    "    for col in categorical_inputs:\n",
    "      string_df = df[col].apply(str)\n",
    "      output[col] = _cat_scalers[col].transform(string_df)\n",
    "    ## Example:\n",
    "    ## _cat_scalers['city'].transform(train['city'].apply(str))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transform_inputs(train) \n",
    "valid_tf = transform_inputs(valid) \n",
    "test_tf = transform_inputs(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 24)\n",
      "(360, 24)\n",
      "(360, 24)\n"
     ]
    }
   ],
   "source": [
    "print(train_tf.shape)\n",
    "print(valid_tf.shape)\n",
    "print(test_tf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file\n",
    "train_tf.to_csv('train_tf.csv',index=False)\n",
    "valid_tf.to_csv('valid_tf.csv',index=False)\n",
    "test_tf.to_csv('test_tf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_time_steps': 120,\n",
       " 'num_encoder_steps': 90,\n",
       " 'num_epochs': 100,\n",
       " 'early_stopping_patience': 5,\n",
       " 'multiprocessing_workers': 5,\n",
       " 'column_definition': [('traj_id',\n",
       "   <DataTypes.REAL_VALUED: 0>,\n",
       "   <InputTypes.ID: 4>),\n",
       "  ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>),\n",
       "  ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>),\n",
       "  ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)],\n",
       " 'input_size': 20,\n",
       " 'output_size': 1,\n",
       " 'category_counts': [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4],\n",
       " 'input_obs_loc': [0],\n",
       " 'static_input_loc': [6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " 'known_regular_inputs': [3, 4, 5],\n",
       " 'known_categorical_inputs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_tft_input_indices():\n",
    "    \"\"\"Returns the relevant indexes and input sizes required by TFT.\"\"\"\n",
    "\n",
    "    # Functions\n",
    "    def _get_locations(input_types, defn):\n",
    "      return [i for i, tup in enumerate(defn) if tup[2] in input_types]\n",
    "\n",
    "    locations = {\n",
    "        'input_size': # not a str\n",
    "            len(real_inputs + categorical_inputs) ,# remove ID, TIME\n",
    "\n",
    "        'output_size': # loc within total inputs\n",
    "            len(_get_locations({InputTypes.TARGET}, real_inputs + categorical_inputs)),\n",
    "\n",
    "        'category_counts':\n",
    "            num_classes_per_cat_input,\n",
    "            \n",
    "        'input_obs_loc': # loc within total inputs\n",
    "            _get_locations({InputTypes.TARGET}, real_inputs + categorical_inputs),\n",
    "\n",
    "        'static_input_loc':# loc within total inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT}, real_inputs + categorical_inputs),\n",
    "\n",
    "        'known_regular_inputs':# loc within real_inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           real_inputs), \n",
    "\n",
    "        'known_categorical_inputs':# loc within categorical_inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           categorical_inputs)  \n",
    "    }\n",
    "    return locations\n",
    "fixed_params.update(_get_tft_input_indices())\n",
    "fixed_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ids(frame): # must have same traj_id among train, valid and test dataset\n",
    "    index = frame['traj_id']\n",
    "    return frame.loc[index.apply(lambda x: x in set(identifiers))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading hyperparm manager ***\n"
     ]
    }
   ],
   "source": [
    "# Parameter overrides for testing only! Small sizes used to speed up script.\n",
    "fixed_params[\"num_epochs\"] = 1\n",
    "params[\"hidden_layer_size\"] = 5\n",
    "\n",
    "model_folder = '0615_result'\n",
    "# model_folder: Folder to store optimisation artifacts.\n",
    "params[\"model_folder\"] = model_folder\n",
    "\n",
    "train_samples, valid_samples = 100, 10\n",
    "\n",
    "# Sets up hyperparam manager\n",
    "print(\"*** Loading hyperparm manager ***\")\n",
    "num_repeats = 1\n",
    "use_gpu = False # use cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Param update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': [0.1],\n",
       " 'hidden_layer_size': [5],\n",
       " 'learning_rate': [0.001],\n",
       " 'minibatch_size': [128],\n",
       " 'max_gradient_norm': [100.0],\n",
       " 'num_heads': [4],\n",
       " 'stack_size': [1],\n",
       " 'model_folder': ['0615_result']}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparam opt\n",
    "param_ranges = {k: [params[k]] for k in params} # k: name of param, params[k]: value of param\n",
    "param_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_parameters(ranges_to_skip=None):\n",
    "    \"\"\"Returns the next set of parameters to optimise.\n",
    "    Args:\n",
    "      ranges_to_skip: Explicitly defines a set of keys to skip.\n",
    "    \"\"\"\n",
    "    _max_tries = 1000\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    if ranges_to_skip is None:\n",
    "      ranges_to_skip = set(results.index)\n",
    "\n",
    "    param_range_keys = list(param_ranges.keys())\n",
    "    param_range_keys.sort()\n",
    "\n",
    "    def _get_next():\n",
    "      \"\"\"Returns next hyperparameter set per try.\"\"\"\n",
    "      parameters = {\n",
    "          k: np.random.choice(param_ranges[k]) for k in param_range_keys\n",
    "      } \n",
    "      # Adds fixed params\n",
    "      for k in fixed_params:\n",
    "        parameters[k] = fixed_params[k]\n",
    "      return parameters\n",
    "    \n",
    "    def _get_name(params):\n",
    "      \"\"\"Returns a unique key for the supplied set of params.\"\"\"\n",
    "\n",
    "      #self._check_params(params)\n",
    "\n",
    "      fields = list(params.keys())\n",
    "      fields.sort()\n",
    "      return \"_\".join([str(params[k]) for k in fields])\n",
    "\n",
    "    for _ in range(_max_tries):\n",
    "\n",
    "      parameters = _get_next()\n",
    "      name = _get_name(parameters)\n",
    "\n",
    "      if name not in ranges_to_skip:\n",
    "        return parameters\n",
    "\n",
    "    raise ValueError(\"Exceeded max number of hyperparameter searches!!\")\n",
    "\n",
    "params_update = get_next_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.1,\n",
       " 'hidden_layer_size': 5,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_gradient_norm': 100.0,\n",
       " 'minibatch_size': 128,\n",
       " 'model_folder': '0615_result',\n",
       " 'num_heads': 4,\n",
       " 'stack_size': 1,\n",
       " 'total_time_steps': 120,\n",
       " 'num_encoder_steps': 90,\n",
       " 'num_epochs': 1,\n",
       " 'early_stopping_patience': 5,\n",
       " 'multiprocessing_workers': 5,\n",
       " 'column_definition': [('traj_id',\n",
       "   <DataTypes.REAL_VALUED: 0>,\n",
       "   <InputTypes.ID: 4>),\n",
       "  ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>),\n",
       "  ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>),\n",
       "  ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)],\n",
       " 'input_size': 20,\n",
       " 'output_size': 1,\n",
       " 'category_counts': [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4],\n",
       " 'input_obs_loc': [0],\n",
       " 'static_input_loc': [6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " 'known_regular_inputs': [3, 4, 5],\n",
       " 'known_categorical_inputs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('params_update.pkl', 'wb') as f:\n",
    "    pickle.dump(params_update, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(self, predictions):\n",
    "    \"\"\"Reverts any normalisation to give predictions in original scale.\n",
    "\n",
    "    Args:\n",
    "      predictions: Dataframe of model predictions.\n",
    "\n",
    "    Returns:\n",
    "      Data frame of unnormalised predictions.\n",
    "    \"\"\"\n",
    "    output = predictions.copy()\n",
    "\n",
    "    column_names = predictions.columns\n",
    "    mean, std = self._target_scaler\n",
    "    for col in column_names:\n",
    "      if col not in {'forecast_time', 'identifier'}:\n",
    "        output[col] = (predictions[col] * std) + mean\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperparamOptManager = libs.hyperparam_opt.HyperparamOptManager\n",
    "ModelClass = libs.tft_model.TemporalFusionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%pip install tensorflow==2.5.0\n",
    "#%pip install keras==2.2.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading hyperparm manager ***\n",
      "*** Running calibration ***\n",
      "Params Selected:\n",
      "dropout_rate: 0.1\n",
      "hidden_layer_size: 5\n",
      "learning_rate: 0.001\n",
      "minibatch_size: 128\n",
      "max_gradient_norm: 100.0\n",
      "num_heads: 4\n",
      "stack_size: 1\n",
      "model_folder: 0615_result\n",
      "Resetting temp folder...\n",
      "*** TemporalFusionTransformer params ***\n",
      "# dropout_rate = 0.1\n",
      "# hidden_layer_size = 5\n",
      "# learning_rate = 0.001\n",
      "# max_gradient_norm = 100.0\n",
      "# minibatch_size = 128\n",
      "# model_folder = 0615_result\n",
      "# num_heads = 4\n",
      "# stack_size = 1\n",
      "# total_time_steps = 120\n",
      "# num_encoder_steps = 90\n",
      "# num_epochs = 1\n",
      "# early_stopping_patience = 5\n",
      "# multiprocessing_workers = 5\n",
      "# column_definition = [('traj_id', <DataTypes.REAL_VALUED: 0>, <InputTypes.ID: 4>), ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>), ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)]\n",
      "# input_size = 20\n",
      "# output_size = 1\n",
      "# category_counts = [14]\n",
      "# input_obs_loc = [0]\n",
      "# static_input_loc = [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "# known_regular_inputs = [3, 4, 5]\n",
      "# known_categorical_inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pan\\miniconda3\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [74]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#params = opt_manager.get_next_parameters()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m params_update \u001b[38;5;241m=\u001b[39m get_next_parameters() \u001b[38;5;66;03m# add fixed params\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModelClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cudnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtraining_data_cached():\n\u001b[0;32m     26\u001b[0m   model\u001b[38;5;241m.\u001b[39mcache_batched_data(train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39mtrain_samples)\n",
      "File \u001b[1;32mc:\\Users\\pan\\Desktop\\Scolar\\300 中广核\\330 algorithm\\TFT\\TFT_code\\libs\\tft_model.py:482\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.__init__\u001b[1;34m(self, raw_params, use_cudnn)\u001b[0m\n\u001b[0;32m    479\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m# \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k, params[k]))\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# Build model\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pan\\Desktop\\Scolar\\300 中广核\\330 algorithm\\TFT\\TFT_code\\libs\\tft_model.py:1055\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;66;03m#with tf.variable_scope(self.name):\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mvariable_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname):\n\u001b[0;32m   1054\u001b[0m   transformer_layer, all_inputs, attention_components \\\n\u001b[1;32m-> 1055\u001b[0m       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_base_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1057\u001b[0m   \u001b[38;5;66;03m# Dense\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mTimeDistributed(\n\u001b[0;32m   1059\u001b[0m       tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantiles))) \\\n\u001b[0;32m   1060\u001b[0m       (transformer_layer[\u001b[38;5;28mEllipsis\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_encoder_steps:, :])\n",
      "File \u001b[1;32mc:\\Users\\pan\\Desktop\\Scolar\\300 中广核\\330 algorithm\\TFT\\TFT_code\\libs\\tft_model.py:807\u001b[0m, in \u001b[0;36mTemporalFusionTransformer._build_base_graph\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;66;03m# Inputs.\u001b[39;00m\n\u001b[0;32m    800\u001b[0m all_inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(\n\u001b[0;32m    801\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    802\u001b[0m         time_steps,\n\u001b[0;32m    803\u001b[0m         combined_input_size,\n\u001b[0;32m    804\u001b[0m     ))\n\u001b[0;32m    806\u001b[0m unknown_inputs, known_combined_layer, obs_inputs, static_inputs \\\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tft_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# Isolate known and observed historical inputs.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unknown_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pan\\Desktop\\Scolar\\300 中广核\\330 algorithm\\TFT\\TFT_code\\libs\\tft_model.py:553\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.get_tft_embeddings\u001b[1;34m(self, all_inputs)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_input_loc:\n\u001b[0;32m    547\u001b[0m   static_inputs \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer_size)(\n\u001b[0;32m    548\u001b[0m       regular_inputs[:, \u001b[38;5;241m0\u001b[39m, i:i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_regular_variables) \u001b[38;5;66;03m# dim(1)=0\u001b[39;00m\n\u001b[0;32m    549\u001b[0m                    \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_input_loc] \\\n\u001b[0;32m    550\u001b[0m       \u001b[38;5;241m+\u001b[39m [embedded_inputs[i][:, \u001b[38;5;241m0\u001b[39m, :] \u001b[38;5;66;03m# dim(1)=0\u001b[39;00m\n\u001b[0;32m    551\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_categorical_variables)\n\u001b[0;32m    552\u001b[0m          \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m num_regular_variables \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_static_input_loc]\n\u001b[1;32m--> 553\u001b[0m   static_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatic_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# concat base on axis=1\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    556\u001b[0m   static_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pan\\miniconda3\\lib\\site-packages\\keras\\src\\legacy\\backend.py:2090\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   2087\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras._legacy.backend.stack\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2088\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"DEPRECATED.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2090\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pan\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\pan\\miniconda3\\lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "# Sets up hyperparam manager\n",
    "print(\"*** Loading hyperparm manager ***\")\n",
    "#opt_manager = HyperparamOptManager({k: [params[k]] for k in params},\n",
    "#                                     fixed_params, model_folder)\n",
    "\n",
    "  # Training -- one iteration only\n",
    "print(\"*** Running calibration ***\")\n",
    "print(\"Params Selected:\")\n",
    "for k in params:\n",
    "    print(\"{}: {}\".format(k, params[k]))\n",
    "\n",
    "best_loss = np.Inf\n",
    "for _ in range(num_repeats):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "\n",
    "      #tf.keras.backend.set_session(sess)\n",
    "      K.set_session(sess)\n",
    "\n",
    "      #params = opt_manager.get_next_parameters()\n",
    "      params_update = get_next_parameters() # add fixed params\n",
    "      model = ModelClass(params_update, use_cudnn=use_gpu)\n",
    "\n",
    "      if not model.training_data_cached():\n",
    "        model.cache_batched_data(train, \"train\", num_samples=train_samples)\n",
    "        model.cache_batched_data(valid, \"valid\", num_samples=valid_samples)\n",
    "\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "      model.fit()\n",
    "\n",
    "      val_loss = model.evaluate()\n",
    "\n",
    "      if val_loss < best_loss:\n",
    "        opt_manager.update_score(params, val_loss, model)\n",
    "        best_loss = val_loss\n",
    "\n",
    "      tf.keras.backend.set_session(default_keras_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Running tests ***\")\n",
    "tf.reset_default_graph()\n",
    "with tf.Graph().as_default(), tf.Session(config=tf_config) as sess:\n",
    "    tf.keras.backend.set_session(sess)\n",
    "    best_params = opt_manager.get_best_params()\n",
    "    model = ModelClass(best_params, use_cudnn=use_gpu)\n",
    "\n",
    "    model.load(opt_manager.hyperparam_folder)\n",
    "\n",
    "    print(\"Computing best validation loss\")\n",
    "    val_loss = model.evaluate(valid)\n",
    "\n",
    "    print(\"Computing test loss\")\n",
    "    output_map = model.predict(test, return_targets=True)\n",
    "    targets = format_predictions(output_map[\"targets\"])\n",
    "    p50_forecast = format_predictions(output_map[\"p50\"])\n",
    "    p90_forecast = format_predictions(output_map[\"p90\"])\n",
    "\n",
    "    def extract_numerical_data(data):\n",
    "      \"\"\"Strips out forecast time and identifier columns.\"\"\"\n",
    "      return data[[\n",
    "          col for col in data.columns\n",
    "          if col not in {\"forecast_time\", \"identifier\"}\n",
    "      ]]\n",
    "\n",
    "    p50_loss = utils.numpy_normalised_quantile_loss(\n",
    "        extract_numerical_data(targets), extract_numerical_data(p50_forecast),\n",
    "        0.5)\n",
    "    p90_loss = utils.numpy_normalised_quantile_loss(\n",
    "        extract_numerical_data(targets), extract_numerical_data(p90_forecast),\n",
    "        0.9)\n",
    "\n",
    "    tf.keras.backend.set_session(default_keras_session)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training completed @ {}\".format(dte.datetime.now()))\n",
    "print(\"Best validation loss = {}\".format(val_loss))\n",
    "print(\"Params:\")\n",
    "\n",
    "for k in best_params:\n",
    "    print(k, \" = \", best_params[k])\n",
    "print()\n",
    "print(\"Normalised Quantile Loss for Test Data: P50={}, P90={}\".format(\n",
    "      p50_loss.mean(), p90_loss.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
