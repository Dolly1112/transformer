{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import data_formatters.base\n",
    "import libs.utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to use CUDNN GPU optimised LSTM\n",
    "use_cudnn = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer definitions.\n",
    "concat = tf.keras.backend.concatenate\n",
    "stack = tf.keras.backend.stack\n",
    "K = tf.keras.backend\n",
    "Add = tf.keras.layers.Add\n",
    "LayerNorm = tf.keras.layers.LayerNormalization\n",
    "Dense = tf.keras.layers.Dense\n",
    "Multiply = tf.keras.layers.Multiply\n",
    "Dropout = tf.keras.layers.Dropout # Inputs elements are randomly set to zero (and the other elements are rescaled)\n",
    "Activation = tf.keras.layers.Activation\n",
    "Lambda = tf.keras.layers.Lambda\n",
    "\n",
    "# Default input types.\n",
    "InputTypes = data_formatters.base.InputTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import enum\n",
    "class DataTypes(enum.IntEnum):\n",
    "  \"\"\"Defines numerical types of each column.\"\"\"\n",
    "  REAL_VALUED = 0\n",
    "  CATEGORICAL = 1\n",
    "  DATE = 2\n",
    "\n",
    "class InputTypes(enum.IntEnum):\n",
    "  \"\"\"Defines input types of each column.\"\"\"\n",
    "  TARGET = 0\n",
    "  OBSERVED_INPUT = 1\n",
    "  KNOWN_INPUT = 2\n",
    "  STATIC_INPUT = 3\n",
    "  ID = 4  # Single column used as an entity identifier\n",
    "  TIME = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('params_update.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.1,\n",
       " 'hidden_layer_size': 5,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_gradient_norm': 100.0,\n",
       " 'minibatch_size': 128,\n",
       " 'model_folder': '0615_result',\n",
       " 'num_heads': 4,\n",
       " 'stack_size': 1,\n",
       " 'total_time_steps': 120,\n",
       " 'num_encoder_steps': 90,\n",
       " 'num_epochs': 1,\n",
       " 'early_stopping_patience': 5,\n",
       " 'multiprocessing_workers': 5,\n",
       " 'column_definition': [('traj_id',\n",
       "   <DataTypes.REAL_VALUED: 0>,\n",
       "   <InputTypes.ID: 4>),\n",
       "  ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>),\n",
       "  ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>),\n",
       "  ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)],\n",
       " 'input_size': 20,\n",
       " 'output_size': 1,\n",
       " 'category_counts': [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4],\n",
       " 'input_obs_loc': [0],\n",
       " 'static_input_loc': [6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " 'known_regular_inputs': [3, 4, 5],\n",
       " 'known_categorical_inputs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "time_steps = int(params['total_time_steps'])\n",
    "input_size = int(params['input_size'])\n",
    "output_size = int(params['output_size'])\n",
    "category_counts = json.loads(str(params['category_counts'])) \n",
    "n_multiprocessing_workers = int(params['multiprocessing_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant indices for TFT\n",
    "_input_obs_loc = json.loads(str(params['input_obs_loc']))\n",
    "_static_input_loc = json.loads(str(params['static_input_loc']))\n",
    "_known_regular_input_idx = json.loads(\n",
    "        str(params['known_regular_inputs']))\n",
    "_known_categorical_input_idx = json.loads(\n",
    "        str(params['known_categorical_inputs']))\n",
    "    # json.loads: parse a valid JSON string and convert it into a Python Dictionary\n",
    "column_definition = params['column_definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network params\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "use_cudnn = True  # Whether to use GPU optimised LSTM\n",
    "hidden_layer_size = int(params['hidden_layer_size'])\n",
    "dropout_rate = float(params['dropout_rate'])\n",
    "max_gradient_norm = float(params['max_gradient_norm'])\n",
    "learning_rate = float(params['learning_rate'])\n",
    "minibatch_size = int(params['minibatch_size'])\n",
    "num_epochs = int(params['num_epochs'])\n",
    "early_stopping_patience = int(params['early_stopping_patience'])\n",
    "\n",
    "num_encoder_steps = int(params['num_encoder_steps'])\n",
    "num_stacks = int(params['stack_size'])\n",
    "num_heads = int(params['num_heads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting temp folder...\n",
      "# dropout_rate = 0.1\n",
      "# hidden_layer_size = 5\n",
      "# learning_rate = 0.001\n",
      "# max_gradient_norm = 100.0\n",
      "# minibatch_size = 128\n",
      "# model_folder = 0615_result\n",
      "# num_heads = 4\n",
      "# stack_size = 1\n",
      "# total_time_steps = 120\n",
      "# num_encoder_steps = 90\n",
      "# num_epochs = 1\n",
      "# early_stopping_patience = 5\n",
      "# multiprocessing_workers = 5\n",
      "# column_definition = [('traj_id', <DataTypes.REAL_VALUED: 0>, <InputTypes.ID: 4>), ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>), ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)]\n",
      "# input_size = 20\n",
      "# output_size = 1\n",
      "# category_counts = [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4]\n",
      "# input_obs_loc = [0]\n",
      "# static_input_loc = [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "# known_regular_inputs = [3, 4, 5]\n",
      "# known_categorical_inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Serialisation options\n",
    "_temp_folder = os.path.join(params['model_folder'], 'tmp')\n",
    "print('Resetting temp folder...')\n",
    "utils.create_folder_if_not_exist(_temp_folder)\n",
    "shutil.rmtree(_temp_folder)\n",
    "os.makedirs(_temp_folder)\n",
    "\n",
    "# Extra components to store Tensorflow nodes for attention computations\n",
    "_input_placeholder = None\n",
    "_attention_components = None\n",
    "_prediction_parts = None\n",
    "\n",
    "#print('*** {} params ***'.format())\n",
    "for k in params:\n",
    "    print('# {} = {}'.format(k, params[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "num_categorical_variables = len(category_counts) # num_categorical_variables=14\n",
    "num_regular_variables = input_size - num_categorical_variables # num_regular_variables=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def _build_base_graph(self):\n",
    "# Inputs.\n",
    "all_inputs = tf.compat.v1.keras.layers.Input(\n",
    "        shape=(\n",
    "            time_steps, \n",
    "            input_size,\n",
    "        )) # shape=(None, 120, 20)\n",
    "regular_inputs, categorical_inputs \\\n",
    "        = all_inputs[:, :, :num_regular_variables],\\\n",
    "          all_inputs[:, :, num_regular_variables:]\n",
    "# regular_inputs: shape=(None, 120, 6)\n",
    "# categorical_inputs: shape=(None, 120, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Embedding for Numerical variables\n",
    "def convert_real_to_embedding(x):\n",
    "      \"\"\"Applies linear transformation for time-varying inputs.\"\"\"\n",
    "      return tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "              x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\TFT\\sep_venv\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\TFT\\sep_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "## (2) Embedding for categorical variables\n",
    "embedding_sizes = [\n",
    "        hidden_layer_size for i, size in enumerate(category_counts)\n",
    "    ] # [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n",
    "embeddings = []\n",
    "for i in range(num_categorical_variables):\n",
    "      embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer([time_steps]),\n",
    "          tf.keras.layers.Embedding(\n",
    "              category_counts[i], # input_dim, 'category_counts': [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4]\n",
    "              embedding_sizes[i], # output_dim , embedding_sizes[i]=hidden_layer_size\n",
    "              input_length=time_steps,\n",
    "              dtype=tf.float32)\n",
    "      ]) \n",
    "      embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x1467fc453c8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fc820b8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fc8bcf8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fc9ca20>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcad748>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcb94a8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fc8c198>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcd6080>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcdeba8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcef898>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fcf9550>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fd0b2b0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fd14e80>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x1467fd23b38>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.layers.InputLayer([time_steps]) defines an input layer for a neural network model. \n",
    "\n",
    "The time_steps parameter specifies the shape of the input data, indicating how many time steps the model will process at once, which is especially useful for sequence data like time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_inputs转化为embedded_inputs\n",
    "### categorical_inputs: shape=(None, 120, 14)\n",
    "### embedded_inputs: 14个shape=(None, 120, 5)\n",
    "embedded_inputs = [\n",
    "        embeddings[i](categorical_inputs[Ellipsis, i]) \n",
    "        for i in range(num_categorical_variables) # # num_categorical_variables=14\n",
    "    ] \n",
    "# embeddings[i]：第i个变量的映射函数\n",
    "# categorical_inputs[Ellipsis, i]: 第i个变量，(None, 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sequential/embedding/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_1/embedding_1/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_2/embedding_2/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_3/embedding_3/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_4/embedding_4/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_5/embedding_5/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_6/embedding_6/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_7/embedding_7/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_8/embedding_8/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_9/embedding_9/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_10/embedding_10/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_11/embedding_11/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_12/embedding_12/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_13/embedding_13/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when a list of keras tensors that each has 3-dim, if we want to concat along last axis:\n",
    "def keras_concat_last_axis(t1):\n",
    "    temp_concat_swapaxes=[]\n",
    "    for i in np.arange(len(t1)):\n",
    "        temp_concat_swapaxes.append(tf.keras.ops.swapaxes((t1)[i],1,2))\n",
    "    stack_temp = tf.keras.ops.hstack(temp_concat_swapaxes)\n",
    "    result = tf.keras.ops.swapaxes(stack_temp,1,2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Static inputs: shape=(None, 9, 5)\n",
    "# static_inputs = [Dense(for each regular_inputs) + embedded_inputs]\n",
    "if _static_input_loc:\n",
    "      static_inputs = [tf.keras.layers.Dense(hidden_layer_size)(\n",
    "          regular_inputs[:, 0, i:i + 1]) for i in range(num_regular_variables) # dim(1)=0\n",
    "                       if i in _static_input_loc] \\\n",
    "            + [embedded_inputs[i][:, 0, :] # dim(1)=0\n",
    "             for i in range(num_categorical_variables)\n",
    "             if i + num_regular_variables in _static_input_loc]\n",
    "      \n",
    "      static_inputs = stack(static_inputs, axis=1) # concat base on axis=1\n",
    "\n",
    "else:\n",
    "      static_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(?, 9, 5) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'strided_slice_25:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_26:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_27:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_28:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_29:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_30:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_31:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_32:0' shape=(?, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_33:0' shape=(?, 5) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_categorical_variables):\n",
    "    if i + num_regular_variables in _static_input_loc:\n",
    "        #static_inputs = [embedded_inputs[i][:, 0, :]]\n",
    "        print([embedded_inputs[i][:, 0, :]])\n",
    "        #static_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_real_to_embedding(x):\n",
    "      \"\"\"Applies linear transformation for time-varying inputs.\"\"\"\n",
    "      return tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "              x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Targets: shape=(None, 120, 5, 1)\n",
    "# obs_inputs = TimeDistributed(Dense())(regular_inputs)  \n",
    "obs_inputs = tf.keras.backend.stack([\n",
    "        convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1]) # for ith regular variables\n",
    "        for i in _input_obs_loc\n",
    "    ],axis=-1)\n",
    "# regular_inputs[Ellipsis, i:i + 1]: (None, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_1/Reshape_1:0\", shape=(?, 120, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in _input_obs_loc:\n",
    "\n",
    "    print(tf.keras.layers.TimeDistributed(\n",
    "\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "            \n",
    "              regular_inputs[Ellipsis, 0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed (a priori unknown) inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Observed (a priori unknown) inputs: shape=(None, 120, 5, 2)\n",
    "# unknown_inputs = unknown_inputs + wired_embeddings\n",
    "wired_embeddings = [] # for categorical & not belongs to known & not belongs to _input_obs_loc\n",
    "for i in range(num_categorical_variables):\n",
    "      if i not in _known_categorical_input_idx \\\n",
    "        and  i + num_regular_variables  not in _input_obs_loc:\n",
    "        e = embeddings[i](categorical_inputs[:, :, i])\n",
    "        wired_embeddings.append(e)\n",
    "\n",
    "unknown_inputs_temp = [] # for regular_inputs & not belongs to known & not belongs to _input_obs_loc\n",
    "for i in range(regular_inputs.shape[-1]):\n",
    "      if i not in _known_regular_input_idx \\\n",
    "          and i not in _input_obs_loc:\n",
    "        e = convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "        unknown_inputs_temp.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unknown_inputs_temp + wired_embeddings:\n",
    "      unknown_inputs = tf.keras.backend.stack(unknown_inputs_temp + wired_embeddings, axis=-1)\n",
    "else:\n",
    "      unknown_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_2:0' shape=(?, 120, 5, 2) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'time_distributed_2/Reshape_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_3/Reshape_1:0' shape=(?, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_inputs_temp + wired_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A priori known inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) A priori known inputs\n",
    "    # known_combined_layer = known_regular_inputs + known_categorical_inputs\n",
    "\n",
    "known_regular_inputs = [ # for _known_regular & not belongs to _static_input\n",
    "        convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "        for i in _known_regular_input_idx\n",
    "        if i not in _static_input_loc\n",
    "    ] \n",
    "known_categorical_inputs = [ # for _known_categorical & & not belongs to _static_input\n",
    "        embedded_inputs[i]\n",
    "        for i in _known_categorical_input_idx\n",
    "        if i + num_regular_variables not in _static_input_loc\n",
    "    ]\n",
    "\n",
    "known_combined_layer = tf.keras.backend.stack(known_regular_inputs + known_categorical_inputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_3:0' shape=(?, 120, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_combined_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'time_distributed_4/Reshape_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_5/Reshape_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_6/Reshape_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_9/embedding_9/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_10/embedding_10/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_11/embedding_11/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_12/embedding_12/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_13/embedding_13/embedding_lookup/Identity_1:0' shape=(?, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_regular_inputs + known_categorical_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate known and observed historical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_41:0' shape=(?, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_combined_layer[:, num_encoder_steps:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate known and observed historical inputs.\n",
    "if unknown_inputs is not None:\n",
    "      historical_inputs = concat([\n",
    "          unknown_inputs[:, :num_encoder_steps, :], # shape=(None, 90, 5, 2)\n",
    "          known_combined_layer[:, :num_encoder_steps, :], # shape=(None, 90, 5, 8)\n",
    "          obs_inputs[:, :num_encoder_steps, :] # shape=(None, 90, 5, 1)\n",
    "      ],axis=-1\n",
    "      )\n",
    "else:\n",
    "      historical_inputs = concat([\n",
    "          known_combined_layer[:, :num_encoder_steps, :],\n",
    "          obs_inputs[:, :num_encoder_steps, :]\n",
    "      ],axis=-1\n",
    "      )\n",
    "# historical_inputs.get_shape().as_list() = _, time_steps, embedding_dim, num_inputs\n",
    "\n",
    "# Isolate only known future inputs.\n",
    "future_inputs = known_combined_layer[:, num_encoder_steps:, :] # shape=(None, 30, 5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, 90, 5, 11) dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_45:0' shape=(?, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module\n",
    "#### Basic Module\n",
    "##### linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size,\n",
    "                 activation=None,\n",
    "                 use_time_distributed=False,\n",
    "                 use_bias=True):\n",
    "  \"\"\"Returns simple Keras linear layer.\n",
    "\n",
    "  Args:\n",
    "    size: Output size\n",
    "    activation: Activation function to apply if required\n",
    "    use_time_distributed: Whether to apply layer across time\n",
    "    use_bias: Whether bias should be included in layer\n",
    "  \"\"\"\n",
    "  linear = tf.keras.layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "  if use_time_distributed:\n",
    "    linear = tf.keras.layers.TimeDistributed(linear)\n",
    "  return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_and_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(x_list):\n",
    "  \"\"\"Applies skip connection followed by layer normalisation.\n",
    "\n",
    "  Args:\n",
    "    x_list: List of inputs to sum for skip connection\n",
    "\n",
    "  Returns:\n",
    "    Tensor output from layer.\n",
    "  \"\"\"\n",
    "  tmp = Add()(x_list)   #tmp = tf.keras.layers.Add(x1,x2)\n",
    "  tmp = LayerNorm()(tmp)\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mlp(inputs,\n",
    "              hidden_size,\n",
    "              output_size,\n",
    "              output_activation=None,\n",
    "              hidden_activation='tanh',\n",
    "              use_time_distributed=False):\n",
    "  \"\"\"Applies simple feed-forward network to an input.\n",
    "\n",
    "  Args:\n",
    "    inputs: MLP inputs\n",
    "    hidden_size: Hidden state size\n",
    "    output_size: Output size of MLP\n",
    "    output_activation: Activation function to apply on output\n",
    "    hidden_activation: Activation function to apply on input\n",
    "    use_time_distributed: Whether to apply across time\n",
    "\n",
    "  Returns:\n",
    "    Tensor for MLP outputs.\n",
    "  \"\"\"\n",
    "  if use_time_distributed:\n",
    "    hidden = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_size, activation=hidden_activation))(\n",
    "            inputs)\n",
    "    return tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(output_size, activation=output_activation))(\n",
    "            hidden)\n",
    "  else:\n",
    "    hidden = tf.keras.layers.Dense(\n",
    "        hidden_size, activation=hidden_activation)(\n",
    "            inputs)\n",
    "    return tf.keras.layers.Dense(\n",
    "        output_size, activation=output_activation)(\n",
    "            hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gating_layer(x,\n",
    "                       hidden_layer_size,\n",
    "                       dropout_rate=None,\n",
    "                       use_time_distributed=True,\n",
    "                       activation=None):\n",
    "  \"\"\"Applies a Gated Linear Unit (GLU) to an input.\n",
    "\n",
    "  Args:\n",
    "    x: Input to gating layer\n",
    "    hidden_layer_size: Dimension of GLU\n",
    "    dropout_rate: Dropout rate to apply if any\n",
    "    use_time_distributed: Whether to apply across time\n",
    "    activation: Activation function to apply to the linear feature transform if\n",
    "      necessary\n",
    "\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GLU output, gate)\n",
    "  \"\"\"\n",
    "  # First, dropout\n",
    "  if dropout_rate is not None:\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "  if use_time_distributed:\n",
    "    activation_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=activation))(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'))(\n",
    "            x)\n",
    "  else:\n",
    "    activation_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=activation)(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation='sigmoid')(\n",
    "            x)\n",
    "\n",
    "  return tf.keras.layers.Multiply()([activation_layer, gated_layer]), gated_layer\n",
    "  #return tf.keras.ops.multiply(activation_layer, gated_layer), gated_layer\n",
    "## output: activation_layer *  gated_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(x,   # primary input\n",
    "                           hidden_layer_size,\n",
    "                           output_size=None,\n",
    "                           dropout_rate=None,\n",
    "                           use_time_distributed=True,\n",
    "                           additional_context=None,\n",
    "                           return_gate=False):\n",
    "  \"\"\"Applies the gated residual network (GRN) as defined in paper.\n",
    "\n",
    "  Args:\n",
    "    x: Network inputs\n",
    "    hidden_layer_size: Internal state size\n",
    "    output_size: Size of output layer\n",
    "    dropout_rate: Dropout rate if dropout is applied\n",
    "    use_time_distributed: Whether to apply network across time dimension\n",
    "    additional_context: Additional context vector to use if relevant\n",
    "    return_gate: Whether to return GLU gate for diagnostic purposes\n",
    "\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GRN output, GLU gate)\n",
    "  \"\"\"\n",
    "\n",
    "  # Setup skip connection\n",
    "  if output_size is None:\n",
    "    output_size = hidden_layer_size\n",
    "    skip = x\n",
    "  else: # skip = TimeDistributed(Dense(output_size))(x)\n",
    "    linear = Dense(output_size)\n",
    "    if use_time_distributed:\n",
    "      linear = tf.keras.layers.TimeDistributed(linear)\n",
    "    skip = linear(x)\n",
    "\n",
    "\n",
    "  # Apply feedforward network\n",
    "  #### (1) Dense\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          x)\n",
    "  if additional_context is not None:\n",
    "    hidden = hidden + linear_layer(\n",
    "        hidden_layer_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=False)(\n",
    "            additional_context)\n",
    "  #### (2) ELU\n",
    "  hidden = Activation('elu')(hidden)\n",
    "  #### (3) Dense\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          hidden)\n",
    "  #### (4) Gate: GLU\n",
    "  gating_layer, gate = apply_gating_layer(\n",
    "      hidden,\n",
    "      output_size,\n",
    "      dropout_rate=dropout_rate,\n",
    "      use_time_distributed=use_time_distributed,\n",
    "      activation=None)\n",
    "  #### (5) Add & Norm\n",
    "  if return_gate:\n",
    "    return add_and_norm([skip, gating_layer]), gate\n",
    "    #return add_and_norm(skip, gating_layer), gate\n",
    "  else:\n",
    "    return add_and_norm([skip, gating_layer])\n",
    "    #return add_and_norm(skip, gating_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_static = static_inputs.shape[1] # num_static = 9\n",
    "\n",
    "# (1) sparse_weights: shape=(None, 9, 1)\n",
    "flatten = tf.keras.layers.Flatten()(static_inputs) # shape=(None, 45) 9*5=45\n",
    "mlp_outputs = gated_residual_network(    # shape=(None, 9)\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_static,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=False,\n",
    "          additional_context=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "##### Variable selection for static inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(?, 9, 5) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_inputs: shape should be (None, 9, 5)\n",
    "num_static = static_inputs.shape[1] # num_static = 9\n",
    "\n",
    "# (1) sparse_weights: shape=(None, 9, 1)\n",
    "flatten = tf.keras.layers.Flatten()(static_inputs) # shape=(None, 45) 9*5=45\n",
    "mlp_outputs = gated_residual_network(    # shape=(None, 9)\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_static,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=False,\n",
    "          additional_context=None)\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.keras.backend.expand_dims(sparse_weights, axis=-1) # tf.keras.backend.expand_dims:在某个位置多加一个维度，数值为1\n",
    "\n",
    "# (2) transformed_embedding: shape=(None, 9, 5)\n",
    "trans_emb_list = []\n",
    "for i in range(num_static):\n",
    "        e = gated_residual_network(\n",
    "            static_inputs[:, i:i + 1, :],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=False)\n",
    "        trans_emb_list.append(e) # e: shape=(None, 1, 5)\n",
    "transformed_embedding = concat(trans_emb_list, axis=1)\n",
    "\n",
    "# combined: shape=(None, 9, 5)\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding])\n",
    "static_vec = K.sum(combined, axis=1) # K = tf.keras.backend # shape=(None, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum:0\", shape=(?, 5), dtype=float32)\n",
      "Tensor(\"ExpandDims:0\", shape=(?, 9, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "static_encoder = static_vec\n",
    "static_weights = sparse_weights\n",
    "print(static_encoder)\n",
    "print(static_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static covariates Encoders(4 kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape=(None, 5), four identical static variables \n",
    "static_context_variable_selection = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_enrichment = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_state_h = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_state_c = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_13/batchnorm/add_1:0' shape=(?, 5) dtype=float32>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_context_state_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable selection for other inputs\n",
    "###### (1) Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, 90, 5, 11) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Historical data\n",
    "historical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, historical_time_steps, embedding_dim, num_inputs = historical_inputs.get_shape().as_list()\n",
    "#Wrong edition:\n",
    "# _, historical_time_steps, embedding_dim, num_inputs = tf.keras.backend.shape(historical_inputs)\n",
    "flatten = tf.keras.backend.reshape(historical_inputs,\n",
    "                          [-1, historical_time_steps, embedding_dim * num_inputs]) # shape=(None, 90, 55)\n",
    "expanded_static_context = tf.keras.backend.expand_dims(\n",
    "          static_context_variable_selection, axis=1) # shape=(None, 1, 5)\n",
    "\n",
    "# Variable selection weights\n",
    "mlp_outputs, static_gate = gated_residual_network(\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_inputs,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=True,\n",
    "          additional_context=expanded_static_context, # here\n",
    "          return_gate=True) # mlp_outputs: shape=(None, 90, 11); static_gate: shape=(None, 90, 11)\n",
    "\n",
    "\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.expand_dims(sparse_weights, axis=2) # shape=(None, 90, 1, 11)\n",
    "\n",
    "# Non-linear Processing & weight application\n",
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            historical_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 11个shape=(None, 90, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 90, 5, 11)\n",
    "\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 90, 5, 11)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1) # shape=(None, 90, 5)\n",
    "\n",
    "historical_features = temporal_ctx # shape=(None, 90, 5)\n",
    "historical_flags = sparse_weights # shape=(None, 90, 1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (2) future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_45:0' shape=(?, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) future data\n",
    "future_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            historical_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 11个shape=(None, 90, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 90, 5, 11)\n",
    "\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 90, 5, 11)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1) # shape=(None, 90, 5)\n",
    "\n",
    "historical_features = temporal_ctx # shape=(None, 90, 5)\n",
    "historical_flags = sparse_weights # shape=(None, 90, 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, future_time_steps, embedding_dim, num_inputs = future_inputs.get_shape().as_list()\n",
    "\n",
    "flatten = tf.keras.backend.reshape(future_inputs,\n",
    "                          [-1, future_time_steps, embedding_dim * num_inputs]) # shape=(None, 30, 40)\n",
    "expanded_static_context = tf.keras.backend.expand_dims(\n",
    "          static_context_variable_selection, axis=1) # shape=(None, 1, 5)\n",
    "\n",
    "# Variable selection weights\n",
    "mlp_outputs, static_gate = gated_residual_network(\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_inputs,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=True,\n",
    "          additional_context=expanded_static_context, # here\n",
    "          return_gate=True) # mlp_outputs: shape=(None, 90, 11); static_gate: shape=(None, 30，8)\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.expand_dims(sparse_weights, axis=2) # hape=(None, 30, 1, 8)\n",
    "\n",
    "# Non-linear Processing & weight application\n",
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            future_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 8个shape=(None, 30, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 30, 5, 8)\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 30, 5, 8)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1)\n",
    "\n",
    "future_features = temporal_ctx # shape=(None, 30, 5)\n",
    "future_flags = sparse_weights # shape=(None, 30, 1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) historical data\n",
    "if use_cudnn:\n",
    "    lstm = tf.keras.layers.CuDNNLSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            stateful=False,\n",
    "        )\n",
    "else:\n",
    "    lstm = tf.keras.layers.LSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=True, # diff\n",
    "            stateful=False,\n",
    "            # Additional params to ensure LSTM matches CuDNN, See TF 2.0 :\n",
    "            # (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)\n",
    "\n",
    "history_lstm, state_h, state_c \\\n",
    "    = lstm(historical_features,initial_state=[static_context_state_h, # h_0\n",
    "                                               static_context_state_c]) # c_0\n",
    "                                               \n",
    "# history_lstm: shape=(None, 90, 5)\n",
    "# state_h: shape=(None, 5)\n",
    "# state_c: shape=(None, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_85:0' shape=(5,) dtype=float32>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) future data\n",
    "if use_cudnn:\n",
    "    lstm = tf.keras.layers.CuDNNLSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False,\n",
    "            stateful=False,\n",
    "        )\n",
    "else:\n",
    "      lstm = tf.keras.layers.LSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False, # diff\n",
    "            stateful=False,\n",
    "            # Additional params to ensure LSTM matches CuDNN, See TF 2.0 :\n",
    "            # (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)\n",
    "\n",
    "future_lstm = lstm(\n",
    "        future_features, \n",
    "        initial_state=[state_h, state_c]\n",
    "        )\n",
    "# future_lstm: shape=(None, 30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) combine\n",
    "lstm_layer = tf.keras.backend.concatenate([history_lstm, future_lstm], axis=1) # shape=(None, 120, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply gated skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = tf.keras.backend.concatenate([historical_features, future_features], axis=1) # shape=(None, 120, 5)\n",
    "\n",
    "lstm_layer, _ = apply_gating_layer(\n",
    "        lstm_layer, hidden_layer_size, dropout_rate, activation=None) # shape=(None, 120, 5)\n",
    "temporal_feature_layer = add_and_norm([lstm_layer, input_embeddings]) # shape=(None, 120, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static enrichment layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add static_context_enrichment\n",
    "expanded_static_context = tf.expand_dims(static_context_enrichment, axis=1) # (None, 1, 5) \n",
    "enriched, _ = gated_residual_network(\n",
    "        temporal_feature_layer,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=True,\n",
    "        additional_context=expanded_static_context,\n",
    "        return_gate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_head, d_model, dropout\n",
    "(num_heads, hidden_layer_size, dropout=dropout_rate)\n",
    "q, k, v, mask=None\n",
    "(enriched, enriched, enriched,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal mask to apply for self-attention layer\n",
    "len_s = tf.shape(enriched)[1]  # 120\n",
    "bs = tf.shape(enriched)[:1] # (None,)\n",
    "mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1) # shape=(120, 120)\n",
    "\n",
    "d_k = d_v = hidden_layer_size // num_heads  # 5//4\n",
    "qs_layers = []\n",
    "ks_layers = []\n",
    "vs_layers = []\n",
    "\n",
    "# Use same value layer to facilitate interp\n",
    "vs_layer = Dense(d_v, use_bias=False)\n",
    "\n",
    "for _ in range(num_heads):\n",
    "  qs_layers.append(Dense(d_k, use_bias=False)) # since # of queries = # of keys\n",
    "  ks_layers.append(Dense(d_k, use_bias=False)) # output_dim = d_k\n",
    "  vs_layers.append(Dense(d_v, use_bias=False))  # output_dim = d_v\n",
    "\n",
    "#attention = ScaledDotProductAttention()\n",
    "heads = []\n",
    "attns = []\n",
    "for i in range(num_heads):\n",
    "  qs = qs_layers[i](enriched) # Q, (None, 120, 1)\n",
    "  ks = ks_layers[i](enriched)\n",
    "  vs = vs_layers[i](enriched)\n",
    "\n",
    "  #head, attn = attention(qs, ks, vs, mask)\n",
    "  temper = tf.sqrt(tf.cast(tf.shape(ks)[-1], dtype='float32')) # cast: type conversion # shape=()\n",
    "  # tf.keras.ops.shape(ks)[-1] refers to d_attn(in the paper) = 1\n",
    "  \n",
    "  ###################### (QK^T)/sqrt(d_attn) #########################\n",
    "  attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [qs, ks])\n",
    "  # equals to:\n",
    "  # K.batch_dot(qs, ks, axes=[2, 2]) / temper\n",
    "  # output_shape=(None, 120, 120)\n",
    "  # # shape=(batch, q, k)\n",
    "\n",
    "\n",
    "  if mask is not None:\n",
    "      mmask = Lambda(lambda x: (-1e+9) * (1. - tf.cast(x, 'float32')))(\n",
    "          mask)  # setting to infinity, # 将 mask 的位置设为一个很大的负数，softmax 后趋近于0\n",
    "      attn = Add()([attn, mmask])\n",
    "\n",
    "  attn = Dropout(0.0)(Activation('softmax')(attn)) # shape=(None, 120, 120)\n",
    "  # Dropout(0.0): 不丢弃任何神经元，相当于没有起作用，网络将正常传递所有的神经元输出。\n",
    "\n",
    "  ###################### Attention(Q, K, V) = A(Q, K)V #########################\n",
    "  head_temp = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, vs]) # shape=(None, 120, 1)\n",
    "  # Wrong edition:\n",
    "  # head_temp = tf.expand_dims(K.batch_dot(attn, v[0,:]),axis=0) # shape=(1, 120, 5)\n",
    "  # equals to:\n",
    "  # output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, enriched])\n",
    "\n",
    "  head_dropout = Dropout(dropout_rate)(head_temp) # shape=(None, 120, 1)\n",
    "  \n",
    "  \n",
    "  heads.append(head_dropout) # 4个shape=(1, 120, 5)\n",
    "  attns.append(attn) # 4个shape=(1, 120, 120)\n",
    "  \n",
    "###################### Multi-Head #########################\n",
    "head = tf.keras.backend.stack(heads) if num_heads > 1 else heads[0]\n",
    "\n",
    "# Result 2: self_att      shape=(4, 1, 120, 120)\n",
    "self_att = tf.keras.backend.stack(attns)\n",
    "\n",
    "# Result 1: x   shape=(None, 120, 5)\n",
    "outputs = K.mean(head, axis=0) if num_heads > 1 else head # (None, 120, 1)\n",
    "w_o = Dense(hidden_layer_size, use_bias=False)\n",
    "outputs = w_o(outputs) # input_dim=d_model; output_dim=K.mean(head, axis=0)\n",
    "outputs = Dropout(dropout_rate)(outputs)  # output dropout\n",
    "x = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_86:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"layer_normalization_49/batchnorm/add_1:0\", shape=(?, 120, 5), dtype=float32)\n",
      "Tensor(\"layer_normalization_48/batchnorm/add_1:0\", shape=(?, 120, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x, _ = apply_gating_layer(\n",
    "        x,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=None)\n",
    "x = add_and_norm([x, enriched])\n",
    "print(x)\n",
    "print(enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder self attention\n",
    "x, self_att \\\n",
    "        = InterpretableMultiHeadAttention(\n",
    "        num_heads, hidden_layer_size, dropout=dropout_rate)(enriched, enriched, enriched,\n",
    "                          mask=mask)\n",
    "class InterpretableMultiHeadAttention():\n",
    "\n",
    "    self.d_k = self.d_v = d_k = d_v = hidden_layer_size // num_heads\n",
    "    self.qs_layers = []\n",
    "    self.ks_layers = []\n",
    "    self.vs_layers = []\n",
    "\n",
    "    # Use same value layer to facilitate interp\n",
    "    vs_layer = Dense(d_v, use_bias=False)\n",
    "\n",
    "    for _ in range(n_head):\n",
    "      self.qs_layers.append(Dense(d_k, use_bias=False)) # since # of queries = # of keys\n",
    "      self.ks_layers.append(Dense(d_k, use_bias=False)) # output_dim = d_k\n",
    "      self.vs_layers.append(Dense(d_v, use_bias=False))  # output_dim = d_v\n",
    "\n",
    "\n",
    "    heads = []\n",
    "    attns = []\n",
    "    for i in range(n_head):\n",
    "      qs = self.qs_layers[i](enriched) # input_dim=q; output_dim = d_k\n",
    "      ks = self.ks_layers[i](enriched)\n",
    "      vs = self.vs_layers[i](enriched)\n",
    "      head, attn = ScaledDotProductAttention()(qs, ks, vs, mask)\n",
    "      head_dropout = Dropout(self.dropout)(head)\n",
    "      \n",
    "      heads.append(head_dropout)\n",
    "      attns.append(attn)\n",
    "\n",
    "\n",
    "    temper = tf.sqrt(tf.cast(tf.shape(ks)[-1], dtype='float32'))\n",
    "    attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [qs, ks])  # shape=(batch, q, k)\n",
    "    if mask is not None:\n",
    "      mmask = Lambda(lambda x: (-1e+9) * (1. - K.cast(x, 'float32')))(\n",
    "          mask)  # setting to infinity\n",
    "      attn = Add()([attn, mmask])\n",
    "    attn = Activation('softmax')(attn)\n",
    "    attn = Dropout(attn_dropout=0.0)(attn)\n",
    "    output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, vs])\n",
    "\n",
    "head = K.stack(heads) if n_head > 1 else heads[0]\n",
    "    attn = K.stack(attns)\n",
    "\n",
    "    outputs = K.mean(head, axis=0) if n_head > 1 else head\n",
    "    outputs = Dense(d_model, use_bias=False)(outputs) # input_dim=d_model; output_dim=K.mean(head, axis=0)\n",
    "    outputs = Dropout(self.dropout)(outputs)  # output dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Position-wise Feed-forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_50/batchnorm/add_1:0' shape=(?, 120, 5) dtype=float32>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = gated_residual_network(\n",
    "        x,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=True)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final skip connection\n",
    "decoder, _ = apply_gating_layer(\n",
    "        decoder, hidden_layer_size, activation=None)\n",
    "transformer_layer = add_and_norm([decoder, temporal_feature_layer])\n",
    "\n",
    "# Attention components for explainability\n",
    "attention_components = {\n",
    "        # Temporal attention weights\n",
    "        'decoder_self_attn': self_att,\n",
    "        # Static variable selection weights\n",
    "        'static_flags': static_weights[Ellipsis, 0],\n",
    "        # Variable selection weights of past inputs\n",
    "        'historical_flags': historical_flags[Ellipsis, 0, :],\n",
    "        # Variable selection weights of future inputs\n",
    "        'future_flags': future_flags[Ellipsis, 0, :]\n",
    "    }\n",
    "\n",
    "# what we want:\n",
    "# transformer_layer: shape=(None, 120, 5)\n",
    "# all_inputs\n",
    "# attention_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_51/batchnorm/add_1:0' shape=(?, 120, 5) dtype=float32>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_self_attn': <tf.Tensor 'stack_8:0' shape=(4, ?, 120, 120) dtype=float32>,\n",
       " 'static_flags': <tf.Tensor 'strided_slice_92:0' shape=(?, 9) dtype=float32>,\n",
       " 'historical_flags': <tf.Tensor 'strided_slice_93:0' shape=(?, 90, 11) dtype=float32>,\n",
       " 'future_flags': <tf.Tensor 'strided_slice_94:0' shape=(?, 30, 8) dtype=float32>}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention_components =  attention_components\n",
    "attention_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense\n",
    "outputs = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(output_size * len(quantiles))) \\\n",
    "          (transformer_layer[Ellipsis, num_encoder_steps:, :])\n",
    "# shape=(None, 30, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31212\\960101334.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#_attention_components = attention_components\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtft_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# all_inputs: ([None, 120, 20])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtft_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#_attention_components = attention_components\n",
    "tft_model = tf.keras.Model(inputs=all_inputs, outputs=outputs) # all_inputs: ([None, 120, 20])\n",
    "print(tft_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_quantiles = quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLossCalculator(object):\n",
    "        \"\"\"Computes the combined quantile loss for prespecified quantiles.\n",
    "\n",
    "        Attributes:\n",
    "          quantiles: Quantiles to compute losses\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, quantiles):\n",
    "          \"\"\"Initializes computer with quantiles for loss calculations.\n",
    "\n",
    "          Args:\n",
    "            quantiles: Quantiles to use for computations.\n",
    "          \"\"\"\n",
    "          \n",
    "\n",
    "        def quantile_loss(self, a, b):\n",
    "          \"\"\"Returns quantile loss for specified quantiles.\n",
    "\n",
    "          Args:\n",
    "            a: Targets\n",
    "            b: Predictions\n",
    "          \"\"\"\n",
    "          quantiles_used = set(quantiles)\n",
    "\n",
    "          loss = 0.\n",
    "          for i, quantile in enumerate(valid_quantiles):\n",
    "            if quantile in quantiles_used:\n",
    "              loss += utils.tensorflow_quantile_loss(\n",
    "                  a[Ellipsis, output_size * i:output_size * (i + 1)],\n",
    "                  b[Ellipsis, output_size * i:output_size * (i + 1)], quantile)\n",
    "          return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_loss = QuantileLossCalculator(valid_quantiles).quantile_loss\n",
    "adam = tf.keras.optimizers.Adam(\n",
    "          learning_rate=learning_rate, clipnorm=max_gradient_norm)\n",
    "tft_model.compile(\n",
    "          loss=quantile_loss, optimizer=adam, sample_weight_mode='temporal')\n",
    "# model.compile()方法用于在配置训练方法时，告知训练时用的优化器、损失函数和准确率评测标准\n",
    "# sample_weight_mode='temporal' is very important, we need to assign different weight for each timestamp\n",
    "\n",
    "_input_placeholder = all_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_model(self):\n",
    "    \"\"\"Build model and defines training losses.\n",
    "\n",
    "    Returns:\n",
    "      Fully defined Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    #with tf.variable_scope(name):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "\n",
    "      transformer_layer, all_inputs, attention_components \\\n",
    "          = _build_base_graph()\n",
    "      \n",
    "      # Dense\n",
    "      outputs = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(output_size * len(quantiles))) \\\n",
    "          (transformer_layer[Ellipsis, num_encoder_steps:, :])\n",
    "\n",
    "      _attention_components = attention_components\n",
    "\n",
    "      adam = tf.keras.optimizers.Adam(\n",
    "          learning_rate=learning_rate, clipnorm=max_gradient_norm)\n",
    "\n",
    "      model = tf.keras.Model(inputs=all_inputs, outputs=outputs)\n",
    "\n",
    "      print(model.summary())\n",
    "\n",
    "      valid_quantiles = quantiles\n",
    "      output_size = output_size\n",
    "      \n",
    "      quantile_loss = QuantileLossCalculator(valid_quantiles).quantile_loss\n",
    "\n",
    "      model.compile(\n",
    "          loss=quantile_loss, optimizer=adam, sample_weight_mode='temporal')\n",
    "\n",
    "      _input_placeholder = all_inputs\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample training & validating data\n",
    "##### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_tf.csv')\n",
    "valid = pd.read_csv('valid_tf.csv')\n",
    "test = pd.read_csv('test_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>open</th>\n",
       "      <th>date</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>...</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>transactions</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>national_hol</th>\n",
       "      <th>regional_hol</th>\n",
       "      <th>local_hol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-05 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>0.186498</td>\n",
       "      <td>0.989231</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519535</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-06 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>0.587660</td>\n",
       "      <td>1.037158</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.715138</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-07 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-07</td>\n",
       "      <td>-0.378907</td>\n",
       "      <td>-1.486235</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.253098</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>-1.345475</td>\n",
       "      <td>1.014551</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620596</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-10 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>-0.378907</td>\n",
       "      <td>0.966172</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr  item_nbr  unit_sales  onpromotion   traj_id  \\\n",
       "0          0         0         3.0            2  1_103520   \n",
       "1          0         0         4.0            2  1_103520   \n",
       "2          0         0         2.0            2  1_103520   \n",
       "3          0         0         1.0            2  1_103520   \n",
       "4          0         0         2.0            2  1_103520   \n",
       "\n",
       "                      unique_id  open        date  log_sales       oil  ...  \\\n",
       "0  1_103520_2013-09-05 00:00:00   1.0  2013-09-05   0.186498  0.989231  ...   \n",
       "1  1_103520_2013-09-06 00:00:00   1.0  2013-09-06   0.587660  1.037158  ...   \n",
       "2  1_103520_2013-09-07 00:00:00   1.0  2013-09-07  -0.378907 -1.486235  ...   \n",
       "3  1_103520_2013-09-09 00:00:00   1.0  2013-09-09  -1.345475  1.014551  ...   \n",
       "4  1_103520_2013-09-10 00:00:00   1.0  2013-09-10  -0.378907  0.966172  ...   \n",
       "\n",
       "   family  class  perishable  transactions  day_of_week  day_of_month  month  \\\n",
       "0       1      0           0      0.519535            3             5      9   \n",
       "1       1      0           0      0.715138            4             6      9   \n",
       "2       1      0           0     -0.253098            5             7      9   \n",
       "3       1      0           0      0.620596            0             9      9   \n",
       "4       1      0           0      0.452703            1            10      9   \n",
       "\n",
       "   national_hol  regional_hol  local_hol  \n",
       "0            34             1          3  \n",
       "1            34             1          3  \n",
       "2            34             1          3  \n",
       "3            34             1          3  \n",
       "4            34             1          3  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     store_nbr  item_nbr  unit_sales  onpromotion    traj_id  \\\n",
      "0            0         0         2.0            2   1_103520   \n",
      "1            0         0         3.0            2   1_103520   \n",
      "2            0         0         2.0            2   1_103520   \n",
      "3            0         0         6.0            2   1_103520   \n",
      "4            0         0         3.0            2   1_103520   \n",
      "..         ...       ...         ...          ...        ...   \n",
      "916          1         1         3.0            2  25_103665   \n",
      "917          1         1         2.0            2  25_103665   \n",
      "918          1         1         5.0            2  25_103665   \n",
      "919          1         1         4.0            2  25_103665   \n",
      "920          1         1         5.0            2  25_103665   \n",
      "\n",
      "                         unique_id  open        date  log_sales       oil  \\\n",
      "0     1_103520_2013-01-04 00:00:00   1.0  2013-01-04  -0.378907  0.641536   \n",
      "1     1_103520_2013-01-05 00:00:00   1.0  2013-01-05   0.186498 -1.486235   \n",
      "2     1_103520_2013-01-07 00:00:00   1.0  2013-01-07  -0.378907  0.643344   \n",
      "3     1_103520_2013-01-08 00:00:00   1.0  2013-01-08   1.153066  0.643570   \n",
      "4     1_103520_2013-01-09 00:00:00   1.0  2013-01-09   0.186498  0.640631   \n",
      "..                             ...   ...         ...        ...       ...   \n",
      "916  25_103665_2013-11-24 00:00:00   1.0  2013-11-24   0.186498 -1.486235   \n",
      "917  25_103665_2013-11-26 00:00:00   1.0  2013-11-26  -0.378907  0.648092   \n",
      "918  25_103665_2013-11-27 00:00:00   1.0  2013-11-27   0.898826  0.617346   \n",
      "919  25_103665_2013-11-28 00:00:00   1.0  2013-11-28   0.587660  0.617346   \n",
      "920  25_103665_2013-11-29 00:00:00   1.0  2013-11-29   0.898826  0.628650   \n",
      "\n",
      "     ...  family  class  perishable  transactions  day_of_week  day_of_month  \\\n",
      "0    ...       1      0           0      0.754258            4             4   \n",
      "1    ...       1      0           0      0.177229            5             5   \n",
      "2    ...       1      0           0      0.662977            0             7   \n",
      "3    ...       1      0           0      0.764038            1             8   \n",
      "4    ...       1      0           0      0.830870            2             9   \n",
      "..   ...     ...    ...         ...           ...          ...           ...   \n",
      "916  ...       0      2           1     -1.224594            6            24   \n",
      "917  ...       0      2           1     -1.363146            1            26   \n",
      "918  ...       0      2           1     -1.157762            2            27   \n",
      "919  ...       0      2           1     -1.332175            3            28   \n",
      "920  ...       0      2           1     -0.944229            4            29   \n",
      "\n",
      "     month  national_hol  regional_hol  local_hol  \n",
      "0        1            34             1          3  \n",
      "1        1            30             1          3  \n",
      "2        1            34             1          3  \n",
      "3        1            34             1          3  \n",
      "4        1            34             1          3  \n",
      "..     ...           ...           ...        ...  \n",
      "916     11            34             1          3  \n",
      "917     11            34             1          3  \n",
      "918     11            34             1          3  \n",
      "919     11            34             1          3  \n",
      "920     11            34             1          3  \n",
      "\n",
      "[921 rows x 24 columns]\n",
      "     store_nbr  item_nbr  unit_sales  onpromotion    traj_id  \\\n",
      "0            0         0         1.0            2   1_103520   \n",
      "1            0         0         2.0            2   1_103520   \n",
      "2            0         0         1.0            2   1_103520   \n",
      "3            0         0         1.0            2   1_103520   \n",
      "4            0         0         1.0            2   1_103520   \n",
      "..         ...       ...         ...          ...        ...   \n",
      "355          1         1         6.0            2  25_103665   \n",
      "356          1         1         3.0            2  25_103665   \n",
      "357          1         1         3.0            2  25_103665   \n",
      "358          1         1         3.0            2  25_103665   \n",
      "359          1         1         6.0            2  25_103665   \n",
      "\n",
      "                         unique_id  open        date  log_sales       oil  \\\n",
      "0     1_103520_2013-07-23 00:00:00   1.0  2013-07-23  -1.345475  0.958260   \n",
      "1     1_103520_2013-07-24 00:00:00   1.0  2013-07-24  -0.378907  0.919376   \n",
      "2     1_103520_2013-07-25 00:00:00   1.0  2013-07-25  -1.345475  0.920732   \n",
      "3     1_103520_2013-07-26 00:00:00   1.0  2013-07-26  -1.345475  0.904681   \n",
      "4     1_103520_2013-07-31 00:00:00   1.0  2013-07-31  -1.345475  0.912367   \n",
      "..                             ...   ...         ...        ...       ...   \n",
      "355  25_103665_2014-01-03 00:00:00   1.0  2014-01-03   1.153066  0.653743   \n",
      "356  25_103665_2014-01-07 00:00:00   1.0  2014-01-07   0.186498  0.645831   \n",
      "357  25_103665_2014-01-08 00:00:00   1.0  2014-01-08   0.186498  0.613955   \n",
      "358  25_103665_2014-01-09 00:00:00   1.0  2014-01-09   0.186498  0.601747   \n",
      "359  25_103665_2014-01-10 00:00:00   1.0  2014-01-10   1.153066  0.625033   \n",
      "\n",
      "     ...  family  class  perishable  transactions  day_of_week  day_of_month  \\\n",
      "0    ...       1      0           0      0.641786            1            23   \n",
      "1    ...       1      0           0      0.645047            2            24   \n",
      "2    ...       1      0           0      0.483674            3            25   \n",
      "3    ...       1      0           0      0.664607            4            26   \n",
      "4    ...       1      0           0      0.697207            2            31   \n",
      "..   ...     ...    ...         ...           ...          ...           ...   \n",
      "355  ...       0      2           1      0.053347            4             3   \n",
      "356  ...       0      2           1     -1.138202            1             7   \n",
      "357  ...       0      2           1     -1.038771            2             8   \n",
      "358  ...       0      2           1     -1.081151            3             9   \n",
      "359  ...       0      2           1     -0.680165            4            10   \n",
      "\n",
      "     month  national_hol  regional_hol  local_hol  \n",
      "0        7            34             1          3  \n",
      "1        7            34             1          3  \n",
      "2        7            34             1          3  \n",
      "3        7            34             1          3  \n",
      "4        7            34             1          3  \n",
      "..     ...           ...           ...        ...  \n",
      "355      1            24             1          3  \n",
      "356      1            34             1          3  \n",
      "357      1            34             1          3  \n",
      "358      1            34             1          3  \n",
      "359      1            34             1          3  \n",
      "\n",
      "[360 rows x 24 columns]\n",
      "     store_nbr  item_nbr  unit_sales  onpromotion    traj_id  \\\n",
      "0            0         0         3.0            2   1_103520   \n",
      "1            0         0         4.0            2   1_103520   \n",
      "2            0         0         2.0            2   1_103520   \n",
      "3            0         0         1.0            2   1_103520   \n",
      "4            0         0         2.0            2   1_103520   \n",
      "..         ...       ...         ...          ...        ...   \n",
      "355          1         1         4.0            2  25_103665   \n",
      "356          1         1         1.0            2  25_103665   \n",
      "357          1         1         9.0            2  25_103665   \n",
      "358          1         1         5.0            2  25_103665   \n",
      "359          1         1         4.0            2  25_103665   \n",
      "\n",
      "                         unique_id  open        date  log_sales       oil  \\\n",
      "0     1_103520_2013-09-05 00:00:00   1.0  2013-09-05   0.186498  0.989231   \n",
      "1     1_103520_2013-09-06 00:00:00   1.0  2013-09-06   0.587660  1.037158   \n",
      "2     1_103520_2013-09-07 00:00:00   1.0  2013-09-07  -0.378907 -1.486235   \n",
      "3     1_103520_2013-09-09 00:00:00   1.0  2013-09-09  -1.345475  1.014551   \n",
      "4     1_103520_2013-09-10 00:00:00   1.0  2013-09-10  -0.378907  0.966172   \n",
      "..                             ...   ...         ...        ...       ...   \n",
      "355  25_103665_2014-02-12 00:00:00   1.0  2014-02-12   0.587660  0.805662   \n",
      "356  25_103665_2014-02-13 00:00:00   1.0  2014-02-13  -1.345475  0.803176   \n",
      "357  25_103665_2014-02-14 00:00:00   1.0  2014-02-14   1.718472  0.804080   \n",
      "358  25_103665_2014-02-15 00:00:00   1.0  2014-02-15   0.898826 -1.486235   \n",
      "359  25_103665_2014-02-16 00:00:00   1.0  2014-02-16   0.587660 -1.486235   \n",
      "\n",
      "     ...  family  class  perishable  transactions  day_of_week  day_of_month  \\\n",
      "0    ...       1      0           0      0.519535            3             5   \n",
      "1    ...       1      0           0      0.715138            4             6   \n",
      "2    ...       1      0           0     -0.253098            5             7   \n",
      "3    ...       1      0           0      0.620596            0             9   \n",
      "4    ...       1      0           0      0.452703            1            10   \n",
      "..   ...     ...    ...         ...           ...          ...           ...   \n",
      "355  ...       0      2           1     -1.007800            2            12   \n",
      "356  ...       0      2           1     -0.830127            3            13   \n",
      "357  ...       0      2           1     -0.197677            4            14   \n",
      "358  ...       0      2           1      0.332082            5            15   \n",
      "359  ...       0      2           1     -0.870878            6            16   \n",
      "\n",
      "     month  national_hol  regional_hol  local_hol  \n",
      "0        9            34             1          3  \n",
      "1        9            34             1          3  \n",
      "2        9            34             1          3  \n",
      "3        9            34             1          3  \n",
      "4        9            34             1          3  \n",
      "..     ...           ...           ...        ...  \n",
      "355      2            34             1          3  \n",
      "356      2            34             1          3  \n",
      "357      2            34             1          3  \n",
      "358      2            34             1          3  \n",
      "359      2            34             1          3  \n",
      "\n",
      "[360 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train)\n",
    "print(valid)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_single_col_by_type(input_type):\n",
    "    \"\"\"Returns name of single column for input type.\"\"\"\n",
    "    return utils.get_single_col_by_input_type(input_type,\n",
    "                                              column_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_sampled_data(data, max_samples):\n",
    "    \"\"\"Samples segments into a compatible format.\n",
    "\n",
    "    Args:\n",
    "      data: Sources data to sample and batch\n",
    "      max_samples: Maximum number of samples in batch\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of batched data with the maximum samples specified.\n",
    "    \"\"\"\n",
    "\n",
    "    if max_samples < 1:\n",
    "      raise ValueError(\n",
    "          'Illegal number of samples specified! samples={}'.format(max_samples))\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID) # 'traj_id'\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME) # 'date'\n",
    "\n",
    "    data.sort_values(by=[id_col, time_col], inplace=True)\n",
    "\n",
    "    print('Getting valid sampling locations.')\n",
    "    valid_sampling_locations = []\n",
    "    split_data_map = {}\n",
    "    for identifier, df in data.groupby(id_col): # for each traj_id\n",
    "      print('Getting locations for {}'.format(identifier))\n",
    "      num_entries = len(df)\n",
    "      if num_entries >= time_steps:\n",
    "        valid_sampling_locations += [\n",
    "            (identifier, time_steps + i)\n",
    "            for i in range(num_entries - time_steps + 1)\n",
    "        ]\n",
    "      split_data_map[identifier] = df\n",
    "    # valid_sampling_locations: for each each traj_id, every day after 120th Day\n",
    "    # split_data_map = {traj_id : df}  \n",
    "\n",
    "    inputs = np.zeros((max_samples, time_steps, input_size), dtype=object) # dtype=object: dataframe contains diff formats\n",
    "    outputs = np.zeros((max_samples, time_steps, output_size), dtype=object)\n",
    "    time = np.empty((max_samples, time_steps, 1), dtype=object)\n",
    "    identifiers = np.empty((max_samples, time_steps, 1), dtype=object)\n",
    "\n",
    "    if max_samples > 0 and len(valid_sampling_locations) > max_samples:\n",
    "      print('Extracting {} samples...'.format(max_samples))\n",
    "      ranges = [\n",
    "          valid_sampling_locations[i] for i in np.random.choice( \n",
    "              len(valid_sampling_locations), max_samples, replace=False)\n",
    "      ] # Random pick max_samples from len(valid_sampling_locations)\n",
    "    else:\n",
    "      print('Max samples={} exceeds # available segments={}'.format(\n",
    "          max_samples, len(valid_sampling_locations)))\n",
    "      ranges = valid_sampling_locations\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID) # 'traj_id'\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME) # 'date'\n",
    "    target_col = _get_single_col_by_type(InputTypes.TARGET) # 'log_sales'\n",
    "    input_cols = [ # other columns in column_definition\n",
    "        tup[0]\n",
    "        for tup in column_definition\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    for i, tup in enumerate(ranges):\n",
    "      if (i + 1 % 1000) == 0:\n",
    "        print(i + 1, 'of', max_samples, 'samples done...')\n",
    "      identifier, start_idx = tup\n",
    "      sliced = split_data_map[identifier].iloc[start_idx -\n",
    "                                               time_steps:start_idx]\n",
    "      inputs[i, :, :] = sliced[input_cols]\n",
    "      outputs[i, :, :] = sliced[[target_col]]\n",
    "      time[i, :, 0] = sliced[time_col]\n",
    "      identifiers[i, :, 0] = sliced[id_col]\n",
    "\n",
    "    sampled_data = {\n",
    "        'inputs': inputs,\n",
    "        'outputs': outputs[:, num_encoder_steps:, :],\n",
    "        'active_entries': np.ones_like(outputs[:, num_encoder_steps:, :]),\n",
    "        'time': time,\n",
    "        'identifier': identifiers\n",
    "    }\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_data(data):\n",
    "    \"\"\"Batches data for training.\n",
    "\n",
    "    Converts raw dataframe from a 2-D tabular format to a batched 3-D array\n",
    "    to feed into Keras model.\n",
    "    将 2D 的 DataFrame 格式数据批处理为 3D 的 NumPy 数组，用于时间序列模型的训练。\n",
    "    它通过对每个实体的时间序列数据进行处理，生成了适合喂入 Keras 模型的训练数据。\n",
    "    Args:\n",
    "      data: DataFrame to batch\n",
    "\n",
    "    Returns:\n",
    "      Batched Numpy array with shape=(?, time_steps, input_size)\n",
    "    \"\"\"\n",
    "\n",
    "    # Functions.\n",
    "    def _batch_single_entity(input_data):\n",
    "      time_steps = len(input_data)\n",
    "      lags = time_steps\n",
    "      x = input_data.values #  将 DataFrame 转换为 NumPy 数组\n",
    "      if time_steps >= lags:\n",
    "        return np.stack(\n",
    "            [x[i:time_steps - (lags - 1) + i, :] for i in range(lags)], axis=1)\n",
    "            # 通过 np.stack 生成形状为 (time_steps, input_size) 的数组，每个实体的数据被堆叠在一起。\n",
    "      else:\n",
    "        return None # 如果时间步长小于滞后步长（lags），则返回 None，表示不能对该实体进行批处理。\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID)\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME)\n",
    "    target_col = _get_single_col_by_type(InputTypes.TARGET)\n",
    "    input_cols = [\n",
    "        tup[0]\n",
    "        for tup in column_definition\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    data_map = {}\n",
    "    for _, sliced in data.groupby(id_col): # 按照 id_col（ID 列）对数据进行分组\n",
    "\n",
    "      col_mappings = {\n",
    "          'identifier': [id_col],\n",
    "          'time': [time_col],\n",
    "          'outputs': [target_col],\n",
    "          'inputs': input_cols\n",
    "      }\n",
    "\n",
    "      for k in col_mappings:\n",
    "        cols = col_mappings[k]\n",
    "        arr = _batch_single_entity(sliced[cols].copy())\n",
    "        # 将批处理后的数据（arr）保存在 data_map 字典中，如果字典中已经存在对应键，就将新数据追加到列表中\n",
    "        if k not in data_map:\n",
    "          data_map[k] = [arr]\n",
    "        else:\n",
    "          data_map[k].append(arr)\n",
    "\n",
    "    # Combine all data\n",
    "    for k in data_map:\n",
    "      data_map[k] = np.concatenate(data_map[k], axis=0)\n",
    "\n",
    "    # Shorten target so we only get decoder steps\n",
    "    data_map['outputs'] = data_map['outputs'][:, num_encoder_steps:, :]\n",
    "\n",
    "    active_entries = np.ones_like(data_map['outputs']) \n",
    "    # active_entries 用来标记哪些时间步是有效的，初始化为与 outputs 数据形状相同的全 1 矩阵。\n",
    "\n",
    "    if 'active_entries' not in data_map: # 如果 data_map 中没有 active_entries，则将其添加进去，否则将新的 active_entries 追加到已有数据中\n",
    "      data_map['active_entries'] = active_entries\n",
    "    else:\n",
    "      data_map['active_entries'].append(active_entries)\n",
    "\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFTDataCache(object):\n",
    "  \"\"\"Caches data for the TFT.\"\"\" \n",
    "  # stores multiple copies of data or files in a temporary storage location—or cache—\n",
    "  # so they can be accessed faster\n",
    "\n",
    "  _data_cache = {}\n",
    "\n",
    "  @classmethod\n",
    "  def update(cls, data, key):  # cls is similar to self\n",
    "    \"\"\"Updates cached data.\n",
    "\n",
    "    Args:\n",
    "      data: Source to update\n",
    "      key: Key to dictionary location\n",
    "    \"\"\"\n",
    "    cls._data_cache[key] = data\n",
    "\n",
    "  @classmethod\n",
    "  def get(cls, key):\n",
    "    \"\"\"Returns data stored at key location.\"\"\"\n",
    "    return cls._data_cache[key].copy()\n",
    "\n",
    "  @classmethod\n",
    "  def contains(cls, key): # TFTDataCache.contains('train') and TFTDataCache.contains('valid')\n",
    "    \"\"\"Retuns boolean indicating whether key is present in cache.\"\"\"\n",
    "    return key in cls._data_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 300\n",
    "valid_samples = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 1_96995\n",
      "Getting locations for 25_103665\n",
      "Extracting 300 samples...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.8988256237605453, -0.5905133221467458, 0.6419878085466167,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.8988256237605453, -0.5644329028101215, 0.6415356686145767,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.8988256237605453, -0.07379501403987561, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-1.3454751863495475, -1.1235318923390063, 0.6146333426581894,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.8988256237605453, -0.7388457071237969, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.3582556663686256, 0.6480916976291583,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[-0.3789074381700053, 0.6059259149208969, 0.7432671533236047,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.7770786668174943, 0.7514056721003269,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.5876603100095371, 0.4999742113658604, 0.7439453632216649,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-0.3789074381700053, 0.6238562032148262, 0.7572834912168483,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.3728321670998167, 0.6815500526001276,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.5876603100095371, 0.4885640279060873, 0.6571344962699609,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[-1.3454751863495475, -1.3435854304917745, 0.6815500526001276,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.8988256237605453, -0.9719394549448772, 0.6571344962699609,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -0.7095052353700945, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-0.3789074381700053, -0.6247438725260653, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, -1.2245935172684257, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, -1.3631457449942426, 0.6480916976291583,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.3454751863495475, -1.3990063215821011, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.5505050238305701, 0.7055134689982543,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.6059259149208969, 0.7432671533236047,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-1.3454751863495475, -0.22212739901692669, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.6466765701343725, 0.7505013922362466,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, 0.615706072172131, 0.7891593564256775, ...,\n",
       "         34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[1.1530661971010592, -0.5579127979759654, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.5876603100095371, -0.8578376203471456, 0.7220165765177189,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [1.5542280581890802, -0.5758430862698947, 0.7165908973332372,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-1.3454751863495475, -1.4087864788333353, 0.7292508154303607,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.131682023381701, 0.7147823376050768,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, -0.6817947898249311, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[-0.3789074381700053, 0.7183977233100894, 0.5739407487745778,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.5700653383330384, 0.5908959962260825,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.8618400296615235, 0.58162712761926, ...,\n",
       "         34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-0.3789074381700053, 0.7037274874332382, 0.6560041464398607,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.7330679591869407, 0.6569084263039408,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, -0.08846524991672683, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0]]], dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_batch_sampled_data(train, max_samples=train_samples)['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 1_96995\n",
      "Getting locations for 25_103665\n",
      "Extracting 300 samples...\n",
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 25_103665\n",
      "Max samples=100 exceeds # available segments=3\n"
     ]
    }
   ],
   "source": [
    "# model.cache_batched_data(train, \"train\", num_samples=train_samples)\n",
    "if train_samples > 0:\n",
    "      TFTDataCache.update(\n",
    "          _batch_sampled_data(train, max_samples=train_samples),\n",
    "            \"train\")  \n",
    "      # create a temperal set, where key='train', value=_batch_sampled_data(train, max_samples=train_samples)\n",
    "      # _batch_sampled_data(train, max_samples=train_samples)\n",
    "      # dict_keys(['inputs', 'outputs', 'active_entries', 'time', 'identifier'])\n",
    "else:\n",
    "      TFTDataCache.update(\n",
    "            _batch_data(train), \n",
    "            \"train\")\n",
    "      \n",
    "# model.cache_batched_data(valid, \"valid\", num_samples=valid_samples)\n",
    "if valid_samples > 0:\n",
    "      TFTDataCache.update(\n",
    "          _batch_sampled_data(valid, max_samples=valid_samples),\n",
    "            \"valid\")  \n",
    "      # create a temperal set, where key='valid', value=_batch_sampled_data(valid, max_samples=valid_samples)\n",
    "else:\n",
    "      TFTDataCache.update(\n",
    "            _batch_data(valid), \n",
    "            \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting batched_data\n",
      "Using cached training data\n",
      "Using cached validation data\n"
     ]
    }
   ],
   "source": [
    "print('Getting batched_data')\n",
    "train_df=None\n",
    "valid_df=None\n",
    "if train_df is None:\n",
    "      print('Using cached training data')\n",
    "      train_data = TFTDataCache.get('train')\n",
    "else:\n",
    "      train_data = _batch_data(train_df)\n",
    "\n",
    "if valid_df is None:\n",
    "      print('Using cached validation data')\n",
    "      valid_data = TFTDataCache.get('valid')\n",
    "else:\n",
    "      valid_data = _batch_data(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using keras standard fit\n"
     ]
    }
   ],
   "source": [
    "print('Using keras standard fit')\n",
    "# Unpack without sample weights\n",
    "data = train_data['inputs'] # (300, 120, 20)\n",
    "labels = train_data['outputs'] # (300, 30, 1)\n",
    "active_flags = (np.sum(train_data['active_entries'], axis=-1) > 0.0) * 1.0  # (300, 30)\n",
    " # 通过 > 0.0 判断每个时间步是否有效，如果有效则返回 True，否则为 False。\n",
    " # 乘以 1.0 将布尔值转换为浮点数，结果 active_flags 为 0 或 1，表示每个样本的每个时间步是否有效。\n",
    "val_data = valid_data['inputs']\n",
    "val_labels = valid_data['outputs']\n",
    "val_flags = (np.sum(valid_data['active_entries'], axis=-1) > 0.0) * 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relevant callbacks\n",
    "name = 'favorita'\n",
    "def get_keras_saved_path(model_folder):\n",
    "    \"\"\"Returns path to keras checkpoint.\"\"\"\n",
    "    return os.path.join(model_folder, '{}.check'.format(name)) # 生成的路径为 {model_folder}/favorita.check\n",
    "\n",
    "# callbacks 是一个列表，包含了三个 Keras 回调函数，用于在训练过程中进行特定操作。\n",
    "callbacks = [ \n",
    "        tf.keras.callbacks.EarlyStopping( # 在验证损失不再改善时提前停止训练\n",
    "            monitor='val_loss',\n",
    "            patience=early_stopping_patience, # 在验证损失没有改善的情况下，允许训练继续的轮次数（即耐心值）\n",
    "            min_delta=1e-4), # 监控的损失改善的最小阈值，只有当改善超过此值时，才认为有改进。\n",
    "        tf.keras.callbacks.ModelCheckpoint( # ModelCheckpoint 用于保存模型的最佳检查点\n",
    "            filepath=get_keras_saved_path(_temp_folder), # 生成模型保存路径，_temp_folder 是保存检查点的文件夹\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True, # 仅保存表现最好的模型\n",
    "            save_weights_only=True), # 只保存模型的权重，而不是整个模型架构。\n",
    "        tf.keras.callbacks.TerminateOnNaN() # TerminateOnNaN 回调会在训练过程中检测到 NaN（不是数字）的情况时终止训练，避免模型在错误状态下继续训练。\n",
    "    ]\n",
    "\n",
    "all_callbacks = callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\TFT\\sep_venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 2 samples, validate on 100 samples\n",
      "2/2 [==============================] - 13s 7s/step - loss: 1.2309 - val_loss: 0.5254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14639f51f28>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft_model.fit(\n",
    "        x=tf.convert_to_tensor(data,dtype=np.float32), # shape=(300, 120, 20)\n",
    "        y=np.concatenate([labels, labels, labels], axis=-1), # shape=(300, 30, 3) # 3 represents # of quantiles\n",
    "        #y=tf.convert_to_tensor(np.concatenate([labels, labels, labels], axis=-1),dtype=np.float32), \n",
    "        sample_weight=active_flags,\n",
    "        # 在训练过程中，Keras 计算每个样本的损失时会考虑 sample_weight。样本的损失会乘以对应的权重：\n",
    "        # 有效的时间步（active_flags 为 1）会正常计算损失。\n",
    "        # 无效的时间步（active_flags 为 0）则不会对损失产生影响。\n",
    "        epochs=num_epochs,\n",
    "        batch_size=minibatch_size,\n",
    "        steps_per_epoch = len(data) // minibatch_size,\n",
    "        validation_data=(val_data,\n",
    "                         np.concatenate([val_labels, val_labels, val_labels],\n",
    "                                        axis=-1), val_flags),\n",
    "        callbacks=all_callbacks,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        workers=n_multiprocessing_workers\n",
    "        )\n",
    "\n",
    "# use_multiprocessing: https://stackoverflow.com/questions/52932406/is-the-class-generator-inheriting-sequence-thread-safe-in-keras-tensorflow/63641535#63641535"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(self, model_folder, use_keras_loadings=False):\n",
    "    \"\"\"Loads TFT weights.\n",
    "\n",
    "    Args:\n",
    "      model_folder: Folder containing serialized models.\n",
    "      use_keras_loadings: Whether to load from Keras checkpoint.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if use_keras_loadings:\n",
    "      # Loads temporary Keras model saved during training.\n",
    "      serialisation_path = self.get_keras_saved_path(model_folder)\n",
    "      print('Loading model from {}'.format(serialisation_path))\n",
    "      self.model.load_weights(serialisation_path)\n",
    "    else:\n",
    "      # Loads tensorflow graph for optimal models.\n",
    "      utils.load(\n",
    "          tf.keras.backend.get_session(),\n",
    "          model_folder,\n",
    "          cp_name=self.name,\n",
    "          scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load from 0615_result\\tmp, skipping ...\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint again\n",
    "tmp_checkpont = get_keras_saved_path(_temp_folder)\n",
    "if os.path.exists(tmp_checkpont):\n",
    "  load(\n",
    "  _temp_folder,\n",
    "  use_keras_loadings=True)\n",
    "\n",
    "else:\n",
    "  print('Cannot load from {}, skipping ...'.format(_temp_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\"\"\"Applies evaluation metric to the training data.\n",
    "\n",
    "    Args:\n",
    "      data: Dataframe for evaluation\n",
    "      eval_metric: Evaluation metic to return, based on model definition.\n",
    "\n",
    "    Returns:\n",
    "      Computed evaluation loss.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_active_locations(x):\n",
    "    \"\"\"Formats sample weights for Keras training.\"\"\"\n",
    "    return (np.sum(x, axis=-1) > 0.0) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached validation data\n",
      "100/100 [==============================] - 1s 14ms/sample - loss: 0.8500\n",
      "0.8499989414215088\n"
     ]
    }
   ],
   "source": [
    "print('Using cached validation data')\n",
    "raw_data = TFTDataCache.get('valid')\n",
    "# _batch_data(data)\n",
    "\n",
    "inputs = raw_data['inputs']\n",
    "outputs = raw_data['outputs']\n",
    "active_entries = _get_active_locations(raw_data['active_entries'])\n",
    "\n",
    "metric_values = tft_model.evaluate(\n",
    "    x=inputs,\n",
    "    y=np.concatenate([outputs, outputs, outputs], axis=-1),\n",
    "    sample_weight=active_entries,\n",
    "    workers=16,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "metrics = pd.Series(metric_values, model.metrics_names)\n",
    "print(metrics['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "\"\"\"Computes predictions for a given input dataset.\n",
    "\n",
    "    Args:\n",
    "      df: Input dataframe\n",
    "      return_targets: Whether to also return outputs aligned with predictions to faciliate evaluation\n",
    "\n",
    "    Returns:\n",
    "      Input dataframe or tuple of (input dataframe, algined output dataframe).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _batch_data(test)\n",
    "inputs = data['inputs']\n",
    "time = data['time']\n",
    "identifier = data['identifier']\n",
    "outputs = data['outputs']\n",
    "\n",
    "combined = tft_model.predict(inputs,\n",
    "                         workers=16,\n",
    "                         use_multiprocessing=True,\n",
    "                         batch_size=minibatch_size)\n",
    "# Format output_csv\n",
    "if output_size != 1:\n",
    "    raise NotImplementedError('Current version only supports 1D targets!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p10':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520 -0.110052 -0.030467  0.035256 -0.358097 -0.087411   \n",
      "1    2014-01-13   1_103665  0.107377 -0.442838 -0.153968  0.008752 -0.283448   \n",
      "2    2014-01-10  25_103665  0.012276 -0.260413 -0.254595 -0.034237 -0.220838   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0 -0.100514 -0.258652 -0.098430  ... -0.050560 -0.038419 -0.354724 -0.110672   \n",
      "1 -0.246053  0.053064  0.017097  ... -0.357304 -0.133147  0.025059  0.014993   \n",
      "2 -0.076114 -0.286801 -0.050017  ... -0.393892 -0.195104 -0.075459 -0.263219   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0 -0.114291 -0.248850 -0.272880 -0.116653 -0.000424 -0.407960  \n",
      "1 -0.467909 -0.173600 -0.007327 -0.258477 -0.000849 -0.481141  \n",
      "2 -0.277795 -0.058543 -0.463335 -0.205537 -0.061876 -0.269727  \n",
      "\n",
      "[3 rows x 32 columns], 'p50':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520  0.226352  0.194617  0.040327  0.478400  0.148161   \n",
      "1    2014-01-13   1_103665 -0.081755  0.512296  0.203078  0.167504  0.422532   \n",
      "2    2014-01-10  25_103665  0.164826  0.398374  0.246627  0.060524  0.268215   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0  0.300709  0.323294  0.234109  ...  0.100630  0.120996  0.473319  0.183525   \n",
      "1  0.235716  0.024951  0.002831  ...  0.415210  0.175268  0.143076  0.003778   \n",
      "2  0.237598  0.272465  0.071711  ...  0.431504  0.237251  0.221860  0.369992   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0  0.318184  0.403928  0.347288  0.251229  0.068223  0.523363  \n",
      "1  0.509607  0.211734  0.176706  0.252269  0.018654  0.520635  \n",
      "2  0.272013  0.080322  0.490627  0.245520  0.217884  0.382784  \n",
      "\n",
      "[3 rows x 32 columns], 'p90':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520  0.071086  0.188287 -0.001887  0.027504 -0.097200   \n",
      "1    2014-01-13   1_103665 -0.033625  0.308635 -0.359788 -0.094824 -0.289506   \n",
      "2    2014-01-10  25_103665 -0.082409 -0.259729  0.283264 -0.071697 -0.319095   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0 -0.171352  0.123934  0.340035  ... -0.080291  0.012497  0.075639 -0.061632   \n",
      "1  0.302927  0.676493 -0.013359  ...  0.310350 -0.188568  0.007324  0.043414   \n",
      "2 -0.080475  0.289604 -0.033493  ...  0.286814 -0.138290  0.021701 -0.062737   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0 -0.125722 -0.140652  0.136236  0.352088  0.032393  0.134869  \n",
      "1  0.333124 -0.208830  0.001233  0.325366  0.036714  0.323583  \n",
      "2  0.305296  0.035202  0.312624 -0.176395  0.012766 -0.080223  \n",
      "\n",
      "[3 rows x 32 columns]}\n"
     ]
    }
   ],
   "source": [
    "def format_outputs(prediction):\n",
    "      \"\"\"Returns formatted dataframes for prediction.\"\"\"\n",
    "\n",
    "      flat_prediction = pd.DataFrame(\n",
    "          prediction[:, :, 0],\n",
    "          columns=[\n",
    "              't+{}'.format(i)\n",
    "              for i in range(time_steps - num_encoder_steps)\n",
    "          ])\n",
    "      cols = list(flat_prediction.columns)\n",
    "      flat_prediction['forecast_time'] = time[:, num_encoder_steps - 1, 0]\n",
    "      flat_prediction['identifier'] = identifier[:, 0, 0]\n",
    "\n",
    "      # Arrange in order\n",
    "      return flat_prediction[['forecast_time', 'identifier'] + cols]\n",
    "      # 重新排列列的顺序，使得 'forecast_time' 和 'identifier' 列位于前面，后面是所有时间步的预测结果。\n",
    "\n",
    "# Extract predictions for each quantile into different entries\n",
    "process_map = {\n",
    "        'p{}'.format(int(q * 100)):\n",
    "        combined[Ellipsis, i * output_size:(i + 1) * output_size]\n",
    "        for i, q in enumerate(quantiles)\n",
    "    } # 使用 Ellipsis 表示选取所有样本，i * output_size:(i + 1) * output_size 则提取特定分位数对应的输出。\n",
    "\n",
    "return_targets = False\n",
    "if return_targets:\n",
    "      # Add targets if relevant\n",
    "      process_map['targets'] = outputs\n",
    "\n",
    "print({k: format_outputs(process_map[k]) for k in process_map})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Attention\n",
    " \"\"\"Computes TFT attention weights for a given dataset.\n",
    "\n",
    "    Args:\n",
    "      df: Input dataframe\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of numpy arrays for temporal attention weights and variable\n",
    "          selection weights, along with their identifiers and time indices\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_attention_weights(input_batch):\n",
    "      \"\"\"Returns weights for a given minibatch of data.\"\"\"\n",
    "      input_placeholder = _input_placeholder\n",
    "      attention_weights = {}\n",
    "      for k in _attention_components:\n",
    "        attention_weight = tf.keras.backend.get_session().run(_attention_components[k], {input_placeholder: input_batch.astype(np.float32)})\n",
    "        attention_weights[k] = attention_weight\n",
    "      return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_self_attn': <tf.Tensor 'stack_8:0' shape=(4, ?, 120, 120) dtype=float32>,\n",
       " 'static_flags': <tf.Tensor 'strided_slice_92:0' shape=(?, 9) dtype=float32>,\n",
       " 'historical_flags': <tf.Tensor 'strided_slice_93:0' shape=(?, 90, 11) dtype=float32>,\n",
       " 'future_flags': <tf.Tensor 'strided_slice_94:0' shape=(?, 30, 8) dtype=float32>}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(?, 120, 20) dtype=float32>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_attention_weights(input_batch):\n",
    "    \"\"\"Returns weights for a given minibatch of data.\"\"\"\n",
    "    input_placeholder = _input_placeholder\n",
    "    attention_weights = {}\n",
    "    \n",
    "    for k in _attention_components:\n",
    "        # 直接调用 TensorFlow 函数\n",
    "        attention_weight = _attention_components[k](input_batch.astype(np.float32))\n",
    "        attention_weights[k] = attention_weight.numpy()  # 如果需要转换为 NumPy 数组\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_92:0' shape=(?, 9) dtype=float32>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention_components['static_flags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'decoder_self_attn': array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.41733664e-01, 6.58266366e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.87719876e-01, 6.29924417e-01, 8.23557451e-02, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [1.48459561e-02, 9.90017131e-03, 2.83475108e-02, ...,\n",
      "          1.19226519e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.83642086e-03, 7.30362907e-03, 6.15172694e-03, ...,\n",
      "          1.03162909e-02, 8.62444192e-03, 0.00000000e+00],\n",
      "         [1.46961762e-02, 1.02706542e-02, 2.60381550e-02, ...,\n",
      "          1.58011157e-03, 4.17176401e-03, 1.84553617e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.15763783e-01, 3.84236187e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.47532225e-01, 3.42000276e-01, 1.04674911e-02, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [9.32625029e-03, 8.84521101e-03, 6.62360899e-03, ...,\n",
      "          1.04523133e-02, 0.00000000e+00, 0.00000000e+00],\n",
      "         [9.99084488e-03, 6.95736613e-03, 9.64034756e-04, ...,\n",
      "          2.17714012e-02, 4.07145964e-03, 0.00000000e+00],\n",
      "         [8.52457341e-03, 8.42702482e-03, 7.91360345e-03, ...,\n",
      "          8.73839762e-03, 8.28463491e-03, 8.84474628e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.82436997e-01, 7.17563033e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.84946045e-01, 5.20019174e-01, 2.95034826e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [9.37649806e-04, 1.83946115e-03, 1.27129979e-03, ...,\n",
      "          5.14116790e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.96695454e-04, 9.24433349e-04, 5.81355533e-04, ...,\n",
      "          3.35960789e-03, 7.66997691e-03, 0.00000000e+00],\n",
      "         [5.55480365e-04, 1.20994193e-03, 7.89599202e-04, ...,\n",
      "          3.96691170e-03, 8.47903918e-03, 7.21102953e-03]]],\n",
      "\n",
      "\n",
      "       [[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.83876276e-01, 3.16123784e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.97729373e-01, 1.59080267e-01, 5.43190360e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [6.10434916e-03, 6.70883385e-03, 5.57570904e-03, ...,\n",
      "          1.04440367e-02, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.35313666e-03, 3.19649419e-03, 1.75401289e-03, ...,\n",
      "          1.34347510e-02, 2.41731629e-02, 0.00000000e+00],\n",
      "         [6.18456770e-03, 6.73613325e-03, 5.69793582e-03, ...,\n",
      "          1.00535862e-02, 1.18432194e-02, 8.63279682e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.35811427e-01, 7.64188588e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.68782085e-01, 5.38458467e-01, 2.92759448e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [1.02334153e-02, 7.75095494e-03, 8.96884128e-03, ...,\n",
      "          1.19698187e-02, 0.00000000e+00, 0.00000000e+00],\n",
      "         [9.79443360e-03, 7.86627084e-03, 8.82631540e-03, ...,\n",
      "          1.10837650e-02, 1.22145284e-02, 0.00000000e+00],\n",
      "         [7.71470414e-03, 8.39719176e-03, 8.03149864e-03, ...,\n",
      "          7.35448301e-03, 7.08335172e-03, 8.00275430e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [5.67724288e-01, 4.32275742e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [5.09470046e-01, 3.88202250e-01, 1.02327682e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [8.03819764e-03, 8.10444076e-03, 8.43733922e-03, ...,\n",
      "          8.88257567e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.82764567e-02, 1.50730507e-02, 5.85751282e-03, ...,\n",
      "          1.75115885e-03, 2.56870757e-03, 0.00000000e+00],\n",
      "         [1.60942655e-02, 1.40065681e-02, 7.08562415e-03, ...,\n",
      "          2.96691386e-03, 3.91082279e-03, 4.61335806e-03]]],\n",
      "\n",
      "\n",
      "       [[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.48159802e-01, 3.51840198e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.00470024e-01, 1.57509968e-01, 5.42019963e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [4.44399985e-03, 4.95629851e-03, 4.02246183e-03, ...,\n",
      "          1.68791041e-02, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.12964641e-03, 3.66172730e-03, 2.71148398e-03, ...,\n",
      "          2.13588029e-02, 1.30659081e-02, 0.00000000e+00],\n",
      "         [9.14249662e-03, 8.97434261e-03, 9.29884799e-03, ...,\n",
      "          7.28536351e-03, 7.72122061e-03, 7.34539563e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [4.38699454e-01, 5.61300516e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.07123494e-01, 1.29460290e-01, 7.63416171e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [8.39862786e-03, 8.48887209e-03, 9.38292220e-03, ...,\n",
      "          7.15814019e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [8.10359046e-03, 8.34758207e-03, 1.10221105e-02, ...,\n",
      "          5.20032924e-03, 7.08714919e-03, 0.00000000e+00],\n",
      "         [7.02940347e-03, 7.60989217e-03, 1.60043463e-02, ...,\n",
      "          2.14598654e-03, 4.91167093e-03, 2.06369930e-03]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.04081035e-01, 3.95918965e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.41096193e-01, 2.43243665e-01, 4.15660113e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [1.17359916e-02, 1.02655198e-02, 1.26915965e-02, ...,\n",
      "          3.94922728e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.40003460e-02, 1.03838360e-02, 1.66735332e-02, ...,\n",
      "          1.23101566e-03, 2.46483646e-03, 0.00000000e+00],\n",
      "         [1.38109187e-02, 1.04001286e-02, 1.63024925e-02, ...,\n",
      "          1.37410325e-03, 2.65593128e-03, 2.05594511e-03]]],\n",
      "\n",
      "\n",
      "       [[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.30233645e-01, 3.69766295e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [3.43277544e-01, 4.26829368e-01, 2.29893103e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [4.36517457e-03, 7.68903992e-04, 1.06637858e-01, ...,\n",
      "          9.67341184e-05, 0.00000000e+00, 0.00000000e+00],\n",
      "         [1.21084880e-02, 6.77677477e-03, 3.52368504e-02, ...,\n",
      "          3.38923372e-03, 9.03869048e-03, 0.00000000e+00],\n",
      "         [3.36906081e-03, 4.89452563e-04, 1.17328443e-01, ...,\n",
      "          4.89253071e-05, 1.27482100e-03, 1.99983857e-04]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.32046187e-01, 7.67953813e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [6.36905849e-01, 2.93537438e-01, 6.95567280e-02, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [1.69837120e-04, 6.17017271e-04, 6.78767543e-03, ...,\n",
      "          1.35595200e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [9.01247654e-03, 8.67127907e-03, 8.07100628e-03, ...,\n",
      "          8.46941397e-03, 8.08976498e-03, 0.00000000e+00],\n",
      "         [1.21844432e-05, 8.50103170e-05, 3.14545422e-03, ...,\n",
      "          2.78219435e-04, 2.79856357e-03, 1.28832195e-04]],\n",
      "\n",
      "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [5.44837654e-01, 4.55162406e-01, 0.00000000e+00, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.14378923e-01, 3.63624901e-01, 4.21996236e-01, ...,\n",
      "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
      "         ...,\n",
      "         [9.60727688e-03, 8.91657453e-03, 8.73109046e-03, ...,\n",
      "          8.69136583e-03, 0.00000000e+00, 0.00000000e+00],\n",
      "         [2.22321320e-02, 1.05072269e-02, 8.50704033e-03, ...,\n",
      "          8.12612753e-03, 1.77478022e-03, 0.00000000e+00],\n",
      "         [2.22219992e-02, 1.04728853e-02, 8.47250782e-03, ...,\n",
      "          8.09174776e-03, 1.75719173e-03, 3.05211637e-03]]]],\n",
      "      dtype=float32), 'static_flags': array([[0.16489264, 0.10217965, 0.08541793, 0.04495512, 0.08401922,\n",
      "        0.03744881, 0.03630661, 0.23473383, 0.21004623],\n",
      "       [0.08831412, 0.06360199, 0.10385614, 0.06364138, 0.22979042,\n",
      "        0.01736767, 0.10062915, 0.09369591, 0.23910329],\n",
      "       [0.1571713 , 0.0281307 , 0.16524427, 0.13303086, 0.3025991 ,\n",
      "        0.01963381, 0.08425715, 0.04074734, 0.06918549]], dtype=float32), 'historical_flags': array([[[0.08356987, 0.00948936, 0.13558705, ..., 0.21677086,\n",
      "         0.20013425, 0.07961018],\n",
      "        [0.09285861, 0.00972393, 0.14255124, ..., 0.19777982,\n",
      "         0.19422281, 0.07819577],\n",
      "        [0.09102192, 0.0138527 , 0.1754121 , ..., 0.24434844,\n",
      "         0.14145352, 0.05109632],\n",
      "        ...,\n",
      "        [0.11372061, 0.23526487, 0.2697806 , ..., 0.01977455,\n",
      "         0.02689107, 0.03209563],\n",
      "        [0.13757628, 0.20921607, 0.2677414 , ..., 0.01788022,\n",
      "         0.02608405, 0.03084273],\n",
      "        [0.12622364, 0.01194388, 0.14432725, ..., 0.32345435,\n",
      "         0.05248809, 0.0260841 ]],\n",
      "\n",
      "       [[0.11927029, 0.03315435, 0.20808133, ..., 0.10232171,\n",
      "         0.10799181, 0.06318127],\n",
      "        [0.10061971, 0.04610096, 0.22268832, ..., 0.10384753,\n",
      "         0.10017203, 0.06291526],\n",
      "        [0.16997114, 0.04328291, 0.21742377, ..., 0.07693662,\n",
      "         0.06542013, 0.03992324],\n",
      "        ...,\n",
      "        [0.06981024, 0.24922918, 0.3206588 , ..., 0.05093123,\n",
      "         0.02858583, 0.0281489 ],\n",
      "        [0.12136219, 0.25059807, 0.29115394, ..., 0.02840811,\n",
      "         0.01901253, 0.02099676],\n",
      "        [0.12692216, 0.12783286, 0.37985197, ..., 0.02488581,\n",
      "         0.03605639, 0.03549958]],\n",
      "\n",
      "       [[0.08995182, 0.02063733, 0.16017169, ..., 0.22328594,\n",
      "         0.13216302, 0.0572351 ],\n",
      "        [0.07691152, 0.02921125, 0.20410423, ..., 0.16521788,\n",
      "         0.1383996 , 0.06987309],\n",
      "        [0.10693357, 0.02373351, 0.2035446 , ..., 0.13509911,\n",
      "         0.13041319, 0.06513759],\n",
      "        ...,\n",
      "        [0.10374518, 0.18066849, 0.38822445, ..., 0.02702244,\n",
      "         0.02571963, 0.0297161 ],\n",
      "        [0.10730034, 0.19891076, 0.36612976, ..., 0.0242831 ,\n",
      "         0.0250221 , 0.02918381],\n",
      "        [0.1615028 , 0.141871  , 0.36451584, ..., 0.01948208,\n",
      "         0.02511294, 0.02774033]]], dtype=float32), 'future_flags': array([[[0.43513665, 0.08023635, 0.04517716, 0.0918963 , 0.18580608,\n",
      "         0.04360766, 0.01192953, 0.10621041],\n",
      "        [0.44292912, 0.06393083, 0.04401072, 0.08107008, 0.20435661,\n",
      "         0.04997846, 0.01228562, 0.10143863],\n",
      "        [0.44174477, 0.05501329, 0.04436781, 0.0764085 , 0.2180805 ,\n",
      "         0.0543519 , 0.01263122, 0.09740191],\n",
      "        [0.4340564 , 0.04859083, 0.0445105 , 0.07294595, 0.23261654,\n",
      "         0.05934561, 0.01302302, 0.09491117],\n",
      "        [0.4276688 , 0.04509134, 0.04455772, 0.07036728, 0.24395788,\n",
      "         0.06127164, 0.01344684, 0.09363851],\n",
      "        [0.42125812, 0.04239052, 0.04474818, 0.06696405, 0.25480843,\n",
      "         0.06298412, 0.01388829, 0.09295829],\n",
      "        [0.40497947, 0.0387751 , 0.04462445, 0.06489371, 0.27448812,\n",
      "         0.06620105, 0.01453729, 0.09150083],\n",
      "        [0.3989725 , 0.03759249, 0.04441429, 0.06355448, 0.28166017,\n",
      "         0.06750082, 0.01483453, 0.09147076],\n",
      "        [0.3947376 , 0.03661334, 0.04471631, 0.06282812, 0.28678134,\n",
      "         0.06856011, 0.01502083, 0.09074222],\n",
      "        [0.38911012, 0.03548893, 0.04489166, 0.06223484, 0.2924377 ,\n",
      "         0.07046898, 0.01522665, 0.0901411 ],\n",
      "        [0.38566446, 0.03494231, 0.04498811, 0.06147766, 0.29672652,\n",
      "         0.0709321 , 0.01539429, 0.08987459],\n",
      "        [0.38238934, 0.03441615, 0.04515556, 0.06015928, 0.3010493 ,\n",
      "         0.07144557, 0.01559008, 0.08979485],\n",
      "        [0.3739597 , 0.03351068, 0.04514885, 0.05947373, 0.30998817,\n",
      "         0.07273971, 0.01586314, 0.08931608],\n",
      "        [0.37090215, 0.03319029, 0.04503559, 0.05887266, 0.31323603,\n",
      "         0.07332606, 0.01599873, 0.08943851],\n",
      "        [0.3687236 , 0.03289079, 0.04526267, 0.05855344, 0.31554106,\n",
      "         0.07384875, 0.01606663, 0.08911309],\n",
      "        [0.36562315, 0.03242139, 0.04540005, 0.05833009, 0.3182435 ,\n",
      "         0.07499832, 0.01616646, 0.08881704],\n",
      "        [0.36370686, 0.03226219, 0.04549377, 0.05793356, 0.3204412 ,\n",
      "         0.07519513, 0.01623367, 0.08873358],\n",
      "        [0.36183202, 0.0320792 , 0.04563593, 0.05715963, 0.3227569 ,\n",
      "         0.07544234, 0.0163323 , 0.08876167],\n",
      "        [0.35309675, 0.031435  , 0.04578744, 0.05632669, 0.3313924 ,\n",
      "         0.0769001 , 0.01655329, 0.08850829],\n",
      "        [0.35103336, 0.03116056, 0.04590261, 0.05623244, 0.33301008,\n",
      "         0.07772497, 0.01660874, 0.08832709],\n",
      "        [0.34965733, 0.03109973, 0.04599603, 0.0559974 , 0.33446124,\n",
      "         0.07784307, 0.01663522, 0.08830997],\n",
      "        [0.42157316, 0.06875639, 0.04689896, 0.08716304, 0.20761056,\n",
      "         0.04974042, 0.01167876, 0.10657871],\n",
      "        [0.42026937, 0.05877181, 0.04643056, 0.08189554, 0.22343582,\n",
      "         0.0546894 , 0.01200735, 0.1025001 ],\n",
      "        [0.41718826, 0.05321411, 0.04606466, 0.07822549, 0.23589453,\n",
      "         0.0568556 , 0.01242111, 0.10013609],\n",
      "        [0.41327304, 0.0490353 , 0.04593777, 0.07378871, 0.24771184,\n",
      "         0.05878032, 0.01285811, 0.0986148 ],\n",
      "        [0.40670925, 0.04621954, 0.04572318, 0.07135745, 0.2587979 ,\n",
      "         0.06060461, 0.01320493, 0.09738307],\n",
      "        [0.40030244, 0.04356434, 0.04542143, 0.07033147, 0.26852575,\n",
      "         0.06237393, 0.01355437, 0.09592636],\n",
      "        [0.3953706 , 0.04176726, 0.04507701, 0.06841075, 0.27617556,\n",
      "         0.06385303, 0.01388103, 0.09546476],\n",
      "        [0.3919105 , 0.04031729, 0.04528223, 0.06726188, 0.2816904 ,\n",
      "         0.06507461, 0.01410361, 0.09435947],\n",
      "        [0.3869616 , 0.03876686, 0.04536979, 0.06628473, 0.28775528,\n",
      "         0.06709111, 0.01433715, 0.09343347]],\n",
      "\n",
      "       [[0.31187707, 0.03166153, 0.03890351, 0.06440786, 0.37619716,\n",
      "         0.07235226, 0.01733269, 0.08726791],\n",
      "        [0.3119133 , 0.0308311 , 0.03947309, 0.06395042, 0.37542945,\n",
      "         0.07393397, 0.01746105, 0.08700763],\n",
      "        [0.31308424, 0.03026841, 0.03997506, 0.06339336, 0.3743431 ,\n",
      "         0.07425241, 0.01758998, 0.08709344],\n",
      "        [0.31407464, 0.02979278, 0.04048608, 0.06221374, 0.37377927,\n",
      "         0.07463049, 0.01774012, 0.08728287],\n",
      "        [0.31378746, 0.02948035, 0.04083596, 0.06161797, 0.3740245 ,\n",
      "         0.07505643, 0.01781727, 0.08737995],\n",
      "        [0.31317815, 0.02904993, 0.04108408, 0.06156344, 0.37435287,\n",
      "         0.07562612, 0.0179259 , 0.08721961],\n",
      "        [0.31333256, 0.02881237, 0.04125275, 0.06095347, 0.3740344 ,\n",
      "         0.07609891, 0.0180172 , 0.08749837],\n",
      "        [0.31394932, 0.02857177, 0.04167538, 0.06061973, 0.37324986,\n",
      "         0.07652834, 0.01804813, 0.08735742],\n",
      "        [0.3135996 , 0.02827596, 0.04198881, 0.06034742, 0.3729686 ,\n",
      "         0.07753161, 0.01809791, 0.08719015],\n",
      "        [0.31402692, 0.02814912, 0.04226538, 0.05990733, 0.37261027,\n",
      "         0.0776682 , 0.01813326, 0.08723947],\n",
      "        [0.3143657 , 0.02803187, 0.04256563, 0.05908568, 0.372536  ,\n",
      "         0.07784998, 0.01819474, 0.08737038],\n",
      "        [0.31310666, 0.02781614, 0.04288703, 0.05860728, 0.37357092,\n",
      "         0.07844361, 0.0182499 , 0.08731844],\n",
      "        [0.3129529 , 0.02776858, 0.04295649, 0.05818466, 0.37360182,\n",
      "         0.07873257, 0.01828296, 0.08751997],\n",
      "        [0.3131049 , 0.02770248, 0.04324251, 0.05794067, 0.37330386,\n",
      "         0.07901736, 0.01827226, 0.087416  ],\n",
      "        [0.3126304 , 0.02755511, 0.0434496 , 0.05777995, 0.37325156,\n",
      "         0.07975082, 0.01828949, 0.08729317],\n",
      "        [0.31266356, 0.02755026, 0.04363367, 0.05746198, 0.373255  ,\n",
      "         0.07982259, 0.01828446, 0.08732855],\n",
      "        [0.20373486, 0.42239165, 0.0309826 , 0.09733061, 0.08446035,\n",
      "         0.02311146, 0.02154168, 0.11644675],\n",
      "        [0.26766106, 0.21192695, 0.03162492, 0.09709893, 0.22977646,\n",
      "         0.04028459, 0.01403786, 0.10758924],\n",
      "        [0.28464553, 0.14258257, 0.03165989, 0.09040011, 0.28533578,\n",
      "         0.04612876, 0.01343392, 0.10581345],\n",
      "        [0.2924322 , 0.10407628, 0.03254991, 0.08722798, 0.31906956,\n",
      "         0.05000089, 0.01322561, 0.10141755],\n",
      "        [0.29767606, 0.07972138, 0.03332571, 0.08275837, 0.3406401 ,\n",
      "         0.05504112, 0.01328363, 0.09755359],\n",
      "        [0.29869485, 0.06629316, 0.03397058, 0.08047095, 0.35437787,\n",
      "         0.05682097, 0.01363185, 0.09573983],\n",
      "        [0.30101895, 0.05698091, 0.03478781, 0.07638041, 0.36357406,\n",
      "         0.05861221, 0.01406151, 0.09458413],\n",
      "        [0.30603346, 0.03966983, 0.03740465, 0.07132026, 0.37345415,\n",
      "         0.06525701, 0.01545304, 0.09140759],\n",
      "        [0.30721676, 0.0374653 , 0.03810482, 0.07024617, 0.37315553,\n",
      "         0.06749477, 0.0156968 , 0.09061977],\n",
      "        [0.30952445, 0.03587111, 0.03873293, 0.06929604, 0.37201288,\n",
      "         0.06822148, 0.01595475, 0.09038624],\n",
      "        [0.3114845 , 0.03458019, 0.03935904, 0.06752161, 0.37153217,\n",
      "         0.06897596, 0.01622134, 0.09032522],\n",
      "        [0.31147876, 0.03269803, 0.04010707, 0.06635238, 0.37233293,\n",
      "         0.07063788, 0.01660549, 0.08978735],\n",
      "        [0.31319985, 0.03146464, 0.04082983, 0.0649014 , 0.37103024,\n",
      "         0.07206959, 0.01689022, 0.08961418],\n",
      "        [0.31311083, 0.03086963, 0.04119941, 0.06440299, 0.3707917 ,\n",
      "         0.07335602, 0.0170016 , 0.08926782]],\n",
      "\n",
      "       [[0.35133547, 0.03356059, 0.04047151, 0.0630255 , 0.33498394,\n",
      "         0.0718997 , 0.01651799, 0.08820532],\n",
      "        [0.3477465 , 0.03276361, 0.04090958, 0.06233534, 0.33879814,\n",
      "         0.07248966, 0.01669367, 0.08826344],\n",
      "        [0.34435502, 0.03184573, 0.04117808, 0.06242961, 0.34207612,\n",
      "         0.07329852, 0.01689898, 0.08791797],\n",
      "        [0.3416542 , 0.03081279, 0.04184037, 0.06133237, 0.3448128 ,\n",
      "         0.07446596, 0.0171587 , 0.0879228 ],\n",
      "        [0.3389263 , 0.02998441, 0.04249859, 0.06051643, 0.34717095,\n",
      "         0.07595413, 0.01734795, 0.08760124],\n",
      "        [0.3382725 , 0.02974052, 0.04282722, 0.05947381, 0.34835652,\n",
      "         0.0761591 , 0.01746246, 0.08770783],\n",
      "        [0.3347062 , 0.02931853, 0.04311455, 0.05899359, 0.35186568,\n",
      "         0.07687524, 0.01759378, 0.08753245],\n",
      "        [0.3332808 , 0.02906646, 0.04345179, 0.05823984, 0.35317954,\n",
      "         0.07754661, 0.01767545, 0.08755955],\n",
      "        [0.33196113, 0.02881963, 0.04367234, 0.05805995, 0.35392445,\n",
      "         0.07847191, 0.0177198 , 0.0873708 ],\n",
      "        [0.33151937, 0.02877927, 0.04384715, 0.05770725, 0.35451743,\n",
      "         0.07851943, 0.0177372 , 0.08737293],\n",
      "        [0.33103102, 0.02872512, 0.04405996, 0.05700026, 0.35530064,\n",
      "         0.07863007, 0.01778656, 0.08746631],\n",
      "        [0.32980525, 0.02872788, 0.04416604, 0.05667022, 0.3565499 ,\n",
      "         0.07879371, 0.01779097, 0.08749595],\n",
      "        [0.3283491 , 0.02859076, 0.04423083, 0.05668604, 0.35784018,\n",
      "         0.07910968, 0.01782583, 0.08736756],\n",
      "        [0.327553  , 0.02857   , 0.04423225, 0.05635505, 0.3585441 ,\n",
      "         0.07934804, 0.01785441, 0.08754307],\n",
      "        [0.3271021 , 0.0285311 , 0.04446147, 0.05618684, 0.3588706 ,\n",
      "         0.07958806, 0.01783951, 0.08742043],\n",
      "        [0.32606444, 0.0283841 , 0.04462082, 0.05609459, 0.35939783,\n",
      "         0.08028796, 0.01785883, 0.08729132],\n",
      "        [0.32556635, 0.02839982, 0.04475508, 0.05585529, 0.35993928,\n",
      "         0.08032443, 0.01785054, 0.08730923],\n",
      "        [0.16143703, 0.4879931 , 0.03434916, 0.09804182, 0.08101351,\n",
      "         0.02896914, 0.01672506, 0.09147117],\n",
      "        [0.26797146, 0.28107136, 0.03562948, 0.10077227, 0.1561554 ,\n",
      "         0.03808758, 0.01349331, 0.10681916],\n",
      "        [0.35676843, 0.07459243, 0.03677349, 0.08370049, 0.28070405,\n",
      "         0.05423434, 0.01275731, 0.10046949],\n",
      "        [0.3571415 , 0.0597011 , 0.03770269, 0.0792699 , 0.2966355 ,\n",
      "         0.05920136, 0.01313671, 0.09721119],\n",
      "        [0.35656968, 0.05154191, 0.03855705, 0.07658843, 0.30646172,\n",
      "         0.06087127, 0.01362918, 0.09578081],\n",
      "        [0.3557875 , 0.04611144, 0.03940099, 0.07258936, 0.3145736 ,\n",
      "         0.06252097, 0.01413949, 0.09487654],\n",
      "        [0.35180107, 0.04270073, 0.03995496, 0.07070903, 0.32212216,\n",
      "         0.06406447, 0.01452118, 0.09412634],\n",
      "        [0.34806186, 0.0397926 , 0.04026034, 0.07022959, 0.328136  ,\n",
      "         0.06565849, 0.01490564, 0.09295548],\n",
      "        [0.34530133, 0.03638599, 0.04107632, 0.06778058, 0.33395934,\n",
      "         0.06803127, 0.01547093, 0.09199426],\n",
      "        [0.34298825, 0.03497483, 0.04151439, 0.06691717, 0.336681  ,\n",
      "         0.07001405, 0.01569201, 0.09121819],\n",
      "        [0.34238625, 0.03411723, 0.04186207, 0.06603122, 0.3383488 ,\n",
      "         0.07045622, 0.01589215, 0.09090602],\n",
      "        [0.34180915, 0.03337381, 0.0422472 , 0.06442358, 0.3402801 ,\n",
      "         0.07098062, 0.0161116 , 0.0907738 ],\n",
      "        [0.33989275, 0.03287057, 0.04246634, 0.06359742, 0.34277925,\n",
      "         0.07155015, 0.01625232, 0.09059121]]], dtype=float32), 'identifiers': array(['1_103520', '1_103665', '25_103665'], dtype=object), 'time': array([['2013-09-05', '2013-09-06', '2013-09-07', '2013-09-09',\n",
      "        '2013-09-10', '2013-09-11', '2013-09-12', '2013-09-19',\n",
      "        '2013-09-20', '2013-09-21', '2013-09-22', '2013-09-23',\n",
      "        '2013-09-24', '2013-09-25', '2013-09-26', '2013-09-27',\n",
      "        '2013-09-28', '2013-09-29', '2013-09-30', '2013-10-01',\n",
      "        '2013-10-02', '2013-10-03', '2013-10-04', '2013-10-05',\n",
      "        '2013-10-06', '2013-10-07', '2013-10-08', '2013-10-16',\n",
      "        '2013-10-17', '2013-10-18', '2013-10-19', '2013-10-20',\n",
      "        '2013-10-21', '2013-10-22', '2013-10-23', '2013-10-24',\n",
      "        '2013-10-25', '2013-10-26', '2013-10-27', '2013-10-28',\n",
      "        '2013-10-29', '2013-10-30', '2013-11-11', '2013-11-12',\n",
      "        '2013-11-13', '2013-11-14', '2013-11-16', '2013-11-17',\n",
      "        '2013-11-18', '2013-11-19', '2013-11-20', '2013-11-21',\n",
      "        '2013-11-22', '2013-11-23', '2013-11-24', '2013-11-25',\n",
      "        '2013-11-27', '2013-11-28', '2013-11-29', '2013-11-30',\n",
      "        '2013-12-01', '2013-12-02', '2013-12-09', '2013-12-18',\n",
      "        '2013-12-19', '2013-12-20', '2013-12-21', '2013-12-23',\n",
      "        '2013-12-24', '2013-12-27', '2014-01-07', '2014-01-08',\n",
      "        '2014-01-09', '2014-01-10', '2014-01-12', '2014-01-13',\n",
      "        '2014-01-15', '2014-01-16', '2014-01-17', '2014-01-18',\n",
      "        '2014-01-20', '2014-01-21', '2014-01-22', '2014-01-23',\n",
      "        '2014-01-24', '2014-01-28', '2014-01-29', '2014-01-30',\n",
      "        '2014-01-31', '2014-02-01', '2014-02-03', '2014-02-04',\n",
      "        '2014-02-05', '2014-02-06', '2014-02-07', '2014-02-08',\n",
      "        '2014-02-10', '2014-02-11', '2014-02-12', '2014-02-13',\n",
      "        '2014-02-14', '2014-02-15', '2014-02-17', '2014-02-18',\n",
      "        '2014-02-19', '2014-02-20', '2014-02-21', '2014-02-22',\n",
      "        '2014-02-26', '2014-02-27', '2014-02-28', '2014-03-05',\n",
      "        '2014-03-06', '2014-03-07', '2014-03-08', '2014-03-09',\n",
      "        '2014-03-10', '2014-03-11', '2014-03-12', '2014-03-13'],\n",
      "       ['2013-09-18', '2013-09-19', '2013-09-21', '2013-09-22',\n",
      "        '2013-09-23', '2013-09-24', '2013-09-25', '2013-09-26',\n",
      "        '2013-09-27', '2013-09-28', '2013-09-30', '2013-10-01',\n",
      "        '2013-10-02', '2013-10-03', '2013-10-04', '2013-10-07',\n",
      "        '2013-10-08', '2013-10-09', '2013-10-10', '2013-10-11',\n",
      "        '2013-10-12', '2013-10-13', '2013-10-14', '2013-10-15',\n",
      "        '2013-10-16', '2013-10-17', '2013-10-18', '2013-10-19',\n",
      "        '2013-10-22', '2013-10-23', '2013-10-24', '2013-10-25',\n",
      "        '2013-10-26', '2013-10-27', '2013-10-28', '2013-10-29',\n",
      "        '2013-10-30', '2013-10-31', '2013-11-01', '2013-11-02',\n",
      "        '2013-11-03', '2013-11-04', '2013-11-05', '2013-11-08',\n",
      "        '2013-11-09', '2013-11-11', '2013-11-12', '2013-11-13',\n",
      "        '2013-11-14', '2013-11-16', '2013-11-17', '2013-11-18',\n",
      "        '2013-11-19', '2013-11-20', '2013-11-21', '2013-11-25',\n",
      "        '2013-11-26', '2013-11-27', '2013-11-28', '2013-11-29',\n",
      "        '2013-12-03', '2013-12-04', '2013-12-05', '2013-12-09',\n",
      "        '2013-12-12', '2013-12-13', '2013-12-14', '2013-12-15',\n",
      "        '2013-12-16', '2013-12-18', '2013-12-19', '2013-12-20',\n",
      "        '2013-12-23', '2013-12-24', '2013-12-26', '2013-12-27',\n",
      "        '2013-12-28', '2013-12-30', '2013-12-31', '2014-01-02',\n",
      "        '2014-01-03', '2014-01-04', '2014-01-05', '2014-01-06',\n",
      "        '2014-01-07', '2014-01-08', '2014-01-10', '2014-01-11',\n",
      "        '2014-01-12', '2014-01-13', '2014-01-15', '2014-01-16',\n",
      "        '2014-01-17', '2014-01-18', '2014-01-19', '2014-01-20',\n",
      "        '2014-01-21', '2014-01-22', '2014-01-23', '2014-01-24',\n",
      "        '2014-01-25', '2014-01-27', '2014-01-28', '2014-01-29',\n",
      "        '2014-01-30', '2014-01-31', '2014-02-01', '2014-02-03',\n",
      "        '2014-02-04', '2014-02-05', '2014-02-06', '2014-02-07',\n",
      "        '2014-02-08', '2014-02-12', '2014-02-13', '2014-02-14',\n",
      "        '2014-02-15', '2014-02-17', '2014-02-19', '2014-02-20'],\n",
      "       ['2013-09-08', '2013-09-10', '2013-09-11', '2013-09-12',\n",
      "        '2013-09-13', '2013-09-14', '2013-09-15', '2013-09-16',\n",
      "        '2013-09-17', '2013-09-19', '2013-09-20', '2013-09-21',\n",
      "        '2013-09-22', '2013-09-23', '2013-09-25', '2013-09-27',\n",
      "        '2013-09-28', '2013-09-29', '2013-09-30', '2013-10-02',\n",
      "        '2013-10-04', '2013-10-07', '2013-10-08', '2013-10-09',\n",
      "        '2013-10-10', '2013-10-11', '2013-10-13', '2013-10-14',\n",
      "        '2013-10-16', '2013-10-17', '2013-10-18', '2013-10-19',\n",
      "        '2013-10-21', '2013-10-22', '2013-10-23', '2013-10-25',\n",
      "        '2013-10-26', '2013-10-28', '2013-10-31', '2013-11-01',\n",
      "        '2013-11-03', '2013-11-05', '2013-11-08', '2013-11-09',\n",
      "        '2013-11-10', '2013-11-12', '2013-11-13', '2013-11-14',\n",
      "        '2013-11-15', '2013-11-16', '2013-11-17', '2013-11-18',\n",
      "        '2013-11-20', '2013-11-21', '2013-11-23', '2013-11-24',\n",
      "        '2013-11-26', '2013-11-27', '2013-11-28', '2013-11-29',\n",
      "        '2013-12-01', '2013-12-03', '2013-12-04', '2013-12-05',\n",
      "        '2013-12-06', '2013-12-08', '2013-12-10', '2013-12-11',\n",
      "        '2013-12-12', '2013-12-13', '2013-12-17', '2013-12-18',\n",
      "        '2013-12-19', '2013-12-20', '2013-12-21', '2013-12-22',\n",
      "        '2013-12-23', '2013-12-24', '2013-12-26', '2013-12-27',\n",
      "        '2013-12-28', '2013-12-29', '2013-12-31', '2014-01-01',\n",
      "        '2014-01-02', '2014-01-03', '2014-01-07', '2014-01-08',\n",
      "        '2014-01-09', '2014-01-10', '2014-01-11', '2014-01-12',\n",
      "        '2014-01-13', '2014-01-15', '2014-01-17', '2014-01-18',\n",
      "        '2014-01-20', '2014-01-22', '2014-01-23', '2014-01-24',\n",
      "        '2014-01-25', '2014-01-26', '2014-01-27', '2014-01-28',\n",
      "        '2014-01-29', '2014-01-30', '2014-01-31', '2014-02-01',\n",
      "        '2014-02-02', '2014-02-05', '2014-02-06', '2014-02-07',\n",
      "        '2014-02-08', '2014-02-09', '2014-02-10', '2014-02-12',\n",
      "        '2014-02-13', '2014-02-14', '2014-02-15', '2014-02-16']],\n",
      "      dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "data = _batch_data(test)\n",
    "inputs = data['inputs'] # (3, 120, 20)\n",
    "identifiers = data['identifier'] # (3, 120, 1)\n",
    "time = data['time'] # (3, 120, 1)\n",
    "\n",
    "def get_batch_attention_weights(input_batch):\n",
    "      \"\"\"Returns weights for a given minibatch of data.\"\"\"\n",
    "      input_placeholder = _input_placeholder\n",
    "      attention_weights = {}\n",
    "      for k in _attention_components:\n",
    "        attention_weight = tf.keras.backend.get_session().run(\n",
    "            _attention_components[k],\n",
    "            {input_placeholder: input_batch.astype(np.float32)})\n",
    "        attention_weights[k] = attention_weight\n",
    "      return attention_weights\n",
    "\n",
    "    # Compute number of batches\n",
    "batch_size = minibatch_size\n",
    "n = inputs.shape[0]\n",
    "num_batches = n // batch_size\n",
    "if n - (num_batches * batch_size) > 0:\n",
    "      num_batches += 1\n",
    "\n",
    "# Split up inputs into batches\n",
    "batched_inputs = [\n",
    "        inputs[i * batch_size:(i + 1) * batch_size, Ellipsis]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "\n",
    "# Get attention weights, while avoiding large memory increases\n",
    "attention_by_batch = [\n",
    "        get_batch_attention_weights(batch) for batch in batched_inputs\n",
    "    ]\n",
    "attention_weights = {}\n",
    "for k in _attention_components:\n",
    "      attention_weights[k] = []\n",
    "      for batch_weights in attention_by_batch:\n",
    "        attention_weights[k].append(batch_weights[k])\n",
    "\n",
    "      if len(attention_weights[k][0].shape) == 4:\n",
    "        tmp = np.concatenate(attention_weights[k], axis=1)\n",
    "      else:\n",
    "        tmp = np.concatenate(attention_weights[k], axis=0)\n",
    "\n",
    "      del attention_weights[k]\n",
    "      gc.collect()\n",
    "      attention_weights[k] = tmp\n",
    "\n",
    "attention_weights['identifiers'] = identifiers[:, 0, 0]\n",
    "attention_weights['time'] = time[:, :, 0]\n",
    "\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
