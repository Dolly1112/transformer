{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import gc\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from expt_settings.configs import ExperimentConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyunpack\n",
    "import wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General functions for data downloading & aggregation.\n",
    "def download_from_url(url, output_path):\n",
    "  \"\"\"Downloads a file froma url.\"\"\"\n",
    "\n",
    "  print('Pulling data from {} to {}'.format(url, output_path))\n",
    "  wget.download(url, output_path)\n",
    "  print('done')\n",
    "\n",
    "\n",
    "def recreate_folder(path):\n",
    "  \"\"\"Deletes and recreates folder.\"\"\"\n",
    "\n",
    "  shutil.rmtree(path)\n",
    "  os.makedirs(path)\n",
    "\n",
    "\n",
    "def unzip(zip_path, output_file, data_folder):\n",
    "  \"\"\"Unzips files and checks successful completion.\"\"\"\n",
    "\n",
    "  print('Unzipping file: {}'.format(zip_path))\n",
    "  pyunpack.Archive(zip_path).extractall(data_folder)\n",
    "\n",
    "  # Checks if unzip was successful\n",
    "  if not os.path.exists(output_file):\n",
    "    raise ValueError(\n",
    "        'Error in unzipping process! {} not found.'.format(output_file))\n",
    "\n",
    "\n",
    "def download_and_unzip(url, zip_path, csv_path, data_folder):\n",
    "  \"\"\"Downloads and unzips an online csv file.\n",
    "\n",
    "  Args:\n",
    "    url: Web address\n",
    "    zip_path: Path to download zip file\n",
    "    csv_path: Expected path to csv file\n",
    "    data_folder: Folder in which data is stored.\n",
    "  \"\"\"\n",
    "\n",
    "  download_from_url(url, zip_path)\n",
    "\n",
    "  unzip(zip_path, csv_path, data_folder)\n",
    "\n",
    "  print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data from https://archive.ics.uci.edu/ml/machine-learning-databases/00204/PEMS-SF.zip to 0927_dataset_traffic\\PEMS-SF.zip\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Downloads traffic dataset from UCI repository.\"\"\"\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00204/PEMS-SF.zip'\n",
    "\n",
    "data_folder = '0927_dataset_traffic'\n",
    "csv_path = os.path.join(data_folder, 'PEMS_train')\n",
    "zip_path = os.path.join(data_folder, 'PEMS-SF.zip')\n",
    "\n",
    "download_and_unzip(url, zip_path, csv_path, data_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aggregating to hourly data')\n",
    "\n",
    "def process_list(s, variable_type=int, delimiter=None):\n",
    "    \"\"\"Parses a line in the PEMS format to a list.\"\"\"3\n",
    "    if delimiter is None:\n",
    "      l = [\n",
    "          variable_type(i) for i in s.replace('[', '').replace(']', '').split()\n",
    "      ]\n",
    "    else:\n",
    "      l = [\n",
    "          variable_type(i)\n",
    "          for i in s.replace('[', '').replace(']', '').split(delimiter)\n",
    "      ]\n",
    "\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_single_list(filename):\n",
    "    \"\"\"Returns single list from a file in the PEMS-custom format.\"\"\"\n",
    "    with open(os.path.join(data_folder, filename), 'r') as dat:\n",
    "      l = process_list(dat.readlines()[0])\n",
    "    return l\n",
    "\n",
    "def read_matrix(filename):\n",
    "    \"\"\"Returns a matrix from a file in the PEMS-custom format.\"\"\"\n",
    "    array_list = []\n",
    "    with open(os.path.join(data_folder, filename), 'r') as dat:\n",
    "\n",
    "      lines = dat.readlines()\n",
    "      for i, line in enumerate(lines):\n",
    "        if (i + 1) % 50 == 0:\n",
    "          print('Completed {} of {} rows for {}'.format(i + 1, len(lines),\n",
    "                                                        filename))\n",
    "\n",
    "        array = [\n",
    "            process_list(row_split, variable_type=float, delimiter=None)\n",
    "            for row_split in process_list(\n",
    "                line, variable_type=str, delimiter=';')\n",
    "        ]\n",
    "        array_list.append(array)\n",
    "\n",
    "    return array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_order = np.array(read_single_list('randperm')) - 1# index from 0\n",
    "train_dayofweek = read_single_list('PEMS_trainlabels')\n",
    "train_tensor = read_matrix('PEMS_train')\n",
    "test_dayofweek = read_single_list('PEMS_testlabels')\n",
    "test_tensor = read_matrix('PEMS_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse permutate shuffle order\n",
    "print('Shuffling')\n",
    "inverse_mapping = {\n",
    "      new_location: previous_location\n",
    "      for previous_location, new_location in enumerate(shuffle_order)\n",
    "  }\n",
    "reverse_shuffle_order = np.array([\n",
    "      inverse_mapping[new_location]\n",
    "      for new_location, _ in enumerate(shuffle_order)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and reoder based on permuation matrix\n",
    "print('Reodering')\n",
    "day_of_week = np.array(train_dayofweek + test_dayofweek)\n",
    "combined_tensor = np.array(train_tensor + test_tensor)\n",
    "\n",
    "day_of_week = day_of_week[reverse_shuffle_order]\n",
    "combined_tensor = combined_tensor[reverse_shuffle_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything back into a dataframe\n",
    "print('Parsing as dataframe')\n",
    "labels = ['traj_{}'.format(i) for i in read_single_list('stations_list')]\n",
    "\n",
    "hourly_list = []\n",
    "for day, day_matrix in enumerate(combined_tensor):\n",
    "\n",
    "    # Hourly data\n",
    "    hourly = pd.DataFrame(day_matrix.T, columns=labels)\n",
    "    hourly['hour_on_day'] = [int(i / 6) for i in hourly.index\n",
    "                            ]  # sampled at 10 min intervals\n",
    "    if hourly['hour_on_day'].max() > 23 or hourly['hour_on_day'].min() < 0:\n",
    "      raise ValueError('Invalid hour! {}-{}'.format(\n",
    "          hourly['hour_on_day'].min(), hourly['hour_on_day'].max()))\n",
    "\n",
    "    hourly = hourly.groupby('hour_on_day', as_index=True).mean()[labels]\n",
    "    hourly['sensor_day'] = day\n",
    "    hourly['time_on_day'] = hourly.index\n",
    "    hourly['day_of_week'] = day_of_week[day]\n",
    "\n",
    "    hourly_list.append(hourly)\n",
    "hourly_frame = pd.concat(hourly_list, axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten such that each entitiy uses one row in dataframe\n",
    "store_columns = [c for c in hourly_frame.columns if 'traj' in c]\n",
    "other_columns = [c for c in hourly_frame.columns if 'traj' not in c]\n",
    "flat_df = pd.DataFrame(columns=['values', 'prev_values', 'next_values'] +\n",
    "                         other_columns + ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_index_string(x):\n",
    "    \"\"\"Returns formatted string for key.\"\"\"\n",
    "\n",
    "    if x < 10:\n",
    "      return '00' + str(x)\n",
    "    elif x < 100:\n",
    "      return '0' + str(x)\n",
    "    elif x < 1000:\n",
    "      return str(x)\n",
    "\n",
    "    raise ValueError('Invalid value of x {}'.format(x))\n",
    "\n",
    "for store in store_columns:\n",
    "    print('Processing {}'.format(store))\n",
    "\n",
    "    sliced = hourly_frame[[store] + other_columns].copy()\n",
    "    sliced.columns = ['values'] + other_columns\n",
    "    sliced['id'] = int(store.replace('traj_', ''))\n",
    "\n",
    "    # Sort by Sensor-date-time\n",
    "    key = sliced['id'].apply(str) \\\n",
    "      + sliced['sensor_day'].apply(lambda x: '_' + format_index_string(x)) \\\n",
    "        + sliced['time_on_day'].apply(lambda x: '_' + format_index_string(x))\n",
    "    sliced = sliced.set_index(key).sort_index()\n",
    "\n",
    "    sliced['values'] = sliced['values'].fillna(method='ffill')\n",
    "    sliced['prev_values'] = sliced['values'].shift(1)\n",
    "    sliced['next_values'] = sliced['values'].shift(-1)\n",
    "\n",
    "    flat_df = flat_df.append(sliced.dropna(), ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to match range used by other academic papers\n",
    "index = flat_df['sensor_day']\n",
    "flat_df = flat_df[index < 173].copy()\n",
    "\n",
    "# Creating columns fo categorical inputs\n",
    "flat_df['categorical_id'] = flat_df['id'].copy()\n",
    "flat_df['hours_from_start'] = flat_df['time_on_day'] \\\n",
    "+ flat_df['sensor_day']*24.\n",
    "flat_df['categorical_day_of_week'] = flat_df['day_of_week'].copy()\n",
    "flat_df['categorical_time_on_day'] = flat_df['time_on_day'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = '0927_traffic_out.csv'\n",
    "flat_df.to_csv(data_csv_path)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = '0927_traffic_out.csv'\n",
    "df = pd.read_csv(data_csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_formatters.base\n",
    "import data_formatters.volatility\n",
    "\n",
    "VolatilityFormatter = data_formatters.volatility.VolatilityFormatter\n",
    "DataTypes = data_formatters.base.DataTypes\n",
    "InputTypes = data_formatters.base.InputTypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_column_definition = [\n",
    "      ('id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.TIME),\n",
    "      ('values', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "      ('time_on_day', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('day_of_week', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('categorical_id', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Formatting train-valid-test splits.')\n",
    "valid_boundary=151\n",
    "test_boundary=166\n",
    "index = df['sensor_day']\n",
    "train = df.loc[index < valid_boundary]\n",
    "valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "test = df.loc[index >= test_boundary - 7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting scalers with training data...')\n",
    "column_definitions = get_column_definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions)\n",
    "target_column = utils.get_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract identifiers in case required\n",
    "identifiers = list(df[id_column].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Format real scalers\n",
    "real_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "data = df[real_inputs].values \n",
    "_real_scalers = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "_target_scaler = sklearn.preprocessing.StandardScaler().fit(\n",
    "        df[[target_column]].values)  # used for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Format categorical scalers\n",
    "categorical_inputs_name = utils.extract_cols_from_data_type(\n",
    "          DataTypes.CATEGORICAL, real_inputs + categorical_inputs,\n",
    "          {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "categorical_scalers = {}\n",
    "num_classes = []\n",
    "\n",
    "valid_idx = df['traj_id'].apply(lambda x: x in set(identifiers))\n",
    "for col in categorical_inputs_name:\n",
    "# Set all to str so that we don't have mixed integer/string columns\n",
    "    srs = df[col].apply(str).loc[valid_idx]\n",
    "    categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(\n",
    "            srs.values)\n",
    "    num_classes.append(srs.nunique())\n",
    "num_classes_per_cat_input = num_classes\n",
    "_cat_scalers = categorical_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_inputs(df):\n",
    "    \"\"\"Performs feature transformations.\n",
    "\n",
    "    This includes both feature engineering, preprocessing and normalisation.\n",
    "\n",
    "    Args:\n",
    "      df: Data frame to transform.\n",
    "\n",
    "    Returns:\n",
    "      Transformed data frame.\n",
    "\n",
    "    \"\"\"\n",
    "    output = df.copy()\n",
    "\n",
    "    if _real_scalers is None and _cat_scalers is None:\n",
    "      raise ValueError('Scalers have not been set!')\n",
    "\n",
    "    column_definitions = get_column_definition()\n",
    "\n",
    "    # (1) Format real inputs: standardization\n",
    "    real_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.REAL_VALUED, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "    \n",
    "    output[real_inputs] = _real_scalers.transform(df[real_inputs].values)\n",
    "\n",
    "    # (2) Format categorical inputs: LabelEncoder()\n",
    "    categorical_inputs = utils.extract_cols_from_data_type(\n",
    "        DataTypes.CATEGORICAL, column_definitions,\n",
    "        {InputTypes.ID, InputTypes.TIME})\n",
    "    \n",
    "    for col in categorical_inputs:\n",
    "      string_df = df[col].apply(str)\n",
    "      output[col] = _cat_scalers[col].transform(string_df)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transform_inputs(train) \n",
    "valid_tf = transform_inputs(valid) \n",
    "test_tf = transform_inputs(test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export file\n",
    "train_tf.to_csv('train_tf.csv',index=False)\n",
    "valid_tf.to_csv('valid_tf.csv',index=False)\n",
    "test_tf.to_csv('test_tf.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params = {\n",
    "        'total_time_steps': 8 * 24,\n",
    "        'num_encoder_steps': 7 * 24,\n",
    "        'num_epochs': 100,\n",
    "        'early_stopping_patience': 5,\n",
    "        'multiprocessing_workers': 5\n",
    "    }\n",
    "\n",
    "# default optimised model parameters.\n",
    "model_params = {\n",
    "        'dropout_rate': 0.3,\n",
    "        'hidden_layer_size': 320,\n",
    "        'learning_rate': 0.001,\n",
    "        'minibatch_size': 128,\n",
    "        'max_gradient_norm': 100.,\n",
    "        'num_heads': 4,\n",
    "        'stack_size': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_samples_for_calibration(self):\n",
    "    \"\"\"Gets the default number of training and validation samples.\n",
    "\n",
    "    Use to sub-sample the data for network calibration and a value of -1 uses\n",
    "    all available samples.\n",
    "\n",
    "    Returns:\n",
    "      Tuple of (training samples, validation samples)\n",
    "    \"\"\"\n",
    "    return 450000, 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_tft_input_indices():\n",
    "    \"\"\"Returns the relevant indexes and input sizes required by TFT.\"\"\"\n",
    "\n",
    "    # Functions\n",
    "    def _get_locations(input_types, defn):\n",
    "      return [i for i, tup in enumerate(defn) if tup[2] in input_types]\n",
    "\n",
    "    locations = {\n",
    "        'input_size': # not a str\n",
    "            len(real_inputs + categorical_inputs) ,# remove ID, TIME\n",
    "\n",
    "        'output_size': # loc within total inputs\n",
    "            len(_get_locations({InputTypes.TARGET}, real_inputs + categorical_inputs)),\n",
    "\n",
    "        'category_counts':\n",
    "            num_classes_per_cat_input,\n",
    "            \n",
    "        'input_obs_loc': # loc within total inputs\n",
    "            _get_locations({InputTypes.TARGET}, real_inputs + categorical_inputs),\n",
    "\n",
    "        'static_input_loc':# loc within total inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT}, real_inputs + categorical_inputs),\n",
    "\n",
    "        'known_regular_inputs':# loc within real_inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           real_inputs), \n",
    "\n",
    "        'known_categorical_inputs':# loc within categorical_inputs\n",
    "            _get_locations({InputTypes.STATIC_INPUT, InputTypes.KNOWN_INPUT},\n",
    "                           categorical_inputs)  \n",
    "    }\n",
    "    return locations\n",
    "fixed_params.update(_get_tft_input_indices())\n",
    "fixed_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
