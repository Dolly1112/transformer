{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import data_formatters.base\n",
    "import libs.utils as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer definitions.\n",
    "concat = tf.keras.backend.concatenate\n",
    "stack = tf.keras.backend.stack\n",
    "K = tf.keras.backend\n",
    "Add = tf.keras.layers.Add\n",
    "LayerNorm = tf.keras.layers.LayerNormalization\n",
    "Dense = tf.keras.layers.Dense\n",
    "Multiply = tf.keras.layers.Multiply\n",
    "Dropout = tf.keras.layers.Dropout # Inputs elements are randomly set to zero (and the other elements are rescaled)\n",
    "Activation = tf.keras.layers.Activation\n",
    "Lambda = tf.keras.layers.Lambda\n",
    "\n",
    "# Default input types.\n",
    "InputTypes = data_formatters.base.InputTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import enum\n",
    "class DataTypes(enum.IntEnum):\n",
    "  \"\"\"Defines numerical types of each column.\"\"\"\n",
    "  REAL_VALUED = 0\n",
    "  CATEGORICAL = 1\n",
    "  DATE = 2\n",
    "\n",
    "class InputTypes(enum.IntEnum):\n",
    "  \"\"\"Defines input types of each column.\"\"\"\n",
    "  TARGET = 0\n",
    "  OBSERVED_INPUT = 1\n",
    "  KNOWN_INPUT = 2\n",
    "  STATIC_INPUT = 3\n",
    "  ID = 4  # Single column used as an entity identifier\n",
    "  TIME = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "with open('params_update.pkl', 'rb') as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.1,\n",
       " 'hidden_layer_size': 5,\n",
       " 'learning_rate': 0.001,\n",
       " 'max_gradient_norm': 100.0,\n",
       " 'minibatch_size': 128,\n",
       " 'model_folder': '0615_result',\n",
       " 'num_heads': 4,\n",
       " 'stack_size': 1,\n",
       " 'total_time_steps': 120,\n",
       " 'num_encoder_steps': 90,\n",
       " 'num_epochs': 1,\n",
       " 'early_stopping_patience': 5,\n",
       " 'multiprocessing_workers': 5,\n",
       " 'column_definition': [('traj_id',\n",
       "   <DataTypes.REAL_VALUED: 0>,\n",
       "   <InputTypes.ID: 4>),\n",
       "  ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>),\n",
       "  ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>),\n",
       "  ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>),\n",
       "  ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>),\n",
       "  ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>),\n",
       "  ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)],\n",
       " 'input_size': 20,\n",
       " 'output_size': 1,\n",
       " 'category_counts': [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4],\n",
       " 'input_obs_loc': [0],\n",
       " 'static_input_loc': [6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
       " 'known_regular_inputs': [3, 4, 5],\n",
       " 'known_categorical_inputs': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "time_steps = int(params['total_time_steps'])\n",
    "input_size = int(params['input_size'])\n",
    "output_size = int(params['output_size'])\n",
    "category_counts = json.loads(str(params['category_counts'])) \n",
    "n_multiprocessing_workers = int(params['multiprocessing_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant indices for TFT\n",
    "_input_obs_loc = json.loads(str(params['input_obs_loc']))\n",
    "_static_input_loc = json.loads(str(params['static_input_loc']))\n",
    "_known_regular_input_idx = json.loads(\n",
    "        str(params['known_regular_inputs']))\n",
    "_known_categorical_input_idx = json.loads(\n",
    "        str(params['known_categorical_inputs']))\n",
    "    # json.loads: parse a valid JSON string and convert it into a Python Dictionary\n",
    "column_definition = params['column_definition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network params\n",
    "quantiles = [0.1, 0.5, 0.9]\n",
    "use_cudnn = False  # Whether to use GPU optimised LSTM\n",
    "hidden_layer_size = int(params['hidden_layer_size'])\n",
    "dropout_rate = float(params['dropout_rate'])\n",
    "max_gradient_norm = float(params['max_gradient_norm'])\n",
    "learning_rate = float(params['learning_rate'])\n",
    "minibatch_size = int(params['minibatch_size'])\n",
    "num_epochs = int(params['num_epochs'])\n",
    "early_stopping_patience = int(params['early_stopping_patience'])\n",
    "\n",
    "num_encoder_steps = int(params['num_encoder_steps'])\n",
    "num_stacks = int(params['stack_size'])\n",
    "num_heads = int(params['num_heads'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting temp folder...\n",
      "# dropout_rate = 0.1\n",
      "# hidden_layer_size = 5\n",
      "# learning_rate = 0.001\n",
      "# max_gradient_norm = 100.0\n",
      "# minibatch_size = 128\n",
      "# model_folder = 0615_result\n",
      "# num_heads = 4\n",
      "# stack_size = 1\n",
      "# total_time_steps = 120\n",
      "# num_encoder_steps = 90\n",
      "# num_epochs = 1\n",
      "# early_stopping_patience = 5\n",
      "# multiprocessing_workers = 5\n",
      "# column_definition = [('traj_id', <DataTypes.REAL_VALUED: 0>, <InputTypes.ID: 4>), ('date', <DataTypes.DATE: 2>, <InputTypes.TIME: 5>), ('log_sales', <DataTypes.REAL_VALUED: 0>, <InputTypes.TARGET: 0>), ('transactions', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('oil', <DataTypes.REAL_VALUED: 0>, <InputTypes.OBSERVED_INPUT: 1>), ('day_of_month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('month', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('open', <DataTypes.REAL_VALUED: 0>, <InputTypes.KNOWN_INPUT: 2>), ('item_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('store_nbr', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('city', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('state', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('type', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('cluster', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('family', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('class', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('perishable', <DataTypes.CATEGORICAL: 1>, <InputTypes.STATIC_INPUT: 3>), ('onpromotion', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('day_of_week', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('national_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('regional_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>), ('local_hol', <DataTypes.CATEGORICAL: 1>, <InputTypes.KNOWN_INPUT: 2>)]\n",
      "# input_size = 20\n",
      "# output_size = 1\n",
      "# category_counts = [3, 2, 2, 2, 1, 2, 2, 3, 2, 3, 7, 35, 2, 4]\n",
      "# input_obs_loc = [0]\n",
      "# static_input_loc = [6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "# known_regular_inputs = [3, 4, 5]\n",
      "# known_categorical_inputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "# Serialisation options\n",
    "_temp_folder = os.path.join(params['model_folder'], 'tmp')\n",
    "print('Resetting temp folder...')\n",
    "utils.create_folder_if_not_exist(_temp_folder)\n",
    "shutil.rmtree(_temp_folder)\n",
    "os.makedirs(_temp_folder)\n",
    "\n",
    "# Extra components to store Tensorflow nodes for attention computations\n",
    "_input_placeholder = None\n",
    "_attention_components = None\n",
    "_prediction_parts = None\n",
    "\n",
    "#print('*** {} params ***'.format())\n",
    "for k in params:\n",
    "    print('# {} = {}'.format(k, params[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "num_categorical_variables = len(category_counts) # num_categorical_variables=14\n",
    "num_regular_variables = input_size - num_categorical_variables # num_regular_variables=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## def _build_base_graph(self):\n",
    "# Inputs.\n",
    "all_inputs = tf.compat.v1.keras.layers.Input(\n",
    "        shape=(\n",
    "            time_steps, \n",
    "            input_size,\n",
    "        )) # shape=(None, 120, 20)\n",
    "regular_inputs, categorical_inputs \\\n",
    "        = all_inputs[:, :, :num_regular_variables],\\\n",
    "          all_inputs[:, :, num_regular_variables:]\n",
    "# regular_inputs: shape=(None, 120, 6)\n",
    "# categorical_inputs: shape=(None, 120, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (1) Embedding for Numerical variables\n",
    "def convert_real_to_embedding(x):\n",
    "      \"\"\"Applies linear transformation for time-varying inputs.\"\"\"\n",
    "      return tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "              x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (2) Embedding for categorical variables\n",
    "embedding_sizes = [\n",
    "        hidden_layer_size for i, size in enumerate(category_counts)\n",
    "    ]\n",
    "embeddings = []\n",
    "for i in range(num_categorical_variables):\n",
    "      embedding = tf.keras.Sequential([\n",
    "          tf.keras.layers.InputLayer([time_steps]),\n",
    "          tf.keras.layers.Embedding(\n",
    "              category_counts[i], # input_dim\n",
    "              embedding_sizes[i], # output_dim , embedding_sizes[i]=hidden_layer_size\n",
    "              input_length=time_steps,\n",
    "              dtype=tf.float32)\n",
    "      ]) \n",
    "      embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.keras.layers.InputLayer([time_steps]) defines an input layer for a neural network model. \n",
    "\n",
    "The time_steps parameter specifies the shape of the input data, indicating how many time steps the model will process at once, which is especially useful for sequence data like time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical_inputs转化为embedded_inputs\n",
    "### categorical_inputs: shape=(None, 120, 14)\n",
    "### embedded_inputs: 14个shape=(None, 120, 5)\n",
    "embedded_inputs = [\n",
    "        embeddings[i](categorical_inputs[Ellipsis, i]) # categorical_inputs[Ellipsis, i]: shape=(None, 120)\n",
    "        for i in range(num_categorical_variables)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sequential/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_1/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_2/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_3/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_4/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_5/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_6/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_7/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_8/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_9/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_10/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_11/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_12/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_13/Identity:0' shape=(None, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when a list of keras tensors that each has 3-dim, if we want to concat along last axis:\n",
    "def keras_concat_last_axis(t1):\n",
    "    temp_concat_swapaxes=[]\n",
    "    for i in np.arange(len(t1)):\n",
    "        temp_concat_swapaxes.append(tf.keras.ops.swapaxes((t1)[i],1,2))\n",
    "    stack_temp = tf.keras.ops.hstack(temp_concat_swapaxes)\n",
    "    result = tf.keras.ops.swapaxes(stack_temp,1,2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Static inputs: shape=(None, 9, 5)\n",
    "# static_inputs = [Dense(for each regular_inputs) + embedded_inputs]\n",
    "if _static_input_loc:\n",
    "      static_inputs = [tf.keras.layers.Dense(hidden_layer_size)(\n",
    "          regular_inputs[:, 0, i:i + 1]) for i in range(num_regular_variables) # dim(1)=0\n",
    "                       if i in _static_input_loc] \\\n",
    "            + [embedded_inputs[i][:, 0, :] # dim(1)=0\n",
    "             for i in range(num_categorical_variables)\n",
    "             if i + num_regular_variables in _static_input_loc]\n",
    "      \n",
    "      static_inputs = stack(static_inputs, axis=1) # concat base on axis=1\n",
    "\n",
    "else:\n",
    "      static_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack:0' shape=(None, 9, 5) dtype=float32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'strided_slice_25:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_26:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_27:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_28:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_29:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_30:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_31:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_32:0' shape=(None, 5) dtype=float32>]\n",
      "[<tf.Tensor 'strided_slice_33:0' shape=(None, 5) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_categorical_variables):\n",
    "    if i + num_regular_variables in _static_input_loc:\n",
    "        #static_inputs = [embedded_inputs[i][:, 0, :]]\n",
    "        print([embedded_inputs[i][:, 0, :]])\n",
    "        #static_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_real_to_embedding(x):\n",
    "      \"\"\"Applies linear transformation for time-varying inputs.\"\"\"\n",
    "      return tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "              x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Targets: shape=(None, 120, 5, 1)\n",
    "# obs_inputs = TimeDistributed(Dense())(regular_inputs)  \n",
    "obs_inputs = tf.keras.backend.stack([\n",
    "        convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1]) # for ith regular variables\n",
    "        for i in _input_obs_loc\n",
    "    ],axis=-1)\n",
    "# regular_inputs[Ellipsis, i:i + 1]: (None, 120, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"time_distributed_1/Identity:0\", shape=(None, 120, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i in _input_obs_loc:\n",
    "\n",
    "    print(tf.keras.layers.TimeDistributed(\n",
    "\n",
    "          tf.keras.layers.Dense(hidden_layer_size))(\n",
    "            \n",
    "              regular_inputs[Ellipsis, 0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observed (a priori unknown) inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Observed (a priori unknown) inputs: shape=(None, 120, 5, 2)\n",
    "# unknown_inputs = unknown_inputs + wired_embeddings\n",
    "wired_embeddings = [] # for categorical & not belongs to known & not belongs to _input_obs_loc\n",
    "for i in range(num_categorical_variables):\n",
    "      if i not in _known_categorical_input_idx \\\n",
    "        and  i + num_regular_variables  not in _input_obs_loc:\n",
    "        e = embeddings[i](categorical_inputs[:, :, i])\n",
    "        wired_embeddings.append(e)\n",
    "\n",
    "unknown_inputs_temp = [] # for regular_inputs & not belongs to known & not belongs to _input_obs_loc\n",
    "for i in range(regular_inputs.shape[-1]):\n",
    "      if i not in _known_regular_input_idx \\\n",
    "          and i not in _input_obs_loc:\n",
    "        e = convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "        unknown_inputs_temp.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unknown_inputs_temp + wired_embeddings:\n",
    "      unknown_inputs = tf.keras.backend.stack(unknown_inputs_temp + wired_embeddings, axis=-1)\n",
    "else:\n",
    "      unknown_inputs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_2:0' shape=(None, 120, 5, 2) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'time_distributed_2/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_3/Identity:0' shape=(None, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_inputs_temp + wired_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A priori known inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) A priori known inputs\n",
    "    # known_combined_layer = known_regular_inputs + known_categorical_inputs\n",
    "\n",
    "known_regular_inputs = [ # for _known_regular & not belongs to _static_input\n",
    "        convert_real_to_embedding(regular_inputs[Ellipsis, i:i + 1])\n",
    "        for i in _known_regular_input_idx\n",
    "        if i not in _static_input_loc\n",
    "    ] \n",
    "known_categorical_inputs = [ # for _known_categorical & & not belongs to _static_input\n",
    "        embedded_inputs[i]\n",
    "        for i in _known_categorical_input_idx\n",
    "        if i + num_regular_variables not in _static_input_loc\n",
    "    ]\n",
    "\n",
    "known_combined_layer = tf.keras.backend.stack(known_regular_inputs + known_categorical_inputs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'stack_3:0' shape=(None, 120, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_combined_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'time_distributed_4/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_5/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'time_distributed_6/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_9/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_10/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_11/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_12/Identity:0' shape=(None, 120, 5) dtype=float32>,\n",
       " <tf.Tensor 'sequential_13/Identity:0' shape=(None, 120, 5) dtype=float32>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_regular_inputs + known_categorical_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate known and observed historical inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_41:0' shape=(None, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_combined_layer[:, num_encoder_steps:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate known and observed historical inputs.\n",
    "if unknown_inputs is not None:\n",
    "      historical_inputs = concat([\n",
    "          unknown_inputs[:, :num_encoder_steps, :], # shape=(None, 90, 5, 2)\n",
    "          known_combined_layer[:, :num_encoder_steps, :], # shape=(None, 90, 5, 8)\n",
    "          obs_inputs[:, :num_encoder_steps, :] # shape=(None, 90, 5, 1)\n",
    "      ],axis=-1\n",
    "      )\n",
    "else:\n",
    "      historical_inputs = concat([\n",
    "          known_combined_layer[:, :num_encoder_steps, :],\n",
    "          obs_inputs[:, :num_encoder_steps, :]\n",
    "      ],axis=-1\n",
    "      )\n",
    "# historical_inputs.get_shape().as_list() = _, time_steps, embedding_dim, num_inputs\n",
    "\n",
    "# Isolate only known future inputs.\n",
    "future_inputs = known_combined_layer[:, num_encoder_steps:, :] # shape=(None, 30, 5, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(None, 90, 5, 11) dtype=float32>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_45:0' shape=(None, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module\n",
    "#### Basic Module\n",
    "##### linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_layer(size,\n",
    "                 activation=None,\n",
    "                 use_time_distributed=False,\n",
    "                 use_bias=True):\n",
    "  \"\"\"Returns simple Keras linear layer.\n",
    "\n",
    "  Args:\n",
    "    size: Output size\n",
    "    activation: Activation function to apply if required\n",
    "    use_time_distributed: Whether to apply layer across time\n",
    "    use_bias: Whether bias should be included in layer\n",
    "  \"\"\"\n",
    "  linear = tf.keras.layers.Dense(size, activation=activation, use_bias=use_bias)\n",
    "  if use_time_distributed:\n",
    "    linear = tf.keras.layers.TimeDistributed(linear)\n",
    "  return linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_and_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(x_list):\n",
    "  \"\"\"Applies skip connection followed by layer normalisation.\n",
    "\n",
    "  Args:\n",
    "    x_list: List of inputs to sum for skip connection\n",
    "\n",
    "  Returns:\n",
    "    Tensor output from layer.\n",
    "  \"\"\"\n",
    "  tmp = Add()(x_list)   #tmp = tf.keras.layers.Add(x1,x2)\n",
    "  tmp = LayerNorm()(tmp)\n",
    "  return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mlp(inputs,\n",
    "              hidden_size,\n",
    "              output_size,\n",
    "              output_activation=None,\n",
    "              hidden_activation='tanh',\n",
    "              use_time_distributed=False):\n",
    "  \"\"\"Applies simple feed-forward network to an input.\n",
    "\n",
    "  Args:\n",
    "    inputs: MLP inputs\n",
    "    hidden_size: Hidden state size\n",
    "    output_size: Output size of MLP\n",
    "    output_activation: Activation function to apply on output\n",
    "    hidden_activation: Activation function to apply on input\n",
    "    use_time_distributed: Whether to apply across time\n",
    "\n",
    "  Returns:\n",
    "    Tensor for MLP outputs.\n",
    "  \"\"\"\n",
    "  if use_time_distributed:\n",
    "    hidden = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_size, activation=hidden_activation))(\n",
    "            inputs)\n",
    "    return tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(output_size, activation=output_activation))(\n",
    "            hidden)\n",
    "  else:\n",
    "    hidden = tf.keras.layers.Dense(\n",
    "        hidden_size, activation=hidden_activation)(\n",
    "            inputs)\n",
    "    return tf.keras.layers.Dense(\n",
    "        output_size, activation=output_activation)(\n",
    "            hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gating_layer(x,\n",
    "                       hidden_layer_size,\n",
    "                       dropout_rate=None,\n",
    "                       use_time_distributed=True,\n",
    "                       activation=None):\n",
    "  \"\"\"Applies a Gated Linear Unit (GLU) to an input.\n",
    "\n",
    "  Args:\n",
    "    x: Input to gating layer\n",
    "    hidden_layer_size: Dimension of GLU\n",
    "    dropout_rate: Dropout rate to apply if any\n",
    "    use_time_distributed: Whether to apply across time\n",
    "    activation: Activation function to apply to the linear feature transform if\n",
    "      necessary\n",
    "\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GLU output, gate)\n",
    "  \"\"\"\n",
    "  # First, dropout\n",
    "  if dropout_rate is not None:\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "  if use_time_distributed:\n",
    "    activation_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation=activation))(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(hidden_layer_size, activation='sigmoid'))(\n",
    "            x)\n",
    "  else:\n",
    "    activation_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation=activation)(\n",
    "            x)\n",
    "    gated_layer = tf.keras.layers.Dense(\n",
    "        hidden_layer_size, activation='sigmoid')(\n",
    "            x)\n",
    "\n",
    "  #return tf.keras.layers.Multiply()([activation_layer, gated_layer]), gated_layer\n",
    "  return activation_layer, gated_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gated_residual_network(x,   # primary input\n",
    "                           hidden_layer_size,\n",
    "                           output_size=None,\n",
    "                           dropout_rate=None,\n",
    "                           use_time_distributed=True,\n",
    "                           additional_context=None,\n",
    "                           return_gate=False):\n",
    "  \"\"\"Applies the gated residual network (GRN) as defined in paper.\n",
    "\n",
    "  Args:\n",
    "    x: Network inputs\n",
    "    hidden_layer_size: Internal state size\n",
    "    output_size: Size of output layer\n",
    "    dropout_rate: Dropout rate if dropout is applied\n",
    "    use_time_distributed: Whether to apply network across time dimension\n",
    "    additional_context: Additional context vector to use if relevant\n",
    "    return_gate: Whether to return GLU gate for diagnostic purposes\n",
    "\n",
    "  Returns:\n",
    "    Tuple of tensors for: (GRN output, GLU gate)\n",
    "  \"\"\"\n",
    "\n",
    "  # Setup skip connection\n",
    "  if output_size is None:\n",
    "    output_size = hidden_layer_size\n",
    "    skip = x\n",
    "  else: # skip = TimeDistributed(Dense(output_size))(x)\n",
    "    skip = linear_layer(\n",
    "        output_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed\n",
    "    )(x)    \n",
    "    # original edition:\n",
    "    #linear = Dense(output_size)\n",
    "    #if use_time_distributed:\n",
    "      #linear = tf.keras.layers.TimeDistributed(linear)\n",
    "    #skip = linear(x)\n",
    "\n",
    "\n",
    "  # Apply feedforward network\n",
    "  #### (1) Dense\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          x)\n",
    "  if additional_context is not None:\n",
    "    hidden = hidden + linear_layer(\n",
    "        hidden_layer_size,\n",
    "        activation=None,\n",
    "        use_time_distributed=use_time_distributed,\n",
    "        use_bias=False)(\n",
    "            additional_context)\n",
    "  #### (2) ELU\n",
    "  hidden = Activation('elu')(hidden)\n",
    "\n",
    "  #### (3) Dense\n",
    "  hidden = linear_layer(\n",
    "      hidden_layer_size,\n",
    "      activation=None,\n",
    "      use_time_distributed=use_time_distributed)(\n",
    "          hidden)\n",
    "  #### (4) Gate: GLU\n",
    "  gating_layer, gate = apply_gating_layer(\n",
    "      hidden,\n",
    "      output_size,\n",
    "      dropout_rate=dropout_rate,\n",
    "      use_time_distributed=use_time_distributed,\n",
    "      activation=None)\n",
    "  #### (5) Add & Norm\n",
    "  if return_gate:\n",
    "    return add_and_norm([skip, gating_layer]), gate\n",
    "    #return add_and_norm(skip, hidden), gate\n",
    "  else:\n",
    "    return add_and_norm([skip, gating_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  gating_layer, gate = apply_gating_layer(\n",
    "      hidden,\n",
    "      output_size,\n",
    "      dropout_rate=dropout_rate,\n",
    "      use_time_distributed=use_time_distributed,\n",
    "      activation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "##### Variable selection for static inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_inputs: shape should be (None, 9, 5)\n",
    "num_static = static_inputs.shape[1] # num_static = 9\n",
    "\n",
    "# (1) sparse_weights: shape=(None, 9, 1)\n",
    "flatten = tf.keras.layers.Flatten()(static_inputs) # shape=(None, 45) 9*5=45\n",
    "mlp_outputs = gated_residual_network(    # shape=(None, 9)\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_static,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=False,\n",
    "          additional_context=None)\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.keras.backend.expand_dims(sparse_weights, axis=-1) # tf.keras.backend.expand_dims:在某个位置多加一个维度，数值为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_inputs: shape should be (None, 9, 5)\n",
    "num_static = static_inputs.shape[1] # num_static = 9\n",
    "\n",
    "# (1) sparse_weights: shape=(None, 9, 1)\n",
    "flatten = tf.keras.layers.Flatten()(static_inputs) # shape=(None, 45) 9*5=45\n",
    "mlp_outputs = gated_residual_network(    # shape=(None, 9)\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_static,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=False,\n",
    "          additional_context=None)\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.keras.backend.expand_dims(sparse_weights, axis=-1) # tf.keras.backend.expand_dims:在某个位置多加一个维度，数值为1\n",
    "\n",
    "# (2) transformed_embedding: shape=(None, 9, 5)\n",
    "trans_emb_list = []\n",
    "for i in range(num_static):\n",
    "        e = gated_residual_network(\n",
    "            static_inputs[:, i:i + 1, :], # static_inputs=(None, 9, 5)\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=False)\n",
    "        trans_emb_list.append(e) # e: shape=(None, 1, 5)\n",
    "transformed_embedding = concat(trans_emb_list, axis=1)\n",
    "\n",
    "# combined: shape=(None, 9, 5)\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding])\n",
    "static_vec = K.sum(combined, axis=1) # K = tf.keras.backend # shape=(None, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum:0\", shape=(None, 5), dtype=float32)\n",
      "Tensor(\"ExpandDims:0\", shape=(None, 9, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "static_encoder = static_vec\n",
    "static_weights = sparse_weights\n",
    "print(static_encoder)\n",
    "print(static_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static covariates Encoders(4 kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape=(None, 5), four identical static variables \n",
    "static_context_variable_selection = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_enrichment = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_state_h = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)\n",
    "static_context_state_c = gated_residual_network(\n",
    "        static_encoder,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_12/Identity:0' shape=(None, 5) dtype=float32>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "static_context_state_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variable selection for other inputs\n",
    "###### (1) Historical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(None, 90, 5, 11) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (1) Historical data\n",
    "historical_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, historical_time_steps, embedding_dim, num_inputs = historical_inputs.get_shape().as_list()\n",
    "#Wrong edition:\n",
    "# _, historical_time_steps, embedding_dim, num_inputs = tf.keras.backend.shape(historical_inputs)\n",
    "flatten = tf.keras.backend.reshape(historical_inputs,\n",
    "                          [-1, historical_time_steps, embedding_dim * num_inputs]) # shape=(None, 90, 55)\n",
    "expanded_static_context = tf.keras.backend.expand_dims(\n",
    "          static_context_variable_selection, axis=1) # shape=(None, 1, 5)\n",
    "\n",
    "# Variable selection weights\n",
    "mlp_outputs, static_gate = gated_residual_network(\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_inputs,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=True,\n",
    "          additional_context=expanded_static_context, # here\n",
    "          return_gate=True) # mlp_outputs: shape=(None, 90, 11); static_gate: shape=(None, 90, 11)\n",
    "\n",
    "\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.expand_dims(sparse_weights, axis=2) # shape=(None, 90, 1, 11)\n",
    "\n",
    "# Non-linear Processing & weight application\n",
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            historical_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 11个shape=(None, 90, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 90, 5, 11)\n",
    "\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 90, 5, 11)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1) # shape=(None, 90, 5)\n",
    "\n",
    "historical_features = temporal_ctx # shape=(None, 90, 5)\n",
    "historical_flags = sparse_weights # shape=(None, 90, 1, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (2) future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_45:0' shape=(None, 30, 5, 8) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (2) future data\n",
    "future_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            historical_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 11个shape=(None, 90, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 90, 5, 11)\n",
    "\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 90, 5, 11)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1) # shape=(None, 90, 5)\n",
    "\n",
    "historical_features = temporal_ctx # shape=(None, 90, 5)\n",
    "historical_flags = sparse_weights # shape=(None, 90, 1, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, future_time_steps, embedding_dim, num_inputs = future_inputs.get_shape().as_list()\n",
    "\n",
    "flatten = tf.keras.backend.reshape(future_inputs,\n",
    "                          [-1, future_time_steps, embedding_dim * num_inputs]) # shape=(None, 30, 40)\n",
    "expanded_static_context = tf.keras.backend.expand_dims(\n",
    "          static_context_variable_selection, axis=1) # shape=(None, 1, 5)\n",
    "\n",
    "# Variable selection weights\n",
    "mlp_outputs, static_gate = gated_residual_network(\n",
    "          flatten,\n",
    "          hidden_layer_size,\n",
    "          output_size=num_inputs,\n",
    "          dropout_rate=dropout_rate,\n",
    "          use_time_distributed=True,\n",
    "          additional_context=expanded_static_context, # here\n",
    "          return_gate=True) # mlp_outputs: shape=(None, 90, 11); static_gate: shape=(None, 30，8)\n",
    "sparse_weights = tf.keras.layers.Activation('softmax')(mlp_outputs)\n",
    "sparse_weights = tf.expand_dims(sparse_weights, axis=2) # hape=(None, 30, 1, 8)\n",
    "\n",
    "# Non-linear Processing & weight application\n",
    "trans_emb_list = []\n",
    "for i in range(num_inputs):\n",
    "        grn_output = gated_residual_network(\n",
    "            future_inputs[Ellipsis, i],\n",
    "            hidden_layer_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_time_distributed=True)\n",
    "        trans_emb_list.append(grn_output) # 8个shape=(None, 30, 5)\n",
    "transformed_embedding = tf.keras.backend.stack(trans_emb_list, axis=-1) # shape=(None, 30, 5, 8)\n",
    "combined = tf.keras.layers.Multiply()(\n",
    "          [sparse_weights, transformed_embedding]) # shape=(None, 30, 5, 8)\n",
    "temporal_ctx = tf.keras.backend.sum(combined, axis=-1)\n",
    "\n",
    "future_features = temporal_ctx # shape=(None, 30, 5)\n",
    "future_flags = sparse_weights # shape=(None, 30, 1, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM encoder & decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) historical data\n",
    "lstm = tf.keras.layers.LSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=True, # diff\n",
    "            stateful=False,\n",
    "            # Additional params to ensure LSTM matches CuDNN, See TF 2.0 :\n",
    "            # (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)\n",
    "\n",
    "history_lstm, state_h, state_c \\\n",
    "    = lstm(historical_features,initial_state=[static_context_state_h, # h_0\n",
    "                                               static_context_state_c]) # c_0\n",
    "                                               \n",
    "# history_lstm: shape=(None, 90, 5)\n",
    "# state_h: shape=(None, 5)\n",
    "# state_c: shape=(None, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_85:0' shape=(5,) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) future data\n",
    "lstm = tf.keras.layers.LSTM(\n",
    "            hidden_layer_size,\n",
    "            return_sequences=True,\n",
    "            return_state=False, # diff\n",
    "            stateful=False,\n",
    "            # Additional params to ensure LSTM matches CuDNN, See TF 2.0 :\n",
    "            # (https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "            activation='tanh',\n",
    "            recurrent_activation='sigmoid',\n",
    "            recurrent_dropout=0,\n",
    "            unroll=False,\n",
    "            use_bias=True)\n",
    "\n",
    "future_lstm = lstm(\n",
    "        future_features, \n",
    "        initial_state=[state_h, state_c]\n",
    "        )\n",
    "# future_lstm: shape=(None, 30, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) combine\n",
    "lstm_layer = tf.keras.backend.concatenate([history_lstm, future_lstm], axis=1) # shape=(None, 120, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Apply gated skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings = tf.keras.backend.concatenate([historical_features, future_features], axis=1) # shape=(None, 120, 5)\n",
    "\n",
    "lstm_layer, _ = apply_gating_layer(\n",
    "        lstm_layer, hidden_layer_size, dropout_rate, activation=None) # shape=(None, 120, 5)\n",
    "temporal_feature_layer = add_and_norm([lstm_layer, input_embeddings]) # shape=(None, 120, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Static enrichment layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add static_context_enrichment\n",
    "expanded_static_context = tf.expand_dims(static_context_enrichment, axis=1) # (None, 1, 5) \n",
    "enriched, _ = gated_residual_network(\n",
    "        temporal_feature_layer,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=True,\n",
    "        additional_context=expanded_static_context,\n",
    "        return_gate=True) # (None, 120, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_62/Identity:0' shape=(None, 120, 5) dtype=float32>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Temporal Self-Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_head, d_model, dropout\n",
    "(num_heads, hidden_layer_size, dropout=dropout_rate)\n",
    "q, k, v, mask=None\n",
    "(enriched, enriched, enriched,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# causal mask to apply for self-attention layer\n",
    "len_s = tf.shape(enriched)[1]  # 120\n",
    "bs = tf.shape(enriched)[:1] # (None,)\n",
    "mask = K.cumsum(tf.eye(len_s, batch_shape=bs), 1) # shape=(120, 120)\n",
    "\n",
    "d_k = d_v = hidden_layer_size // num_heads  # 5//4=1\n",
    "qs_layers = []\n",
    "ks_layers = []\n",
    "vs_layers = []\n",
    "\n",
    "# Use same value layer to facilitate interp\n",
    "vs_layer = Dense(d_v, use_bias=False)\n",
    "\n",
    "for _ in range(num_heads):\n",
    "  qs_layers.append(Dense(d_k, use_bias=False)) # since # of queries = # of keys\n",
    "  ks_layers.append(Dense(d_k, use_bias=False)) # output_dim = d_k\n",
    "  vs_layers.append(Dense(d_v, use_bias=False))  # output_dim = d_v\n",
    "\n",
    "#attention = ScaledDotProductAttention()\n",
    "heads = []\n",
    "attns = []\n",
    "for i in range(num_heads):\n",
    "  qs = qs_layers[i](enriched) # Q, (None, 120, 1)\n",
    "  ks = ks_layers[i](enriched)\n",
    "  vs = vs_layers[i](enriched)\n",
    "\n",
    "  #head, attn = attention(qs, ks, vs, mask)\n",
    "  temper = tf.sqrt(tf.cast(tf.shape(ks)[-1], dtype='float32')) # cast: type conversion # shape=()\n",
    "  # tf.keras.ops.shape(ks)[-1] refers to d_attn(in the paper) = 1\n",
    "  \n",
    "  ###################### (QK^T)/sqrt(d_attn) #########################\n",
    "  attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [qs, ks])\n",
    "  # equals to:\n",
    "  # K.batch_dot(qs, ks, axes=[2, 2]) / temper\n",
    "  # output_shape=(None, 120, 120)\n",
    "  # # shape=(batch, q, k)\n",
    "\n",
    "\n",
    "  if mask is not None:\n",
    "      mmask = Lambda(lambda x: (-1e+9) * (1. - tf.cast(x, 'float32')))(\n",
    "          mask)  # setting to infinity, # 将 mask=1 的位置设为一个很大的负数，softmax 后趋近于0\n",
    "      attn = Add()([attn, mmask])\n",
    "\n",
    "  attn = Dropout(0.0)(Activation('softmax')(attn)) # shape=(None, 120, 120)\n",
    "  # Dropout(0.0): 不丢弃任何神经元，相当于没有起作用，网络将正常传递所有的神经元输出。\n",
    "\n",
    "  ###################### Attention(Q, K, V) = A(Q, K)V #########################\n",
    "  head_temp = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, vs]) # shape=(None, 120, 1)\n",
    "  # Wrong edition:\n",
    "  # head_temp = tf.expand_dims(K.batch_dot(attn, v[0,:]),axis=0) # shape=(1, 120, 5)\n",
    "  # equals to:\n",
    "  # output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, enriched])\n",
    "\n",
    "  head_dropout = Dropout(dropout_rate)(head_temp) # shape=(None, 120, 1)\n",
    "  \n",
    "  \n",
    "  heads.append(head_dropout) # 4个shape=(1, 120, 5)\n",
    "  attns.append(attn) # 4个shape=(1, 120, 120)\n",
    "  \n",
    "###################### Multi-Head #########################\n",
    "head = tf.keras.backend.stack(heads) if num_heads > 1 else heads[0] # (4, None, 120, 1)\n",
    "\n",
    "# Result 2: self_att      shape=(4, None, 120, 120)\n",
    "self_att = tf.keras.backend.stack(attns)\n",
    "\n",
    "# Result 1: x   shape=(None, 120, 5)\n",
    "outputs = K.mean(head, axis=0) if num_heads > 1 else head # (None, 120, 1)\n",
    "w_o = Dense(hidden_layer_size, use_bias=False)\n",
    "outputs = w_o(outputs) # input_dim=d_model; output_dim=K.mean(head, axis=0)\n",
    "outputs = Dropout(dropout_rate)(outputs)  # output dropout\n",
    "x = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, _ = apply_gating_layer(\n",
    "        x,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"layer_normalization_48/Identity:0\", shape=(None, 120, 5), dtype=float32)\n",
      "Tensor(\"layer_normalization_47/Identity:0\", shape=(None, 120, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = add_and_norm([x, enriched])\n",
    "print(x)\n",
    "print(enriched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder self attention\n",
    "x, self_att \\\n",
    "        = InterpretableMultiHeadAttention(\n",
    "        num_heads, hidden_layer_size, dropout=dropout_rate)(enriched, enriched, enriched,\n",
    "                          mask=mask)\n",
    "class InterpretableMultiHeadAttention():\n",
    "\n",
    "    self.d_k = self.d_v = d_k = d_v = hidden_layer_size // num_heads\n",
    "    self.qs_layers = []\n",
    "    self.ks_layers = []\n",
    "    self.vs_layers = []\n",
    "\n",
    "    # Use same value layer to facilitate interp\n",
    "    vs_layer = Dense(d_v, use_bias=False)\n",
    "\n",
    "    for _ in range(n_head):\n",
    "      self.qs_layers.append(Dense(d_k, use_bias=False)) # since # of queries = # of keys\n",
    "      self.ks_layers.append(Dense(d_k, use_bias=False)) # output_dim = d_k\n",
    "      self.vs_layers.append(Dense(d_v, use_bias=False))  # output_dim = d_v\n",
    "\n",
    "\n",
    "    heads = []\n",
    "    attns = []\n",
    "    for i in range(n_head):\n",
    "      qs = self.qs_layers[i](enriched) # input_dim=q; output_dim = d_k\n",
    "      ks = self.ks_layers[i](enriched)\n",
    "      vs = self.vs_layers[i](enriched)\n",
    "      head, attn = ScaledDotProductAttention()(qs, ks, vs, mask)\n",
    "      head_dropout = Dropout(self.dropout)(head)\n",
    "      \n",
    "      heads.append(head_dropout)\n",
    "      attns.append(attn)\n",
    "\n",
    "\n",
    "    temper = tf.sqrt(tf.cast(tf.shape(ks)[-1], dtype='float32'))\n",
    "    attn = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]) / temper)(\n",
    "        [qs, ks])  # shape=(batch, q, k)\n",
    "    if mask is not None:\n",
    "      mmask = Lambda(lambda x: (-1e+9) * (1. - K.cast(x, 'float32')))(\n",
    "          mask)  # setting to infinity\n",
    "      attn = Add()([attn, mmask])\n",
    "    attn = Activation('softmax')(attn)\n",
    "    attn = Dropout(attn_dropout=0.0)(attn)\n",
    "    output = Lambda(lambda x: K.batch_dot(x[0], x[1]))([attn, vs])\n",
    "\n",
    "head = K.stack(heads) if n_head > 1 else heads[0]\n",
    "    attn = K.stack(attns)\n",
    "\n",
    "    outputs = K.mean(head, axis=0) if n_head > 1 else head\n",
    "    outputs = Dense(d_model, use_bias=False)(outputs) # input_dim=d_model; output_dim=K.mean(head, axis=0)\n",
    "    outputs = Dropout(self.dropout)(outputs)  # output dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Position-wise Feed-forward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_49/Identity:0' shape=(None, 120, 5) dtype=float32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = gated_residual_network(\n",
    "        x,\n",
    "        hidden_layer_size,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_time_distributed=True)\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final skip connection\n",
    "decoder, _ = apply_gating_layer(\n",
    "        decoder, hidden_layer_size, activation=None)\n",
    "transformer_layer = add_and_norm([decoder, temporal_feature_layer])\n",
    "\n",
    "# Attention components for explainability\n",
    "attention_components = {\n",
    "        # Temporal attention weights\n",
    "        'decoder_self_attn': self_att,\n",
    "        # Static variable selection weights\n",
    "        'static_flags': static_weights[Ellipsis, 0],\n",
    "        # Variable selection weights of past inputs\n",
    "        'historical_flags': historical_flags[Ellipsis, 0, :],\n",
    "        # Variable selection weights of future inputs\n",
    "        'future_flags': future_flags[Ellipsis, 0, :]\n",
    "    }\n",
    "\n",
    "# what we want:\n",
    "# transformer_layer: shape=(None, 120, 5)\n",
    "# all_inputs\n",
    "# attention_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'layer_normalization_50/Identity:0' shape=(None, 120, 5) dtype=float32>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_self_attn': <tf.Tensor 'stack_8:0' shape=(4, None, 120, 120) dtype=float32>,\n",
       " 'static_flags': <tf.Tensor 'strided_slice_92:0' shape=(None, 9) dtype=float32>,\n",
       " 'historical_flags': <tf.Tensor 'strided_slice_93:0' shape=(None, 90, 11) dtype=float32>,\n",
       " 'future_flags': <tf.Tensor 'strided_slice_94:0' shape=(None, 30, 8) dtype=float32>}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attention_components =  attention_components\n",
    "attention_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense\n",
    "outputs = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(output_size * len(quantiles))) \\\n",
    "          (transformer_layer[Ellipsis, num_encoder_steps:, :])\n",
    "# shape=(None, 30, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'time_distributed_164/Identity:0' shape=(None, 30, 3) dtype=float32>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(output_size * len(quantiles))) \\\n",
    "          (transformer_layer[Ellipsis, num_encoder_steps:, :])\n",
    "# shape=(None, 30, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 120, 20)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_1 (Te [(None, 120, 14)]    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_2 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_3 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_4 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_5 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_8 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_9 (Te [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_10 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 120, 5)       15          tf_op_layer_strided_slice_2[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_3[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_4[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_5[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 120, 5)       5           tf_op_layer_strided_slice_6[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_8[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 120, 5)       15          tf_op_layer_strided_slice_9[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 120, 5)       10          tf_op_layer_strided_slice_10[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_16 (T [(None, 5)]          0           sequential[1][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_17 (T [(None, 5)]          0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_18 (T [(None, 5)]          0           sequential_2[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_19 (T [(None, 5)]          0           sequential_3[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_20 (T [(None, 5)]          0           sequential_4[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_21 (T [(None, 5)]          0           sequential_5[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_22 (T [(None, 5)]          0           sequential_6[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_23 (T [(None, 5)]          0           sequential_7[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_24 (T [(None, 5)]          0           sequential_8[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack (TensorFlowOp [(None, 9, 5)]       0           tf_op_layer_strided_slice_16[0][0\n",
      "                                                                 tf_op_layer_strided_slice_17[0][0\n",
      "                                                                 tf_op_layer_strided_slice_18[0][0\n",
      "                                                                 tf_op_layer_strided_slice_19[0][0\n",
      "                                                                 tf_op_layer_strided_slice_20[0][0\n",
      "                                                                 tf_op_layer_strided_slice_21[0][0\n",
      "                                                                 tf_op_layer_strided_slice_22[0][0\n",
      "                                                                 tf_op_layer_strided_slice_23[0][0\n",
      "                                                                 tf_op_layer_strided_slice_24[0][0\n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 45)           0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 5)            230         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_46 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_47 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_48 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_49 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_50 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_51 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_52 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_53 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_54 (T [(None, 1, 5)]       0           tf_op_layer_stack[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 5)            0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_46[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_47[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_48[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_49[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_50[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_51[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_52[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_53[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 1, 5)         30          tf_op_layer_strided_slice_54[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 5)            30          activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 1, 5)         0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 1, 5)         0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 1, 5)         0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 1, 5)         0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 1, 5)         0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1, 5)         0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1, 5)         0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1, 5)         0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 1, 5)         0           dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 5)            0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1, 5)         30          activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 1, 5)         30          activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 1, 5)         30          activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 1, 5)         30          activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 1, 5)         30          activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 1, 5)         30          activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1, 5)         30          activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 1, 5)         30          activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 1, 5)         30          activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 9)            414         flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 9)            54          dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 5)         0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1, 5)         0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1, 5)         0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1, 5)         0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 1, 5)         0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 1, 5)         0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 1, 5)         0           dense_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 1, 5)         0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 1, 5)         0           dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 9)            0           dense_7[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1, 5)         30          dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1, 5)         30          dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 1, 5)         30          dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 1, 5)         30          dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 1, 5)         30          dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 1, 5)         30          dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_38 (Dense)                (None, 1, 5)         30          dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 1, 5)         30          dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 1, 5)         30          dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, 9)            18          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_46[0][0\n",
      "                                                                 dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_47[0][0\n",
      "                                                                 dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_48[0][0\n",
      "                                                                 dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_49[0][0\n",
      "                                                                 dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_50[0][0\n",
      "                                                                 dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_51[0][0\n",
      "                                                                 dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_52[0][0\n",
      "                                                                 dense_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_53[0][0\n",
      "                                                                 dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 1, 5)         0           tf_op_layer_strided_slice_54[0][0\n",
      "                                                                 dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 9)            0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 1, 5)         10          add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 1, 5)         10          add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 1, 5)         10          add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 1, 5)         10          add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 1, 5)         10          add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 1, 5)         10          add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 1, 5)         10          add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 1, 5)         10          add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 1, 5)         10          add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 9, 1)]       0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 9, 5)]       0           layer_normalization_1[0][0]      \n",
      "                                                                 layer_normalization_2[0][0]      \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 9, 5)         0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 5)]          0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 5)            30          tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 120, 6)]     0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 5)            0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_36 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_37 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_38 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_39 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_40 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_11 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_12 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_13 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_14 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_15 (T [(None, 120)]        0           tf_op_layer_strided_slice_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_34 (T [(None, 120, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 5)            30          activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 120, 5)       10          tf_op_layer_strided_slice_36[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 120, 5)       10          tf_op_layer_strided_slice_37[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 120, 5)       10          tf_op_layer_strided_slice_38[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 120, 5)       10          tf_op_layer_strided_slice_39[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 120, 5)       10          tf_op_layer_strided_slice_40[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 120, 5)       15          tf_op_layer_strided_slice_11[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 120, 5)       35          tf_op_layer_strided_slice_12[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sequential_11 (Sequential)      (None, 120, 5)       175         tf_op_layer_strided_slice_13[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sequential_12 (Sequential)      (None, 120, 5)       10          tf_op_layer_strided_slice_14[0][0\n",
      "__________________________________________________________________________________________________\n",
      "sequential_13 (Sequential)      (None, 120, 5)       20          tf_op_layer_strided_slice_15[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 120, 5)       10          tf_op_layer_strided_slice_34[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 5)            0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_2 (TensorFlow [(None, 120, 5, 2)]  0           time_distributed_2[0][0]         \n",
      "                                                                 time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_3 (TensorFlow [(None, 120, 5, 8)]  0           time_distributed_4[0][0]         \n",
      "                                                                 time_distributed_5[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "                                                                 sequential_9[1][0]               \n",
      "                                                                 sequential_10[1][0]              \n",
      "                                                                 sequential_11[1][0]              \n",
      "                                                                 sequential_12[1][0]              \n",
      "                                                                 sequential_13[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_1 (TensorFlow [(None, 120, 5, 1)]  0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 5)            30          dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_42 (T [(None, 90, 5, 2)]   0           tf_op_layer_stack_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_43 (T [(None, 90, 5, 8)]   0           tf_op_layer_stack_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_44 (T [(None, 90, 5, 1)]   0           tf_op_layer_stack_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 5)            0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 dense_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 90, 5, 11)]  0           tf_op_layer_strided_slice_42[0][0\n",
      "                                                                 tf_op_layer_strided_slice_43[0][0\n",
      "                                                                 tf_op_layer_strided_slice_44[0][0\n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 5)            10          add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape (TensorFlow [(None, 90, 55)]     0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 5)]       0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_45 (T [(None, 30, 5, 8)]   0           tf_op_layer_stack_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 90, 5)        280         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, 1, 5)         25          tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_1 (TensorFl [(None, 30, 40)]     0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 5)]       0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_14 (TensorFlowO [(None, 90, 5)]      0           time_distributed_8[0][0]         \n",
      "                                                                 time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_66 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_67 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_68 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_69 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_70 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_71 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_72 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_73 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_74 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_75 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_76 (T [(None, 90, 5)]      0           tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_102 (TimeDistr (None, 30, 5)        205         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_103 (TimeDistr (None, 1, 5)         25          tf_op_layer_ExpandDims_3[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 90, 5)        0           tf_op_layer_add_14[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_57 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_66[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_61 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_67[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_65 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_68[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_69 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_69[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_73 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_70[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_77 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_71[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_81 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_72[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_85 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_73[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_89 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_74[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_93 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_75[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_97 (TimeDistri (None, 90, 5)        30          tf_op_layer_strided_slice_76[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_37 (TensorFlowO [(None, 30, 5)]      0           time_distributed_102[0][0]       \n",
      "                                                                 time_distributed_103[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_77 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_78 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_79 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_80 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_81 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_82 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_83 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_84 (T [(None, 30, 5)]      0           tf_op_layer_strided_slice_45[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 90, 5)        30          activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 90, 5)        0           time_distributed_57[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 90, 5)        0           time_distributed_61[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 90, 5)        0           time_distributed_65[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 90, 5)        0           time_distributed_69[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 90, 5)        0           time_distributed_73[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 90, 5)        0           time_distributed_77[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 90, 5)        0           time_distributed_81[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 90, 5)        0           time_distributed_85[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 90, 5)        0           time_distributed_89[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 90, 5)        0           time_distributed_93[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 90, 5)        0           time_distributed_97[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 30, 5)        0           tf_op_layer_add_37[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_107 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_77[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_111 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_78[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_115 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_79[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_119 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_80[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_123 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_81[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_127 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_82[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_131 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_83[0][0\n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_135 (TimeDistr (None, 30, 5)        30          tf_op_layer_strided_slice_84[0][0\n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 90, 5)        0           time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_58 (TimeDistri (None, 90, 5)        30          activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_62 (TimeDistri (None, 90, 5)        30          activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_66 (TimeDistri (None, 90, 5)        30          activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_70 (TimeDistri (None, 90, 5)        30          activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_74 (TimeDistri (None, 90, 5)        30          activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_78 (TimeDistri (None, 90, 5)        30          activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_82 (TimeDistri (None, 90, 5)        30          activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_86 (TimeDistri (None, 90, 5)        30          activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_90 (TimeDistri (None, 90, 5)        30          activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_94 (TimeDistri (None, 90, 5)        30          activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_98 (TimeDistri (None, 90, 5)        30          activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_104 (TimeDistr (None, 30, 5)        30          activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 30, 5)        0           time_distributed_107[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 30, 5)        0           time_distributed_111[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 30, 5)        0           time_distributed_115[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 30, 5)        0           time_distributed_119[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 30, 5)        0           time_distributed_123[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 30, 5)        0           time_distributed_127[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 30, 5)        0           time_distributed_131[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 30, 5)        0           time_distributed_135[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 90, 11)       616         tf_op_layer_Reshape[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 90, 11)       66          dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 90, 5)        0           time_distributed_58[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 90, 5)        0           time_distributed_62[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 90, 5)        0           time_distributed_66[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_29 (Dropout)            (None, 90, 5)        0           time_distributed_70[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_30 (Dropout)            (None, 90, 5)        0           time_distributed_74[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_31 (Dropout)            (None, 90, 5)        0           time_distributed_78[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_32 (Dropout)            (None, 90, 5)        0           time_distributed_82[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 90, 5)        0           time_distributed_86[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 90, 5)        0           time_distributed_90[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 90, 5)        0           time_distributed_94[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 90, 5)        0           time_distributed_98[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_56 (Dense)                (None, 5)            30          tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_60 (Dense)                (None, 5)            30          tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 30, 5)        0           time_distributed_104[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_108 (TimeDistr (None, 30, 5)        30          activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_112 (TimeDistr (None, 30, 5)        30          activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_116 (TimeDistr (None, 30, 5)        30          activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_120 (TimeDistr (None, 30, 5)        30          activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_124 (TimeDistr (None, 30, 5)        30          activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_128 (TimeDistr (None, 30, 5)        30          activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_132 (TimeDistr (None, 30, 5)        30          activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_136 (TimeDistr (None, 30, 5)        30          activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 90, 11)       0           time_distributed_7[0][0]         \n",
      "                                                                 time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_59 (TimeDistri (None, 90, 5)        30          dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_63 (TimeDistri (None, 90, 5)        30          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_67 (TimeDistri (None, 90, 5)        30          dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_71 (TimeDistri (None, 90, 5)        30          dropout_29[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_75 (TimeDistri (None, 90, 5)        30          dropout_30[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_79 (TimeDistri (None, 90, 5)        30          dropout_31[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_83 (TimeDistri (None, 90, 5)        30          dropout_32[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_87 (TimeDistri (None, 90, 5)        30          dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_91 (TimeDistri (None, 90, 5)        30          dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_95 (TimeDistri (None, 90, 5)        30          dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_99 (TimeDistri (None, 90, 5)        30          dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 5)            0           dense_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 5)            0           dense_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_101 (TimeDistr (None, 30, 8)        328         tf_op_layer_Reshape_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_105 (TimeDistr (None, 30, 8)        48          dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 30, 5)        0           time_distributed_108[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 30, 5)        0           time_distributed_112[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 30, 5)        0           time_distributed_116[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 30, 5)        0           time_distributed_120[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 30, 5)        0           time_distributed_124[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 30, 5)        0           time_distributed_128[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 30, 5)        0           time_distributed_132[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 30, 5)        0           time_distributed_136[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, 90, 11)       22          add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_66[0][0\n",
      "                                                                 time_distributed_59[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_67[0][0\n",
      "                                                                 time_distributed_63[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_68[0][0\n",
      "                                                                 time_distributed_67[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_69[0][0\n",
      "                                                                 time_distributed_71[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_70[0][0\n",
      "                                                                 time_distributed_75[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_71[0][0\n",
      "                                                                 time_distributed_79[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_72[0][0\n",
      "                                                                 time_distributed_83[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_73[0][0\n",
      "                                                                 time_distributed_87[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_74[0][0\n",
      "                                                                 time_distributed_91[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_75[0][0\n",
      "                                                                 time_distributed_95[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, 90, 5)        0           tf_op_layer_strided_slice_76[0][0\n",
      "                                                                 time_distributed_99[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_57 (Dense)                (None, 5)            30          activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_61 (Dense)                (None, 5)            30          activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, 30, 8)        0           time_distributed_101[0][0]       \n",
      "                                                                 time_distributed_105[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_109 (TimeDistr (None, 30, 5)        30          dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_113 (TimeDistr (None, 30, 5)        30          dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_117 (TimeDistr (None, 30, 5)        30          dropout_40[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_121 (TimeDistr (None, 30, 5)        30          dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_125 (TimeDistr (None, 30, 5)        30          dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_129 (TimeDistr (None, 30, 5)        30          dropout_43[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_133 (TimeDistr (None, 30, 5)        30          dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_137 (TimeDistr (None, 30, 5)        30          dropout_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 90, 11)       0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, 90, 5)        10          add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, 90, 5)        10          add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, 90, 5)        10          add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, 90, 5)        10          add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, 90, 5)        10          add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, 90, 5)        10          add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, 90, 5)        10          add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, 90, 5)        10          add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, 90, 5)        10          add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, 90, 5)        10          add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, 90, 5)        10          add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 5)            0           dense_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 5)            0           dense_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, 30, 8)        16          add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_77[0][0\n",
      "                                                                 time_distributed_109[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_78[0][0\n",
      "                                                                 time_distributed_113[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_40 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_79[0][0\n",
      "                                                                 time_distributed_117[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_41 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_80[0][0\n",
      "                                                                 time_distributed_121[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_42 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_81[0][0\n",
      "                                                                 time_distributed_125[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_43 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_82[0][0\n",
      "                                                                 time_distributed_129[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_44 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_83[0][0\n",
      "                                                                 time_distributed_133[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_45 (Add)                    (None, 30, 5)        0           tf_op_layer_strided_slice_84[0][0\n",
      "                                                                 time_distributed_137[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(None, 90, 1, 11)]  0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_5 (TensorFlow [(None, 90, 5, 11)]  0           layer_normalization_26[0][0]     \n",
      "                                                                 layer_normalization_27[0][0]     \n",
      "                                                                 layer_normalization_28[0][0]     \n",
      "                                                                 layer_normalization_29[0][0]     \n",
      "                                                                 layer_normalization_30[0][0]     \n",
      "                                                                 layer_normalization_31[0][0]     \n",
      "                                                                 layer_normalization_32[0][0]     \n",
      "                                                                 layer_normalization_33[0][0]     \n",
      "                                                                 layer_normalization_34[0][0]     \n",
      "                                                                 layer_normalization_35[0][0]     \n",
      "                                                                 layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_58 (Dense)                (None, 5)            30          dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_62 (Dense)                (None, 5)            30          dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 30, 8)        0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, 30, 5)        10          add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, 30, 5)        10          add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, 30, 5)        10          add_40[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, 30, 5)        10          add_41[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, 30, 5)        10          add_42[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, 30, 5)        10          add_43[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, 30, 5)        10          add_44[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, 30, 5)        10          add_45[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 5)            30          tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 90, 5, 11)    0           tf_op_layer_ExpandDims_2[0][0]   \n",
      "                                                                 tf_op_layer_stack_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 5)            0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 dense_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 5)            0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 dense_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_4 (Tenso [(None, 30, 1, 8)]   0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_6 (TensorFlow [(None, 30, 5, 8)]   0           layer_normalization_38[0][0]     \n",
      "                                                                 layer_normalization_39[0][0]     \n",
      "                                                                 layer_normalization_40[0][0]     \n",
      "                                                                 layer_normalization_41[0][0]     \n",
      "                                                                 layer_normalization_42[0][0]     \n",
      "                                                                 layer_normalization_43[0][0]     \n",
      "                                                                 layer_normalization_44[0][0]     \n",
      "                                                                 layer_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 5)            0           dense_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 90, 5)]      0           multiply_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 5)            10          add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 5)            10          add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 30, 5, 8)     0           tf_op_layer_ExpandDims_4[0][0]   \n",
      "                                                                 tf_op_layer_stack_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 5)            30          activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 90, 5), (Non 220         tf_op_layer_Sum_2[0][0]          \n",
      "                                                                 layer_normalization_12[0][0]     \n",
      "                                                                 layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 30, 5)]      0           multiply_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 5)            0           dense_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 30, 5)        220         tf_op_layer_Sum_3[0][0]          \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_54 (Dense)                (None, 5)            30          dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 120, 5)]     0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 120, 5)]     0           tf_op_layer_Sum_2[0][0]          \n",
      "                                                                 tf_op_layer_Sum_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 5)            0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 dense_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_46 (Add)                    (None, 120, 5)       0           tf_op_layer_concat_2[0][0]       \n",
      "                                                                 tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 5)            10          add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, 120, 5)       10          add_46[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 5)]       0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_139 (TimeDistr (None, 120, 5)       30          layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_140 (TimeDistr (None, 1, 5)         25          tf_op_layer_ExpandDims_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_47 (TensorFlowO [(None, 120, 5)]     0           time_distributed_139[0][0]       \n",
      "                                                                 time_distributed_140[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 120, 5)       0           tf_op_layer_add_47[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_141 (TimeDistr (None, 120, 5)       30          activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_46 (Dropout)            (None, 120, 5)       0           time_distributed_141[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_142 (TimeDistr (None, 120, 5)       30          dropout_46[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 120, 5)       0           layer_normalization_46[0][0]     \n",
      "                                                                 time_distributed_142[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, 120, 5)       10          add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape (TensorFlowOp [(3,)]               0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_86 (T [()]                 0           tf_op_layer_Shape[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Shape_1 (TensorFlow [(3,)]               0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_eye/Minimum (Tensor [()]                 0           tf_op_layer_strided_slice_86[0][0\n",
      "                                                                 tf_op_layer_strided_slice_86[0][0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_87 (T [(1,)]               0           tf_op_layer_Shape_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_eye/concat/values_1 [(1,)]               0           tf_op_layer_eye/Minimum[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_eye/concat (TensorF [(2,)]               0           tf_op_layer_strided_slice_87[0][0\n",
      "                                                                 tf_op_layer_eye/concat/values_1[0\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_eye/ones (TensorFlo [(None, None)]       0           tf_op_layer_eye/concat[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_eye/diag (TensorFlo [(None, None, None)] 0           tf_op_layer_eye/ones[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_202 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_203 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Cumsum (TensorFlowO [(None, None, None)] 0           tf_op_layer_eye/diag[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_205 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_206 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_208 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 120, 120)     0           dense_202[0][0]                  \n",
      "                                                                 dense_203[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, None, None)   0           tf_op_layer_Cumsum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 120, 120)     0           dense_205[0][0]                  \n",
      "                                                                 dense_206[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, None, None)   0           tf_op_layer_Cumsum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 120, 120)     0           dense_208[0][0]                  \n",
      "                                                                 dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, None, None)   0           tf_op_layer_Cumsum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 120, 120)     0           dense_211[0][0]                  \n",
      "                                                                 dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, None, None)   0           tf_op_layer_Cumsum[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 120, 120)     0           lambda[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 120, 120)     0           lambda_3[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 120, 120)     0           lambda_6[0][0]                   \n",
      "                                                                 lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 120, 120)     0           lambda_9[0][0]                   \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 120, 120)     0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 120, 120)     0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 120, 120)     0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 120, 120)     0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_47 (Dropout)            (None, 120, 120)     0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_204 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 120, 120)     0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_207 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_51 (Dropout)            (None, 120, 120)     0           activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 120, 120)     0           activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 120, 1)       5           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 120, 1)       0           dropout_47[0][0]                 \n",
      "                                                                 dense_204[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 120, 1)       0           dropout_49[0][0]                 \n",
      "                                                                 dense_207[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 120, 1)       0           dropout_51[0][0]                 \n",
      "                                                                 dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 120, 1)       0           dropout_53[0][0]                 \n",
      "                                                                 dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_48 (Dropout)            (None, 120, 1)       0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 120, 1)       0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 120, 1)       0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 120, 1)       0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_stack_7 (TensorFlow [(4, None, 120, 1)]  0           dropout_48[0][0]                 \n",
      "                                                                 dropout_50[0][0]                 \n",
      "                                                                 dropout_52[0][0]                 \n",
      "                                                                 dropout_54[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(None, 120, 1)]     0           tf_op_layer_stack_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_214 (Dense)               (None, 120, 5)       5           tf_op_layer_Mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_55 (Dropout)            (None, 120, 5)       0           dense_214[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 120, 5)       0           dropout_55[0][0]                 \n",
      "                                                                 layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, 120, 5)       10          add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_144 (TimeDistr (None, 120, 5)       30          layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 120, 5)       0           time_distributed_144[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_145 (TimeDistr (None, 120, 5)       30          activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_56 (Dropout)            (None, 120, 5)       0           time_distributed_145[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_146 (TimeDistr (None, 120, 5)       30          dropout_56[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 120, 5)       0           layer_normalization_48[0][0]     \n",
      "                                                                 time_distributed_146[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, 120, 5)       10          add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_148 (TimeDistr (None, 120, 5)       30          layer_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 120, 5)       0           time_distributed_148[0][0]       \n",
      "                                                                 layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, 120, 5)       10          add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_95 (T [(None, 30, 5)]      0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_150 (TimeDistr (None, 30, 3)        18          tf_op_layer_strided_slice_95[0][0\n",
      "==================================================================================================\n",
      "Total params: 6,855\n",
      "Trainable params: 6,855\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#_attention_components = attention_components\n",
    "model = tf.keras.Model(inputs=all_inputs, outputs=outputs) # all_inputs: ([None, 120, 20])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_quantiles = quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileLossCalculator(object):\n",
    "        \"\"\"Computes the combined quantile loss for prespecified quantiles.\n",
    "\n",
    "        Attributes:\n",
    "          quantiles: Quantiles to compute losses\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, quantiles):\n",
    "          \"\"\"Initializes computer with quantiles for loss calculations.\n",
    "\n",
    "          Args:\n",
    "            quantiles: Quantiles to use for computations.\n",
    "          \"\"\"\n",
    "          \n",
    "\n",
    "        def quantile_loss(self, a, b):\n",
    "          \"\"\"Returns quantile loss for specified quantiles.\n",
    "\n",
    "          Args:\n",
    "            a: Targets\n",
    "            b: Predictions\n",
    "          \"\"\"\n",
    "          quantiles_used = set(quantiles)\n",
    "\n",
    "          loss = 0.\n",
    "          for i, quantile in enumerate(valid_quantiles):\n",
    "            if quantile in quantiles_used:\n",
    "              loss += utils.tensorflow_quantile_loss(\n",
    "                  a[Ellipsis, output_size * i:output_size * (i + 1)],\n",
    "                  b[Ellipsis, output_size * i:output_size * (i + 1)], quantile)\n",
    "          return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_loss = QuantileLossCalculator(valid_quantiles).quantile_loss\n",
    "adam = tf.keras.optimizers.Adam(\n",
    "          learning_rate=learning_rate, clipnorm=max_gradient_norm)\n",
    "model.compile(\n",
    "          loss=quantile_loss, optimizer=adam, sample_weight_mode='temporal')\n",
    "# model.compile()方法用于在配置训练方法时，告知训练时用的优化器、损失函数和准确率评测标准\n",
    "# sample_weight_mode='temporal' is very important, we need to assign different weight for each timestamp\n",
    "\n",
    "_input_placeholder = all_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def build_model(self):\n",
    "    \"\"\"Build model and defines training losses.\n",
    "\n",
    "    Returns:\n",
    "      Fully defined Keras model.\n",
    "    \"\"\"\n",
    "\n",
    "    #with tf.variable_scope(name):\n",
    "    with tf.compat.v1.variable_scope(name):\n",
    "\n",
    "      transformer_layer, all_inputs, attention_components \\\n",
    "          = _build_base_graph()\n",
    "      \n",
    "      # Dense\n",
    "      outputs = tf.keras.layers.TimeDistributed(\n",
    "          tf.keras.layers.Dense(output_size * len(quantiles))) \\\n",
    "          (transformer_layer[Ellipsis, num_encoder_steps:, :])\n",
    "\n",
    "      _attention_components = attention_components\n",
    "\n",
    "      adam = tf.keras.optimizers.Adam(\n",
    "          learning_rate=learning_rate, clipnorm=max_gradient_norm)\n",
    "\n",
    "      model = tf.keras.Model(inputs=all_inputs, outputs=outputs)\n",
    "\n",
    "      print(model.summary())\n",
    "\n",
    "      valid_quantiles = quantiles\n",
    "      output_size = output_size\n",
    "      \n",
    "      quantile_loss = QuantileLossCalculator(valid_quantiles).quantile_loss\n",
    "\n",
    "      model.compile(\n",
    "          loss=quantile_loss, optimizer=adam, sample_weight_mode='temporal')\n",
    "\n",
    "      _input_placeholder = all_inputs\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample training & validating data\n",
    "##### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_tf.csv')\n",
    "valid = pd.read_csv('valid_tf.csv')\n",
    "test = pd.read_csv('test_tf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>open</th>\n",
       "      <th>date</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>...</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>transactions</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>national_hol</th>\n",
       "      <th>regional_hol</th>\n",
       "      <th>local_hol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-05 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-05</td>\n",
       "      <td>0.186498</td>\n",
       "      <td>0.989231</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.519535</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-06 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-06</td>\n",
       "      <td>0.587660</td>\n",
       "      <td>1.037158</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.715138</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-07 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-07</td>\n",
       "      <td>-0.378907</td>\n",
       "      <td>-1.486235</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.253098</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-09 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-09</td>\n",
       "      <td>-1.345475</td>\n",
       "      <td>1.014551</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.620596</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1_103520</td>\n",
       "      <td>1_103520_2013-09-10 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>-0.378907</td>\n",
       "      <td>0.966172</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.452703</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr  item_nbr  unit_sales  onpromotion   traj_id  \\\n",
       "0          0         0         3.0            2  1_103520   \n",
       "1          0         0         4.0            2  1_103520   \n",
       "2          0         0         2.0            2  1_103520   \n",
       "3          0         0         1.0            2  1_103520   \n",
       "4          0         0         2.0            2  1_103520   \n",
       "\n",
       "                      unique_id  open        date  log_sales       oil  ...  \\\n",
       "0  1_103520_2013-09-05 00:00:00   1.0  2013-09-05   0.186498  0.989231  ...   \n",
       "1  1_103520_2013-09-06 00:00:00   1.0  2013-09-06   0.587660  1.037158  ...   \n",
       "2  1_103520_2013-09-07 00:00:00   1.0  2013-09-07  -0.378907 -1.486235  ...   \n",
       "3  1_103520_2013-09-09 00:00:00   1.0  2013-09-09  -1.345475  1.014551  ...   \n",
       "4  1_103520_2013-09-10 00:00:00   1.0  2013-09-10  -0.378907  0.966172  ...   \n",
       "\n",
       "   family  class  perishable  transactions  day_of_week  day_of_month  month  \\\n",
       "0       1      0           0      0.519535            3             5      9   \n",
       "1       1      0           0      0.715138            4             6      9   \n",
       "2       1      0           0     -0.253098            5             7      9   \n",
       "3       1      0           0      0.620596            0             9      9   \n",
       "4       1      0           0      0.452703            1            10      9   \n",
       "\n",
       "   national_hol  regional_hol  local_hol  \n",
       "0            34             1          3  \n",
       "1            34             1          3  \n",
       "2            34             1          3  \n",
       "3            34             1          3  \n",
       "4            34             1          3  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921, 24)\n",
      "(360, 24)\n",
      "(360, 24)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_single_col_by_type(input_type):\n",
    "    \"\"\"Returns name of single column for input type.\"\"\"\n",
    "    return utils.get_single_col_by_input_type(input_type,\n",
    "                                              column_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max samples=300 exceeds # available segments=127\n"
     ]
    }
   ],
   "source": [
    "identifier = '1_103520'\n",
    "df = train[train['traj_id']=='1_103520']\n",
    "num_entries = len(df) # num_entries=246\n",
    "valid_sampling_locations = []\n",
    "split_train_map = {}\n",
    "if num_entries >= time_steps: # time_steps=120\n",
    "  valid_sampling_locations += [(identifier, time_steps + i) for i in range(num_entries - time_steps + 1)]\n",
    "  split_train_map[identifier] = df\n",
    "# valid_sampling_locations: for each each traj_id, every day after 120th Day\n",
    "# split_train_map = {traj_id : df}  \n",
    "\n",
    "inputs = np.zeros((300, 120, input_size), dtype=object) # dtype=object: trainframe contains diff formats\n",
    "outputs = np.zeros((300, 120, output_size), dtype=object)\n",
    "time = np.empty((300, 120, 1), dtype=object)\n",
    "identifiers = np.empty((300, 120, 1), dtype=object)\n",
    "\n",
    "# len(ranges) <= 300, ranges=[('1_103520', 120),('1_103520', 121),...('1_103520', 246)]]\n",
    "if 300 > 0 and len(valid_sampling_locations) > 300:\n",
    "  print('Extracting {} samples...'.format(300))\n",
    "  ranges = [\n",
    "  valid_sampling_locations[i] for i in np.random.choice( \n",
    "  len(valid_sampling_locations), 300, replace=False)\n",
    "  ] # Random pick 300 from len(valid_sampling_locations)\n",
    "else:\n",
    "  print('Max samples={} exceeds # available segments={}'.format(\n",
    "  300, len(valid_sampling_locations)))\n",
    "  ranges = valid_sampling_locations\n",
    "\n",
    "id_col = _get_single_col_by_type(InputTypes.ID) # 'traj_id'\n",
    "time_col = _get_single_col_by_type(InputTypes.TIME) # 'date'\n",
    "target_col = _get_single_col_by_type(InputTypes.TARGET) # 'log_sales'\n",
    "input_cols = [ # other columns in column_definition\n",
    "  tup[0]\n",
    "  for tup in column_definition\n",
    "  if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "]\n",
    "\n",
    "for i, tup in enumerate(ranges): # eg: i=0, tup = ('1_103520', 120)\n",
    "  if (i + 1 % 1000) == 0:\n",
    "    print(i + 1, 'of', max_samples, 'samples done...')\n",
    "  identifier, start_idx = tup\n",
    "  sliced = split_train_map[identifier].iloc[start_idx - 120:start_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=126\n",
    "tup = ('1_103520', 246)\n",
    "identifier, start_idx = tup # identifier='1_103520',start_idx=120\n",
    "sliced = split_train_map[identifier].iloc[start_idx - 120:start_idx]\n",
    "inputs[i, :, :] = sliced[input_cols] # 填在(1, 120, 20)\n",
    "outputs[i, :, :] = sliced[[target_col]] # (1, 120, 1)\n",
    "time[i, :, 0] = sliced[time_col]\n",
    "identifiers[i, :, 0] = sliced[id_col] # since time and identifier are 1-D variable\n",
    "\n",
    "sampled_data = {\n",
    "'inputs': inputs, # (300, 120, 20)\n",
    "'outputs': outputs[:, 90:, :], # (300,30,1)\n",
    "'active_entries': np.ones_like(outputs[:, 90:, :]), # (300,30,1)\n",
    "'time': time, # (300, 120, 1)\n",
    "'identifier': identifiers\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_sampled_data(data, max_samples):\n",
    "    \"\"\"Samples segments into a compatible format.\n",
    "\n",
    "    Args:\n",
    "      data: Sources data to sample and batch\n",
    "      max_samples: Maximum number of samples in batch\n",
    "\n",
    "    Returns:\n",
    "      Dictionary of batched data with the maximum samples specified.\n",
    "    \"\"\"\n",
    "\n",
    "    if max_samples < 1:\n",
    "      raise ValueError(\n",
    "          'Illegal number of samples specified! samples={}'.format(max_samples))\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID) # 'traj_id'\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME) # 'date'\n",
    "\n",
    "    data.sort_values(by=[id_col, time_col], inplace=True)\n",
    "\n",
    "    print('Getting valid sampling locations.')\n",
    "    valid_sampling_locations = []\n",
    "    split_data_map = {}\n",
    "    for identifier, df in data.groupby(id_col): # for each traj_id\n",
    "      print('Getting locations for {}'.format(identifier))\n",
    "      num_entries = len(df)\n",
    "      if num_entries >= time_steps:\n",
    "        valid_sampling_locations += [\n",
    "            (identifier, time_steps + i)\n",
    "            for i in range(num_entries - time_steps + 1)\n",
    "        ]\n",
    "      split_data_map[identifier] = df\n",
    "    # valid_sampling_locations: for each each traj_id, every day after 120th Day\n",
    "    # split_data_map = {traj_id : df}  \n",
    "\n",
    "    inputs = np.zeros((max_samples, time_steps, input_size), dtype=object) # dtype=object: dataframe contains diff formats\n",
    "    outputs = np.zeros((max_samples, time_steps, output_size), dtype=object)\n",
    "    time = np.empty((max_samples, time_steps, 1), dtype=object)\n",
    "    identifiers = np.empty((max_samples, time_steps, 1), dtype=object)\n",
    "\n",
    "    if max_samples > 0 and len(valid_sampling_locations) > max_samples:\n",
    "      print('Extracting {} samples...'.format(max_samples))\n",
    "      ranges = [\n",
    "          valid_sampling_locations[i] for i in np.random.choice( \n",
    "              len(valid_sampling_locations), max_samples, replace=False)\n",
    "      ] # Random pick max_samples from len(valid_sampling_locations)\n",
    "    else:\n",
    "      print('Max samples={} exceeds # available segments={}'.format(\n",
    "          max_samples, len(valid_sampling_locations)))\n",
    "      ranges = valid_sampling_locations\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID) # 'traj_id'\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME) # 'date'\n",
    "    target_col = _get_single_col_by_type(InputTypes.TARGET) # 'log_sales'\n",
    "    input_cols = [ # other columns in column_definition\n",
    "        tup[0]\n",
    "        for tup in column_definition\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    for i, tup in enumerate(ranges):\n",
    "      if (i + 1 % 1000) == 0:\n",
    "        print(i + 1, 'of', max_samples, 'samples done...')\n",
    "      identifier, start_idx = tup\n",
    "      sliced = split_data_map[identifier].iloc[start_idx -\n",
    "                                               time_steps:start_idx]\n",
    "      inputs[i, :, :] = sliced[input_cols]\n",
    "      outputs[i, :, :] = sliced[[target_col]]\n",
    "      time[i, :, 0] = sliced[time_col]\n",
    "      identifiers[i, :, 0] = sliced[id_col]\n",
    "\n",
    "    sampled_data = {\n",
    "        'inputs': inputs,\n",
    "        'outputs': outputs[:, num_encoder_steps:, :],\n",
    "        'active_entries': np.ones_like(outputs[:, num_encoder_steps:, :]),\n",
    "        'time': time,\n",
    "        'identifier': identifiers\n",
    "    }\n",
    "\n",
    "    return sampled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 1_96995\n",
      "Getting locations for 25_103665\n",
      "Extracting 300 samples...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-1.3454751863495475, 0.6189661245892091, 0.6675337147068836,\n",
       "         ..., 31.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.5619152072903433, 0.6978270901535721,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.6026658625038188, 0.6978270901535721,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [0.1864984489215168, 0.6841671729307701, 0.7269901157701603,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.5668052859159604, 0.7213383666196587,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, 0.5048642899914775, 0.758413841046949,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[-1.3454751863495475, -1.1593924689268649, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, -1.2783843821502134, 0.7021224195079532,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, -1.2816444345672915, 0.69036678127491,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [1.1530661971010592, -0.6410441346114555, 0.8836566022220633,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.0387705294949772, 0.8664752848045385,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.040400555703516, 0.8771005732074817,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[0.1864984489215168, 0.6760170418880749, 0.6774807932117664,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.4511671602553502, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.6939473301820042, 0.7025745594399933,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [0.5876603100095371, -1.447907107838272, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.5876603100095371, 0.7835987716516504, 0.639501038920396, ...,\n",
       "         34.0, 1.0, 3.0],\n",
       "        [1.9982998432578143, 0.6254862294233652, 0.6467352778330379,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.3454751863495475, -1.3305452208234625, 0.7131998478429364,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.5876603100095371, -1.369665849828399, 0.6964706703574517,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, -1.2197034386428087, 0.6609776856923021,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-0.3789074381700053, -0.6997250781188603, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [1.1530661971010592, -1.3305452208234625, 0.780794767682935,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.3289151946149231, 0.7434932232896246,\n",
       "         ..., 34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[1.1530661971010592, -0.15692635067536578, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [1.3680235390044249, -1.1447222330500135, 0.6437963682747772,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [1.1530661971010592, -1.0893013419596869, 0.6546477266437403,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [0.1864984489215168, -0.5041219330941776, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, -1.1675425999695601, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [1.1530661971010592, -1.25067393660505, 0.8504243172171142, ...,\n",
       "         34.0, 1.0, 3.0]],\n",
       "\n",
       "       [[-1.3454751863495475, -1.3875961381223278, -1.4862348515662502,\n",
       "         ..., 5.0, 1.0, 3.0],\n",
       "        [0.1864984489215168, 0.5651752597074213, 0.6605255457602617,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, 0.7933789289028845, 0.6602994757942419,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        ...,\n",
       "        [-0.3789074381700053, 0.543984918996414, 0.816739892280125, ...,\n",
       "         34.0, 1.0, 3.0],\n",
       "        [-0.3789074381700053, -0.08846524991672683, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0],\n",
       "        [-1.3454751863495475, -1.46746742234074, -1.4862348515662502,\n",
       "         ..., 34.0, 1.0, 3.0]]], dtype=object)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples = 300\n",
    "valid_samples = 100\n",
    "_batch_sampled_data(train, max_samples=train_samples)['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_data(data):\n",
    "    \"\"\"Batches data for training.\n",
    "\n",
    "    Converts raw dataframe from a 2-D tabular format to a batched 3-D array\n",
    "    to feed into Keras model.\n",
    "\n",
    "    Args:\n",
    "      data: DataFrame to batch\n",
    "\n",
    "    Returns:\n",
    "      Batched Numpy array with shape=(?, time_steps, input_size)\n",
    "    \"\"\"\n",
    "\n",
    "    # Functions.\n",
    "    def _batch_single_entity(input_data):\n",
    "      time_steps = len(input_data)\n",
    "      lags = time_steps\n",
    "      x = input_data.values\n",
    "      if time_steps >= lags:\n",
    "        return np.stack(\n",
    "            [x[i:time_steps - (lags - 1) + i, :] for i in range(lags)], axis=1)\n",
    "\n",
    "      else:\n",
    "        return None\n",
    "\n",
    "    id_col = _get_single_col_by_type(InputTypes.ID)\n",
    "    time_col = _get_single_col_by_type(InputTypes.TIME)\n",
    "    target_col = _get_single_col_by_type(InputTypes.TARGET)\n",
    "    input_cols = [\n",
    "        tup[0]\n",
    "        for tup in column_definition\n",
    "        if tup[2] not in {InputTypes.ID, InputTypes.TIME}\n",
    "    ]\n",
    "\n",
    "    data_map = {}\n",
    "    for _, sliced in data.groupby(id_col):\n",
    "\n",
    "      col_mappings = {\n",
    "          'identifier': [id_col],\n",
    "          'time': [time_col],\n",
    "          'outputs': [target_col],\n",
    "          'inputs': input_cols\n",
    "      }\n",
    "\n",
    "      for k in col_mappings:\n",
    "        cols = col_mappings[k]\n",
    "        arr = _batch_single_entity(sliced[cols].copy())\n",
    "\n",
    "        if k not in data_map:\n",
    "          data_map[k] = [arr]\n",
    "        else:\n",
    "          data_map[k].append(arr)\n",
    "\n",
    "    # Combine all data\n",
    "    for k in data_map:\n",
    "      data_map[k] = np.concatenate(data_map[k], axis=0)\n",
    "\n",
    "    # Shorten target so we only get decoder steps\n",
    "    data_map['outputs'] = data_map['outputs'][:, num_encoder_steps:, :]\n",
    "\n",
    "    active_entries = np.ones_like(data_map['outputs'])\n",
    "    if 'active_entries' not in data_map:\n",
    "      data_map['active_entries'] = active_entries\n",
    "    else:\n",
    "      data_map['active_entries'].append(active_entries)\n",
    "\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFTDataCache(object):\n",
    "  \"\"\"Caches data for the TFT.\"\"\" \n",
    "  # stores multiple copies of data or files in a temporary storage location—or cache—\n",
    "  # so they can be accessed faster\n",
    "\n",
    "  _data_cache = {}\n",
    "\n",
    "  @classmethod\n",
    "  def update(cls, data, key):  # cls is similar to self\n",
    "    \"\"\"Updates cached data.\n",
    "\n",
    "    Args:\n",
    "      data: Source to update\n",
    "      key: Key to dictionary location\n",
    "    \"\"\"\n",
    "    cls._data_cache[key] = data\n",
    "\n",
    "  @classmethod\n",
    "  def get(cls, key):\n",
    "    \"\"\"Returns data stored at key location.\"\"\"\n",
    "    return cls._data_cache[key].copy()\n",
    "\n",
    "  @classmethod\n",
    "  def contains(cls, key): # TFTDataCache.contains('train') and TFTDataCache.contains('valid')\n",
    "    \"\"\"Retuns boolean indicating whether key is present in cache.\"\"\"\n",
    "    return key in cls._data_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 1_96995\n",
      "Getting locations for 25_103665\n",
      "Extracting 300 samples...\n",
      "Getting valid sampling locations.\n",
      "Getting locations for 1_103520\n",
      "Getting locations for 1_103665\n",
      "Getting locations for 25_103665\n",
      "Max samples=100 exceeds # available segments=3\n"
     ]
    }
   ],
   "source": [
    "# model.cache_batched_data(train, \"train\", num_samples=train_samples)\n",
    "if train_samples > 0:\n",
    "      TFTDataCache.update(\n",
    "          _batch_sampled_data(train, max_samples=train_samples),\n",
    "            \"train\")  \n",
    "      # create a temperal set, where key='train', value=_batch_sampled_data(train, max_samples=train_samples)\n",
    "      # _batch_sampled_data(train, max_samples=train_samples)\n",
    "      # dict_keys(['inputs', 'outputs', 'active_entries', 'time', 'identifier'])\n",
    "else:\n",
    "      TFTDataCache.update(\n",
    "            _batch_data(train), \n",
    "            \"train\")\n",
    "      \n",
    "# model.cache_batched_data(valid, \"valid\", num_samples=valid_samples)\n",
    "if valid_samples > 0:\n",
    "      TFTDataCache.update(\n",
    "          _batch_sampled_data(valid, max_samples=valid_samples),\n",
    "            \"valid\")  \n",
    "      # create a temperal set, where key='valid', value=_batch_sampled_data(valid, max_samples=valid_samples)\n",
    "else:\n",
    "      TFTDataCache.update(\n",
    "            _batch_data(valid), \n",
    "            \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting batched_data\n",
      "Using cached training data\n",
      "Using cached validation data\n"
     ]
    }
   ],
   "source": [
    "print('Getting batched_data')\n",
    "train_df=None\n",
    "valid_df=None\n",
    "if train_df is None:\n",
    "      print('Using cached training data')\n",
    "      train_data = TFTDataCache.get('train')\n",
    "else:\n",
    "      train_data = _batch_data(train_df)\n",
    "\n",
    "if valid_df is None:\n",
    "      print('Using cached validation data')\n",
    "      valid_data = TFTDataCache.get('valid')\n",
    "else:\n",
    "      valid_data = _batch_data(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using keras standard fit\n"
     ]
    }
   ],
   "source": [
    "print('Using keras standard fit')\n",
    "# Unpack without sample weights\n",
    "data = train_data['inputs'] # (300, 120, 20)\n",
    "labels = train_data['outputs'] # (300, 30, 1)\n",
    "active_flags = (np.sum(train_data['active_entries'], axis=-1) > 0.0) * 1.0  # (300, 30)\n",
    " \n",
    "val_data = valid_data['inputs']\n",
    "val_labels = valid_data['outputs']\n",
    "val_flags = (np.sum(valid_data['active_entries'], axis=-1) > 0.0) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add relevant callbacks\n",
    "name = 'favorita'\n",
    "def get_keras_saved_path(model_folder):\n",
    "    \"\"\"Returns path to keras checkpoint.\"\"\"\n",
    "    return os.path.join(model_folder, '{}.check'.format(name))\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=early_stopping_patience,\n",
    "            min_delta=1e-4),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=get_keras_saved_path(_temp_folder),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True),\n",
    "        tf.keras.callbacks.TerminateOnNaN()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting batched_data\n",
      "Using cached training data\n",
      "Using cached validation data\n",
      "Using keras standard fit\n"
     ]
    }
   ],
   "source": [
    "print('Getting batched_data')\n",
    "if train_df is None:\n",
    "  print('Using cached training data')\n",
    "  train_data = TFTDataCache.get('train')\n",
    "else:\n",
    "  train_data = _batch_data(train_df)\n",
    "\n",
    "if valid_df is None:\n",
    "  print('Using cached validation data')\n",
    "  valid_data = TFTDataCache.get('valid')\n",
    "else:\n",
    "  valid_data = _batch_data(valid_df)\n",
    "\n",
    "print('Using keras standard fit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_active_locations(x):\n",
    "    \"\"\"Formats sample weights for Keras training.\"\"\"\n",
    "    return (np.sum(x, axis=-1) > 0.0) * 1.0\n",
    "\n",
    "def _unpack(data):\n",
    "  return data['inputs'], data['outputs'], \\\n",
    "  _get_active_locations(data['active_entries'])\n",
    "\n",
    "# Unpack without sample weights\n",
    "data, labels, active_flags = _unpack(train_data)\n",
    "val_data, val_labels, val_flags = _unpack(valid_data)\n",
    "\n",
    "all_callbacks = callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 30, 1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 300 samples, validate on 100 samples\n",
      "300/300 [==============================] - 10s 32ms/sample - loss: 2.3337 - val_loss: 0.4742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2356a2215f8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "        x=tf.convert_to_tensor(data,dtype=np.float32), # shape=(300, 120, 20)\n",
    "        y=np.concatenate([labels, labels, labels], axis=-1), # shape=(300, 30, 3) # 3 represents # of quantiles\n",
    "        #y=tf.convert_to_tensor(np.concatenate([labels, labels, labels], axis=-1),dtype=np.float32), \n",
    "        sample_weight=active_flags,\n",
    "        epochs=num_epochs,\n",
    "        batch_size=minibatch_size,\n",
    "        validation_data=(val_data,\n",
    "                         np.concatenate([val_labels, val_labels, val_labels],\n",
    "                                        axis=-1), val_flags),\n",
    "        callbacks=all_callbacks,\n",
    "        shuffle=True,\n",
    "        use_multiprocessing=True,\n",
    "        workers=n_multiprocessing_workers\n",
    "        )\n",
    "\n",
    "# use_multiprocessing: https://stackoverflow.com/questions/52932406/is-the-class-generator-inheriting-sequence-thread-safe-in-keras-tensorflow/63641535#63641535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot load from 0615_result\\tmp, skipping ...\n"
     ]
    }
   ],
   "source": [
    "# Load best checkpoint again\n",
    "tmp_checkpont = get_keras_saved_path(_temp_folder)\n",
    "if os.path.exists(tmp_checkpont):\n",
    "  load(\n",
    "  _temp_folder,\n",
    "  use_keras_loadings=True)\n",
    "\n",
    "else:\n",
    "  print('Cannot load from {}, skipping ...'.format(_temp_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\"\"\"Applies evaluation metric to the training data.\n",
    "\n",
    "    Args:\n",
    "      data: Dataframe for evaluation\n",
    "      eval_metric: Evaluation metic to return, based on model definition.\n",
    "\n",
    "    Returns:\n",
    "      Computed evaluation loss.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached validation data\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 0.4742\n",
      "0.47415144205093385\n"
     ]
    }
   ],
   "source": [
    "print('Using cached validation data')\n",
    "raw_data = TFTDataCache.get('valid')\n",
    "# _batch_data(data)\n",
    "\n",
    "inputs = raw_data['inputs']\n",
    "outputs = raw_data['outputs']\n",
    "active_entries = _get_active_locations(raw_data['active_entries'])\n",
    "\n",
    "metric_values = model.evaluate(\n",
    "    x=inputs,\n",
    "    y=np.concatenate([outputs, outputs, outputs], axis=-1),\n",
    "    sample_weight=active_entries,\n",
    "    workers=16,\n",
    "    use_multiprocessing=True)\n",
    "\n",
    "metrics = pd.Series(metric_values, model.metrics_names)\n",
    "print(metrics['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict\n",
    "\"\"\"Computes predictions for a given input dataset.\n",
    "\n",
    "    Args:\n",
    "      df: Input dataframe\n",
    "      return_targets: Whether to also return outputs aligned with predictions to\n",
    "        faciliate evaluation\n",
    "\n",
    "    Returns:\n",
    "      Input dataframe or tuple of (input dataframe, algined output dataframe).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = _batch_data(test)\n",
    "inputs = data['inputs']\n",
    "time = data['time']\n",
    "identifier = data['identifier']\n",
    "outputs = data['outputs']\n",
    "\n",
    "combined = model.predict(inputs,\n",
    "                         workers=16,\n",
    "                         use_multiprocessing=True,\n",
    "                         batch_size=minibatch_size)\n",
    "# Format output_csv\n",
    "if output_size != 1:\n",
    "    raise NotImplementedError('Current version only supports 1D targets!')\n",
    "\n",
    "def format_outputs(prediction):\n",
    "      \"\"\"Returns formatted dataframes for prediction.\"\"\"\n",
    "\n",
    "      flat_prediction = pd.DataFrame(\n",
    "          prediction[:, :, 0],\n",
    "          columns=[\n",
    "              't+{}'.format(i)\n",
    "              for i in range(time_steps - num_encoder_steps)\n",
    "          ])\n",
    "      cols = list(flat_prediction.columns)\n",
    "      flat_prediction['forecast_time'] = time[:, num_encoder_steps - 1, 0]\n",
    "      flat_prediction['identifier'] = identifier[:, 0, 0]\n",
    "\n",
    "      # Arrange in order\n",
    "      return flat_prediction[['forecast_time', 'identifier'] + cols]\n",
    "# Extract predictions for each quantile into different entries\n",
    "process_map = {\n",
    "        'p{}'.format(int(q * 100)):\n",
    "        combined[Ellipsis, i * output_size:(i + 1) * output_size]\n",
    "        for i, q in enumerate(quantiles)\n",
    "    }\n",
    "return_targets = False\n",
    "if return_targets:\n",
    "      # Add targets if relevant\n",
    "      process_map['targets'] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p10':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520 -0.921138 -0.975129 -0.858840 -0.950917 -1.011550   \n",
      "1    2014-01-13   1_103665 -0.663760 -0.763067 -0.881042 -0.723420 -0.942091   \n",
      "2    2014-01-10  25_103665 -0.411772 -0.710101 -0.678902 -0.659965 -0.801824   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0 -0.827111 -0.973458 -0.991614  ... -0.891888 -0.567603 -0.584162 -0.750056   \n",
      "1 -0.867548 -0.896401 -0.795341  ... -0.620795 -0.728647 -0.564356 -0.657984   \n",
      "2 -0.624304 -0.790431 -0.726994  ... -0.596657 -0.762090 -0.524362 -0.793863   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0 -0.538907 -0.803590 -0.717859 -0.751176 -0.647569 -0.644404  \n",
      "1 -0.670879 -0.766426 -0.585354 -0.720180 -0.658800 -0.671636  \n",
      "2 -0.701905 -0.634675 -0.616152 -0.759249 -0.531140 -0.788158  \n",
      "\n",
      "[3 rows x 32 columns], 'p50':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520  0.854397  0.779657  0.845148  1.153586  0.792984   \n",
      "1    2014-01-13   1_103665  0.743370  1.073851  0.715247  1.086122  0.740100   \n",
      "2    2014-01-10  25_103665  1.033620  0.611969  0.701160  0.790162  0.736007   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0  1.162444  0.830248  0.798323  ...  0.800088  0.953040  1.233700  0.869070   \n",
      "1  0.779039  0.752956  0.827731  ...  1.048886  0.751878  1.070177  0.801021   \n",
      "2  1.141250  0.806210  0.869431  ...  1.289713  0.857889  1.268352  0.824819   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0  1.238119  0.839098  0.871532  0.825338  0.908296  1.213454  \n",
      "1  1.069676  0.745608  1.091218  0.769947  0.808103  1.092929  \n",
      "2  0.871119  0.921457  1.216116  0.830196  1.243926  0.811375  \n",
      "\n",
      "[3 rows x 32 columns], 'p90':   forecast_time identifier       t+0       t+1       t+2       t+3       t+4  \\\n",
      "0    2014-02-01   1_103520  0.676101  0.751127  0.575569  0.854668  0.839359   \n",
      "1    2014-01-13   1_103665  0.209921  0.506779  0.588589  0.461486  0.707844   \n",
      "2    2014-01-10  25_103665 -0.050507  0.271881  0.254654  0.263835  0.490332   \n",
      "\n",
      "        t+5       t+6       t+7  ...      t+20      t+21      t+22      t+23  \\\n",
      "0  0.656787  0.786173  0.808719  ...  0.659832  0.159830  0.320498  0.434982   \n",
      "1  0.593801  0.635602  0.496485  ...  0.305444  0.367512  0.226544  0.272258   \n",
      "2  0.375296  0.499772  0.424341  ...  0.453483  0.500317  0.316695  0.534731   \n",
      "\n",
      "       t+24      t+25      t+26      t+27      t+28      t+29  \n",
      "0  0.254297  0.513516  0.383028  0.420555  0.285911  0.421514  \n",
      "1  0.408069  0.436675  0.276645  0.363743  0.280241  0.424844  \n",
      "2  0.398894  0.312329  0.443074  0.486249  0.316503  0.521966  \n",
      "\n",
      "[3 rows x 32 columns]}\n"
     ]
    }
   ],
   "source": [
    "print({k: format_outputs(process_map[k]) for k in process_map})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Attention\n",
    " \"\"\"Computes TFT attention weights for a given dataset.\n",
    "\n",
    "    Args:\n",
    "      df: Input dataframe\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of numpy arrays for temporal attention weights and variable\n",
    "          selection weights, along with their identifiers and time indices\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batched_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11068\\716023743.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_attention_components\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_attention_components\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0m_input_placeholder\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'batched_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "for k in _attention_components:\n",
    "    print(_attention_components[k],{_input_placeholder : batched_inputs[0].astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_46:0' shape=(None, 120, 20) dtype=float32>"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.18649845,  0.5195345 ,  0.9892313 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.5876603 ,  0.71513766,  1.0371581 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [-0.37890744, -0.2530979 , -1.4862349 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        ...,\n",
       "        [-1.3454752 ,  0.71839774,  0.80362785, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [-1.3454752 ,  0.7917489 ,  0.75841385, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 1.3680235 ,  0.5733254 ,  0.7647438 , ..., 34.        ,\n",
       "          1.        ,  3.        ]],\n",
       "\n",
       "       [[ 0.18649845,  0.7852288 ,  0.9831274 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [-1.3454752 ,  0.46411362,  0.9385916 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 1.7184721 , -0.15692635, -1.4862349 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        ...,\n",
       "        [-0.37890744,  0.66297686,  0.80407995, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.5876603 ,  0.84390974,  0.875292  , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.89882565,  0.55050504,  0.8694142 , ..., 34.        ,\n",
       "          1.        ,  3.        ]],\n",
       "\n",
       "       [[-0.37890744, -1.2278535 , -1.4862349 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [-1.3454752 , -1.3598857 ,  0.96617216, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.18649845, -1.1153817 ,  0.97001535, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        ...,\n",
       "        [ 1.7184721 , -0.197677  ,  0.80407995, ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.89882565,  0.33208153, -1.4862349 , ..., 34.        ,\n",
       "          1.        ,  3.        ],\n",
       "        [ 0.5876603 , -0.8708778 , -1.4862349 , ..., 34.        ,\n",
       "          1.        ,  3.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_inputs[0].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1847211028.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Get attention weights, while avoiding large memory increases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m attention_by_batch = [\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mget_batch_attention_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     ]\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1847211028.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Get attention weights, while avoiding large memory increases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m attention_by_batch = [\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mget_batch_attention_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     ]\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1847211028.py\u001b[0m in \u001b[0;36mget_batch_attention_weights\u001b[1;34m(input_batch)\u001b[0m\n\u001b[0;32m     11\u001b[0m         attention_weight = tf.compat.v1.keras.backend.get_session().run(\n\u001b[0;32m     12\u001b[0m             \u001b[0m_attention_components\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             {input_placeholder: input_batch.astype(np.float32)})\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mattention_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\TFT\\sep_venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    711\u001b[0m     if (Tensor._USE_EQUALITY and executing_eagerly_outside_functions() and\n\u001b[0;32m    712\u001b[0m         (g is None or g._building_function)):  # pylint: disable=protected-access\n\u001b[1;32m--> 713\u001b[1;33m       raise TypeError(\"Tensor is unhashable if Tensor equality is enabled. \"\n\u001b[0m\u001b[0;32m    714\u001b[0m                       \"Instead, use tensor.experimental_ref() as the key.\")\n\u001b[0;32m    715\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Tensor is unhashable if Tensor equality is enabled. Instead, use tensor.experimental_ref() as the key."
     ]
    }
   ],
   "source": [
    "\n",
    "data = _batch_data(test)\n",
    "inputs = data['inputs']\n",
    "identifiers = data['identifier']\n",
    "time = data['time']\n",
    "\n",
    "def get_batch_attention_weights(input_batch):\n",
    "      \"\"\"Returns weights for a given minibatch of data.\"\"\"\n",
    "      input_placeholder = _input_placeholder\n",
    "      attention_weights = {}\n",
    "      for k in _attention_components:\n",
    "        attention_weight = tf.compat.v1.keras.backend.get_session().run(\n",
    "            _attention_components[k],\n",
    "            {input_placeholder: input_batch.astype(np.float32)})\n",
    "        \n",
    "        attention_weights[k] = attention_weight\n",
    "      return attention_weights\n",
    "\n",
    "    # Compute number of batches\n",
    "batch_size = minibatch_size\n",
    "n = inputs.shape[0]\n",
    "num_batches = n // batch_size\n",
    "if n - (num_batches * batch_size) > 0:\n",
    "      num_batches += 1\n",
    "\n",
    "# Split up inputs into batches\n",
    "batched_inputs = [\n",
    "        inputs[i * batch_size:(i + 1) * batch_size, Ellipsis]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "\n",
    "# Get attention weights, while avoiding large memory increases\n",
    "attention_by_batch = [\n",
    "        get_batch_attention_weights(batch) for batch in batched_inputs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.backend' has no attribute 'get_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1272094066.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Get attention weights, while avoiding large memory increases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m attention_by_batch = [\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mget_batch_attention_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     ]\n\u001b[0;32m     34\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1272094066.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Get attention weights, while avoiding large memory increases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m attention_by_batch = [\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mget_batch_attention_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatched_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     ]\n\u001b[0;32m     34\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\1272094066.py\u001b[0m in \u001b[0;36mget_batch_attention_weights\u001b[1;34m(input_batch)\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_attention_components\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         attention_weight = tf.keras.backend.get_session().run(\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0m_attention_components\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             {input_placeholder: input_batch.astype(np.float32)})\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.backend' has no attribute 'get_session'"
     ]
    }
   ],
   "source": [
    "data = _batch_data(test)\n",
    "inputs = data['inputs']\n",
    "identifiers = data['identifier']\n",
    "time = data['time']\n",
    "\n",
    "def get_batch_attention_weights(input_batch):\n",
    "      \"\"\"Returns weights for a given minibatch of data.\"\"\"\n",
    "      input_placeholder = _input_placeholder\n",
    "      attention_weights = {}\n",
    "      for k in _attention_components:\n",
    "        attention_weight = tf.keras.backend.get_session().run(\n",
    "            _attention_components[k],\n",
    "            {input_placeholder: input_batch.astype(np.float32)})\n",
    "        attention_weights[k] = attention_weight\n",
    "      return attention_weights\n",
    "\n",
    "    # Compute number of batches\n",
    "batch_size = minibatch_size\n",
    "n = inputs.shape[0]\n",
    "num_batches = n // batch_size\n",
    "if n - (num_batches * batch_size) > 0:\n",
    "      num_batches += 1\n",
    "\n",
    "# Split up inputs into batches\n",
    "batched_inputs = [\n",
    "        inputs[i * batch_size:(i + 1) * batch_size, Ellipsis]\n",
    "        for i in range(num_batches)\n",
    "    ]\n",
    "\n",
    "# Get attention weights, while avoiding large memory increases\n",
    "attention_by_batch = [\n",
    "        get_batch_attention_weights(batch) for batch in batched_inputs\n",
    "    ]\n",
    "attention_weights = {}\n",
    "for k in _attention_components:\n",
    "      attention_weights[k] = []\n",
    "      for batch_weights in attention_by_batch:\n",
    "        attention_weights[k].append(batch_weights[k])\n",
    "\n",
    "      if len(attention_weights[k][0].shape) == 4:\n",
    "        tmp = np.concatenate(attention_weights[k], axis=1)\n",
    "      else:\n",
    "        tmp = np.concatenate(attention_weights[k], axis=0)\n",
    "\n",
    "      del attention_weights[k]\n",
    "      gc.collect()\n",
    "      attention_weights[k] = tmp\n",
    "\n",
    "attention_weights['identifiers'] = identifiers[:, 0, 0]\n",
    "attention_weights['time'] = time[:, :, 0]\n",
    "\n",
    "print(attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "      input_placeholder = _input_placeholder\n",
    "      attention_weights = {}\n",
    "      for k in _attention_components:\n",
    "        attention_weight = tf.keras.backend.get_session().run(\n",
    "            _attention_components[k],\n",
    "            {input_placeholder: batched_inputs[0].astype(np.float32)})\n",
    "        attention_weights[k] = attention_weight\n",
    "      return attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_self_attn\n",
      "static_flags\n",
      "historical_flags\n",
      "future_flags\n"
     ]
    }
   ],
   "source": [
    "for k in _attention_components:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.keras.backend' has no attribute 'get_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13564\\208681753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m tf.keras.backend.get_session().run(\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0m_attention_components\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_attention_components\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             {input_placeholder: batched_inputs[0].astype(np.float32)})\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.keras.backend' has no attribute 'get_session'"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.get_session().run(\n",
    "            _attention_components[_attention_components[0]],\n",
    "            {input_placeholder: batched_inputs[0].astype(np.float32)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sep_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
